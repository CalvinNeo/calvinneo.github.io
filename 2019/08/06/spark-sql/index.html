<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>





<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="并行计算,数据挖掘,数据库,Scala,Spark," />





  <link rel="alternate" href="/atom.xml" title="Calvin's Marbles" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Spark是MapReduce的下一代的分布式计算框架。相比更早期的MapReduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。本文写作基于Spark 2.4.4版本的源码。 【TLDR】本来写文章确实是简练清楚为最佳，不过我发现Spark架构实在是很庞大，其中涉及到的一些架构知识我觉得都很有启发意义，因此这篇文章就被">
<meta name="keywords" content="并行计算,数据挖掘,数据库,Scala,Spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark和SparkSQL">
<meta property="og:url" content="http://www.calvinneo.com/2019/08/06/spark-sql/index.html">
<meta property="og:site_name" content="Calvin&#39;s Marbles">
<meta property="og:description" content="Spark是MapReduce的下一代的分布式计算框架。相比更早期的MapReduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。本文写作基于Spark 2.4.4版本的源码。 【TLDR】本来写文章确实是简练清楚为最佳，不过我发现Spark架构实在是很庞大，其中涉及到的一些架构知识我觉得都很有启发意义，因此这篇文章就被">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/sparkexe.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/rela.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/blockmanager.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/rdd.saveas.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/mem_layout_16.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/shuffle.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/shuffle-comp.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/yarn.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/spark-sql-detail.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/case1/ori_job_4.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/case1/ori_stage_4.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/case1/stage7.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/case1/tasks2.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/case1/new_job_4.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/yarn-exe-1.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/yarn-exe-2.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/spark-yarn-mem.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/dfexplainjoin.png">
<meta property="og:updated_time" content="2023-05-31T04:02:15.817Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark和SparkSQL">
<meta name="twitter:description" content="Spark是MapReduce的下一代的分布式计算框架。相比更早期的MapReduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。本文写作基于Spark 2.4.4版本的源码。 【TLDR】本来写文章确实是简练清楚为最佳，不过我发现Spark架构实在是很庞大，其中涉及到的一些架构知识我觉得都很有启发意义，因此这篇文章就被">
<meta name="twitter:image" content="http://www.calvinneo.com/img/sparksql/sparkexe.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.calvinneo.com/2019/08/06/spark-sql/"/>





  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5487541356791902"
     crossorigin="anonymous"></script>
  <title>Spark和SparkSQL | Calvin's Marbles</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Calvin's Marbles</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.calvinneo.com/2019/08/06/spark-sql/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Calvin Neo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Calvin's Marbles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark和SparkSQL
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-06T16:42:32+08:00">
                2019-08-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Spark是MapReduce的下一代的分布式计算框架。相比更早期的MapReduce的<a href="http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html" target="_blank" rel="noopener">Job和Task的两层</a>，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。本文写作基于Spark 2.4.4版本的源码。</p>
<p>【TLDR】本来写文章确实是简练清楚为最佳，不过我发现Spark架构实在是很庞大，其中涉及到的一些架构知识我觉得都很有启发意义，因此这篇文章就被我写得很长。为了简化论述，我将部分细节放到了源码中作为注释，因此正文中是主要内容。</p>
<p>【注】本篇文章经授权已被腾讯技术工程<a href="https://zhuanlan.zhihu.com/p/103073929" target="_blank" rel="noopener">知乎号</a>和微信收录。</p>
<a id="more"></a>

<h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h1><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD(Resilient Distributed Dataset)，即弹性数据集是Spark中的基础结构。RDD是distributive的、immutable的，可以存在在内存中，也可以被缓存。<br>对RDD具有转换操作和行动操作两种截然不同的操作。转换(Transform)操作从一个RDD生成另一个RDD，但行动(Action)操作会去掉RDD的壳。例如<code>take</code>是行动操作，返回的是一个数组而不是RDD了，在Scala中可以看到。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(<span class="type">Seq</span>(<span class="number">10</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">12</span>, <span class="number">3</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">40</span>] at makeRDD at :<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(<span class="number">1</span>)</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>)                                                    </span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.take(<span class="number">2</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<p>转换操作是Lazy的，直到遇到一个Action操作，Spark才会生成关于整条链的执行计划并执行。这些Action操作将一个Spark Application分为了多个Job。<br>常见的<a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">Action操作</a>包括：<code>reduce</code>、<code>collect</code>、<code>count</code>、<code>take(n)</code>、<code>first</code>、<code>takeSample(withReplacement, num, [seed])</code>、<code>takeOrdered(n, [ordering])</code>、<code>saveAsTextFile(path)</code>、<code>saveAsSequenceFile(path)</code>、<code>saveAsObjectFile(path)</code>、<code>countByKey()</code>、<code>foreach(func)</code>。<br>我们需要注意的是，有一些Transform操作也会得到一个Job，例如<code>sortBy</code>，<a href="https://stackoverflow.com/questions/41403670/why-does-sortby-transformation-trigger-a-spark-job" target="_blank" rel="noopener">这是因为</a>这个Job是用来初始化<code>RangePartitioner</code>，然后Sample输入RDD的partition边界的，和<code>sortBy</code>的业务无关，在实践中所占用的时间也是远小于实际占用的时间的。</p>
<h3 id="RDD的常见成员"><a href="#RDD的常见成员" class="headerlink" title="RDD的常见成员"></a>RDD的常见成员</h3><ol>
<li>   <code>def getPartitions: Array[Partition]</code>：获得这个RDD的所有分区，由Partition的子类来描述</li>
<li>   <code>def compute(partition: Partition, context: TaskContext): Iterator[T]</code></li>
<li><code>def getDependencies: Seq[Dependency[_]]</code>：用来获取依赖关系<br> 包含ShuffleDependency、OneToOneDependency、RangeDependency等。</li>
<li><code>part: Partitioner</code><br> Partitioner是一个<code>abstract class</code>，具有<code>numPartitions: Int</code>和<code>getPartition(key: Any): Int</code>两个方法。通过继承<code>Partitioner</code>可以自定义分区的实现方式，目前官方提供的有<code>RangePartitioner</code>和<code>HashPartitioner</code>等。<br> HashPartitioner是默认分区器，对key的hashCode取模，得到其对应的RDD分区的值。注意这里的hashCode可能还是个坑，例如Java里面数组的hashCode并不蕴含数组内容的信息，所以可能相同的数组被分到不同的分区。如果我们有这样的需求，就需要自定义分区器。<br> RangePartitioner会从整个RDD中Sample出一些Key。Sample的Key的数量是基于生成的子RDD的partition数量来计算的，默认是每个partition取20个，再乘以分区数，最懂不超过1e6个。得到总共要sample多少个之后，我们要乘以3，再平摊到父RDD上。<a href="https://blog.csdn.net/qq_22473611/article/details/107822168" target="_blank" rel="noopener">乘以三的意思是便于判断Data skew</a>，如果父RDD的某个partition的数量大于了乘以3之后平摊的值，就可以认为这个partition偏斜了，需要丢这些partition进行重新抽样。</li>
<li>   <code>def getPreferredLocations(partition: Partition): Seq[String]</code></li>
</ol>
<h3 id="常见RDD"><a href="#常见RDD" class="headerlink" title="常见RDD"></a>常见RDD</h3><p>RDD是一个抽象类<code>abstract class RDD[T] extends Serializable with Logging</code>，在Spark中有诸如<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.package" target="_blank" rel="noopener">ShuffledRDD、HadoopRDD</a>等实现。每个RDD都有对应的<code>compute</code>方法，用来描述这个RDD的计算方法。需要注意的是，这些RDD可能被作为某些RDD计算的中间结果，例如<code>CoGroupedRDD</code>，对应的，例如<code>MapPartitionsRDD</code>也可能是经过多个RDD变换得到的，其决定权在于所使用的算子。<br>我们来具体查看一些RDD。</p>
<ol>
<li><p><code>ParallelCollectionRDD</code><br> 这个RDD由<code>parallelize</code>得到</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> arr = sc.parallelize(<span class="number">0</span> to <span class="number">1000</span>)</span><br><span class="line">arr: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>HadoopRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HadoopRDD</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] <span class="keyword">with</span> <span class="title">Logging</span></span></span><br></pre></td></tr></table></figure></li>
<li><p><code>FileScanRDD</code><br> 这个RDD一般从<code>spark.read.text(...)</code>语句中产生，所以实现在<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala" target="_blank" rel="noopener">sql模块中</a>。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FileScanRDD</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private val sparkSession: <span class="type">SparkSession</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    readFunction: (<span class="type">PartitionedFile</span></span>) <span class="title">=&gt;</span> <span class="title">Iterator</span>[<span class="type">InternalRow</span>],</span></span><br><span class="line"><span class="class">    <span class="title">@transient</span> <span class="title">val</span> <span class="title">filePartitions</span></span>: <span class="type">Seq</span>[<span class="type">FilePartition</span>])</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">RDD</span>[<span class="type">InternalRow</span>](sparkSession.sparkContext, <span class="type">Nil</span>) &#123;</span><br></pre></td></tr></table></figure></li>
<li><p><code>MapPartitionsRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">U</span>]</span></span><br></pre></td></tr></table></figure>

<p> 这个RDD是<code>map</code>、<code>mapPartitions</code>、<code>mapPartitionsWithIndex</code>操作的结果。<br> 注意，在较早期的版本中，<code>map</code>会得到一个<code>MappedRDD</code>，<code>filter</code>会得到一个<code>FilteredRDD</code>、<code>flatMap</code>会得到一个<code>FlatMappedRDD</code>，不过目前已经找不到了，统一变成<code>MapPartitionsRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a3 = arr.map(i =&gt; (i+<span class="number">1</span>, i))</span><br><span class="line">a3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a3 = arr.filter(i =&gt; i &gt; <span class="number">3</span>)</span><br><span class="line">a3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at filter at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> a3 = arr.flatMap(i =&gt; <span class="type">Array</span>(i))</span><br><span class="line">a3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">5</span>] at flatMap at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>

<p> <code>join</code>操作的结果也是<code>MapPartitionsRDD</code>，这是因为其执行过程的最后一步<code>flatMapValues</code>会创建一个<code>MapPartitionsRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">3</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">1</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">3</span>),(<span class="number">2</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">3</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">9</span>] at parallelize at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rddj = rdd1.join(rdd2)</span><br><span class="line">rddj: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">MapPartitionsRDD</span>[<span class="number">12</span>] at join at &lt;console&gt;:<span class="number">27</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>ShuffledRDD</code><br> <code>ShuffledRDD</code>用来存储所有Shuffle操作的结果，其中<code>K</code>、<code>V</code>很好理解，<code>C</code>是Combiner Class。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span></span><br></pre></td></tr></table></figure>

<p> 以<code>groupByKey</code>为例</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a2 = arr.map(i =&gt; (i+<span class="number">1</span>, i))</span><br><span class="line">a2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; a2.groupByKey</span><br><span class="line">res1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">3</span>] at groupByKey at &lt;console&gt;:<span class="number">26</span></span><br></pre></td></tr></table></figure>

<p> 注意，<code>groupByKey</code>需要K是Hashable的，否则会报错。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a2 = arr.map(i =&gt; (<span class="type">Array</span>.fill(<span class="number">10</span>)(i), i))</span><br><span class="line">a2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Array</span>[<span class="type">Int</span>], <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; a2.groupByKey</span><br><span class="line">org.apache.spark.<span class="type">SparkException</span>: <span class="type">HashPartitioner</span> cannot partition array keys.</span><br><span class="line">  at org.apache.spark.rdd.<span class="type">PairRDDFunctions</span>$$anonfun$combineByKeyWithClassTag$<span class="number">1.</span>apply(<span class="type">PairRDDFunctions</span>.scala:<span class="number">84</span>)</span><br><span class="line">  at org.apache.spark.rdd.<span class="type">PairRDDFunctions</span>$$anonfun$combineByKeyWithClassTag$<span class="number">1.</span>apply(<span class="type">PairRDDFunctions</span>.scala:<span class="number">77</span>)</span><br></pre></td></tr></table></figure></li>
<li><p><code>CoGroupedRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoGroupedRDD</span>[<span class="type">K</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">Array</span>[<span class="type">Iterable</span>[_]])]</span></span><br></pre></td></tr></table></figure>

<p> 首先，我们需要了解一下什么是<code>cogroup</code>操作，这个方法有多个重载版本。如下所示的版本，对<code>this</code>或<code>other1</code>或<code>other2</code>的所有的key，生成一个<code>RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))</code>，表示对于这个key，这三个RDD中所有值的集合。容易看到，这个算子能够被用来实现Join和Union（不过后者有点大材小用了）</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], partitioner: <span class="type">Partitioner</span>)</span><br><span class="line">  : <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))]</span><br></pre></td></tr></table></figure></li>
<li><p><code>UnionRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionRDD</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">T</span>]</span></span><br></pre></td></tr></table></figure>

<p> <code>UnionRDD</code>一般通过<code>union</code>算子得到</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> a5 = arr.union(arr2)</span><br><span class="line">a5: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">7</span>] at union at &lt;console&gt;:<span class="number">27</span></span><br></pre></td></tr></table></figure></li>
<li><p><code>CoalescedRDD</code></p>
</li>
</ol>
<h3 id="常见RDD外部函数"><a href="#常见RDD外部函数" class="headerlink" title="常见RDD外部函数"></a>常见RDD外部函数</h3><p>Spark在RDD之外提供了一些外部函数，它们可以通过隐式转换的方式变成RDD。</p>
<ol>
<li><p><code>PairRDDFunctions</code><br> 这个RDD被用来处理KV对，相比<code>RDD</code>，它提供了<code>groupByKey</code>、<code>join</code>等方法。以<code>combineByKey</code>为例，他有三个模板参数，从RDD过来的<code>K</code>和<code>V</code>以及自己的<code>C</code>。相比reduce和fold系列的<code>(V, V) =&gt; V</code>，这多出来的<code>C</code>使<code>combineByKey</code>更灵活，通过<code>combineByKey</code>能够将<code>V</code>变换为<code>C</code>。需要注意的是，这三个函数将来在<code>ExternalSorter</code>里面还将会被看到。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="comment">//实现略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p><code>OrderedRDDFunctions</code><br> 这个用来提供<code>sortByKey</code>、<code>filterByRange</code>等方法。</p>
</li>
</ol>
<h2 id="Spark的架构概览"><a href="#Spark的架构概览" class="headerlink" title="Spark的架构概览"></a>Spark的架构概览</h2><p>Spark在设计上的一个特点是它和下层的集群管理是分开的，一个Spark Application可以看做是由集群上的若干进程组成的。因此需要区分Spark中的概念和下层集群中的概念，例如常见的Master和Worker是集群中的概念，表示节点；而Driver和Executor是Spark中的概念，表示进程。根据<a href="https://stackoverflow.com/questions/34722415/understand-spark-cluster-manager-master-and-driver-nodes" target="_blank" rel="noopener">爆栈网</a>，Driver可能位于某个Worker节点中，或者位于Master节点上，这取决于部署的方式</p>
<p>在<a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">官网</a>上给了这样一幅图，详细阐明了Spark集群下的基础架构。<code>SparkContext</code>是整个Application的管理核心，由Driver来负责管理。<code>SparkContext</code>负责管理所有的Executor，并且和下层的集群管理进行交互，以请求资源。</p>
<p><img src="/img/sparksql/sparkexe.png"></p>
<p>在Stage层次及以上接受<code>DAGScheduler</code>的调度，而<code>TaskScheduler</code>则调度一个Taskset。在Spark on Yarn模式下，<a href="https://blog.csdn.net/chic_data/article/details/77317730" target="_blank" rel="noopener">CoarseGrainedExecutorBackend和Executor一一对应</a>，它是一个独立于Worker主进程之外的一个进程，我们可以jps查看到。而Task是作为一个Executor启动的一个线程来跑的，一个Executor中可以跑多个Task。在实现上，<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-CoarseGrainedExecutorBackend.html" target="_blank" rel="noopener"><code>CoarseGrainedExecutorBackend</code></a>继承了<code>ExecutorBackend</code>这个trait，作为一个<code>IsolatedRpcEndpoint</code>，维护<code>Executor</code>对象实例，并通过创建的<code>DriverEndpoint</code>实例的与Driver进行交互。在进程启动时，<code>CoarseGrainedExecutorBackend</code>调用<code>onStart()</code>方法向Driver注册自己，并产生一条<code>&quot;Connecting to driver&quot;</code>的INFO。<code>CoarseGrainedExecutorBackend</code>通过<code>DriverEndpoint.receive</code>方法来处理来自Driver的命令，包括<code>LaunchTask</code>、<code>KillTask</code>等。这里注意一下，在scheduler中有一个<code>CoarseGrainedSchedulerBackend</code>，里面实现相似，在看代码时要注意区分开。</p>
<p>有关Executor和Driver的关系，下面这张图更加直观，需要说明的是，<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">一个Worker上面也可能跑有多个Executor</a>，<a href="https://blog.51cto.com/10120275/2364992" target="_blank" rel="noopener">每个Task也可以在多个CPU核心上面运行</a></p>
<p><img src="/img/sparksql/rela.png"></p>
<h2 id="Spark上下文"><a href="#Spark上下文" class="headerlink" title="Spark上下文"></a>Spark上下文</h2><p>在代码里我们操作一个Spark任务有两种方式，通过SparkContext，或者通过SparkSession</p>
<ol>
<li><p>SparkContext方式<br> SparkContext是Spark自创建来一直存在的类。我们通过SparkConf直接创建SparkContext</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"AppName"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf).set(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br></pre></td></tr></table></figure></li>
<li><p>SparkSession方式<br> SparkSession是在Spark2.0之后提供的API，相比SparkContext，他提供了对SparkSQL的支持（持有<code>SQLContext</code>），例如<code>createDataFrame</code>等方法就可以通过SparkSession来访问。<br> 在<code>builder.getOrCreate()</code>的过程中，虽然最终得到的是一个SparkSession，但实际上内部已经创建了一个SparkContext，并由这个SparkSession持有。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder() <span class="comment">// 得到一个Builder</span></span><br><span class="line">.master(<span class="string">"local"</span>).appName(<span class="string">"AppName"</span>).config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.getOrCreate() <span class="comment">// 得到一个SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SparkSession.scala</span></span><br><span class="line"><span class="keyword">val</span> sparkContext = userSuppliedContext.getOrElse &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  options.foreach &#123; <span class="keyword">case</span> (k, v) =&gt; sparkConf.set(k, v) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set a random app name if not given.</span></span><br><span class="line">  <span class="keyword">if</span> (!sparkConf.contains(<span class="string">"spark.app.name"</span>)) &#123;</span><br><span class="line">    sparkConf.setAppName(java.util.<span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">SparkContext</span>.getOrCreate(sparkConf)</span><br><span class="line">  <span class="comment">// Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">applyExtensions(</span><br><span class="line">  sparkContext.getConf.get(<span class="type">StaticSQLConf</span>.<span class="type">SPARK_SESSION_EXTENSIONS</span>).getOrElse(<span class="type">Seq</span>.empty),</span><br><span class="line">  extensions)</span><br><span class="line"></span><br><span class="line">session = <span class="keyword">new</span> <span class="type">SparkSession</span>(sparkContext, <span class="type">None</span>, <span class="type">None</span>, extensions)</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h3><p><a href="https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/SparkEnv.html" target="_blank" rel="noopener"><code>SparkEnv</code>持有</a>一个Spark实例在运行时所需要的所有对象，包括Serializer、RpcEndpoint（在早期用的是Akka actor）、BlockManager、MemoryManager、BroadcastManager、SecurityManager、MapOutputTrackerMaster/Worker等等。SparkEnv由SparkContext创建，并在之后通过伴生对象<code>SparkEnv</code>的<code>get</code>方法来访问。在创建时，Driver端的SparkEnv是SparkContext创建的时候调用<code>SparkEnv.createDriverEnv</code>创建的。Executor端的是其守护进程<code>CoarseGrainedExecutorBackend</code>创建的时候调用<code>SparkEnv.createExecutorEnv</code>方法创建的。这两个方法最后都会调用<code>create</code>方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Driver端</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createSparkEnv</span></span>(</span><br><span class="line">    conf: <span class="type">SparkConf</span>,</span><br><span class="line">    isLocal: <span class="type">Boolean</span>,</span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line">  <span class="type">SparkEnv</span>.createDriverEnv(conf, isLocal, listenerBus, <span class="type">SparkContext</span>.numDriverCores(master, conf))</span><br><span class="line">&#125;</span><br><span class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)</span><br><span class="line"><span class="type">SparkEnv</span>.set(_env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Executor端</span></span><br><span class="line"><span class="comment">// CoarseGrainedExecutorBackend.scala</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">SparkEnv</span>.createExecutorEnv(driverConf, arguments.executorId, arguments.bindAddress,</span><br><span class="line">  arguments.hostname, arguments.cores, cfg.ioEncryptionKey, isLocal = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">env.rpcEnv.setupEndpoint(<span class="string">"Executor"</span>, backendCreateFn(env.rpcEnv, arguments, env))</span><br><span class="line">arguments.workerUrl.foreach &#123; url =&gt;</span><br><span class="line">  env.rpcEnv.setupEndpoint(<span class="string">"WorkerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(env.rpcEnv, url))</span><br><span class="line">&#125;</span><br><span class="line">env.rpcEnv.awaitTermination()</span><br><span class="line"></span><br><span class="line"><span class="comment">// SparkEnv.scala</span></span><br><span class="line"><span class="comment">// create函数</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(...)</span><br></pre></td></tr></table></figure>

<h2 id="Spark的任务调度"><a href="#Spark的任务调度" class="headerlink" title="Spark的任务调度"></a>Spark的任务调度</h2><p>Spark的操作可以分为两种，Transform操作是lazy的，而Action操作是Eager的。每一个Action会产生一个Job。<br>Spark的Transform操作可以分为宽依赖(<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/Dependency.scala" target="_blank" rel="noopener"><code>ShuffleDependency</code></a>)和窄依赖(<code>NarrowDependency</code>)操作两种，其中窄依赖还有两个子类<code>OneToOneDependency</code>和<code>RangeDependency</code>。窄依赖操作表示父RDD的每个分区只被子RDD的一个分区所使用，例如<code>union</code>、<code>map</code>、<code>filter</code>等的操作；而宽依赖恰恰相反。宽依赖需要shuffle操作，因为需要将父RDD的结果需要复制给不同节点用来生成子RDD，有关<code>ShuffleDependency</code>将在下面的Shuffle源码分析中详细说明。当DAG的执行中出现宽依赖操作时，Spark会将其前后划分为不同的Stage，在下一章节中将具体分析相关代码。这里需要注意的一点是<code>coalesce</code>这样的操作<a href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank" rel="noopener">也是窄依赖</a>，因为它涉及的输入分区是有限的。</p>
<p>在Stage之下，就是若干个Task了。这些Task也就是Spark的并行单元，通常来说，按照当前Stage的最后一个RDD的分区数来计算，每一个分区都会启动一个Task来进行计算。我们可以通过<code>rdd.partitions.size</code>来获取一个RDD有多少个分区。一般来说，初始的partition数是<a href="https://juejin.im/post/5ca45510f265da30cd184d41" target="_blank" rel="noopener">在HDFS中文件block的数量</a>。</p>
<p>Task具有两种类型，<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala" target="_blank" rel="noopener"><code>ShuffleMapTask</code></a>和<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala" target="_blank" rel="noopener"><code>ResultTask</code></a>。其中<code>ResultTask</code>是<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-scheduler-ResultTask.html" target="_blank" rel="noopener"><code>ResultStage</code></a>的Task，也就是最后一个Stage的Task。</p>
<p>下面提出几个有趣的问题：</p>
<ol>
<li>Job是可并行的么？<br> <a href="http://spark.apache.org/docs/latest/job-scheduling.html" target="_blank" rel="noopener">官网指出</a>，within each Spark application, multiple “jobs” (Spark actions) may be running concurrently if they were submitted by different threads。所以如果你用多线程跑多个Action，确实是可以的。这也是容易理解的，因为跑一个Action相当于就是向Spark的DAGScheduler去提交一个任务嘛。</li>
<li>Stage是可并行的么？<br> <a href="https://stackoverflow.com/questions/41340612/does-stages-in-an-application-run-parallel-in-spark" target="_blank" rel="noopener">可以</a>。Stage描述了宽依赖间的RDD的变化过程，而RDD的变化总体上是一个DAG。因此可以认识到，对于两个<code>NarrowDependency</code>的Stage，它们确实是可以并行的。</li>
</ol>
<h3 id="启动一个任务"><a href="#启动一个任务" class="headerlink" title="启动一个任务"></a>启动一个任务</h3><p>Executor</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">Executor</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    executorId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    executorHostname: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    env: <span class="type">SparkEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    userClassPath: <span class="type">Seq</span>[<span class="type">URL</span>] = <span class="type">Nil</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    isLocal: <span class="type">Boolean</span> = false,</span></span></span><br><span class="line"><span class="class"><span class="params">    uncaughtExceptionHandler: <span class="type">UncaughtExceptionHandler</span> = new <span class="type">SparkUncaughtExceptionHandler</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskDescription)</span><br><span class="line">    runningTasks.put(taskDescription.taskId, tr)</span><br><span class="line">    threadPool.execute(tr)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>TaskRunner</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaskRunner</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    execBackend: <span class="type">ExecutorBackend</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private val taskDescription: <span class="type">TaskDescription</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    threadId = <span class="type">Thread</span>.currentThread.getId</span><br><span class="line">    <span class="type">Thread</span>.currentThread.setName(threadName)</span><br><span class="line">    <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean</span><br><span class="line">    <span class="keyword">val</span> taskMemoryManager = <span class="keyword">new</span> <span class="type">TaskMemoryManager</span>(env.memoryManager, taskId)</span><br><span class="line">    <span class="keyword">val</span> deserializeStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">    <span class="type">Thread</span>.currentThread.setContextClassLoader(replClassLoader)</span><br><span class="line">    <span class="keyword">val</span> ser = env.closureSerializer.newInstance()</span><br><span class="line">    logInfo(<span class="string">s"Running <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>)</span><br><span class="line">    execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">RUNNING</span>, <span class="type">EMPTY_BYTE_BUFFER</span>)</span><br><span class="line">    <span class="keyword">var</span> taskStartTime: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> taskStartCpu: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">    startGCTime = computeTotalGcTime()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// Must be set before updateDependencies() is called, in case fetching dependencies</span></span><br><span class="line">      <span class="comment">// requires access to properties contained within (e.g. for access control).</span></span><br><span class="line">      <span class="type">Executor</span>.taskDeserializationProps.set(taskDescription.properties)</span><br><span class="line"></span><br><span class="line">      updateDependencies(taskDescription.addedFiles, taskDescription.addedJars)</span><br><span class="line">      task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](</span><br><span class="line">        taskDescription.serializedTask, <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">      task.localProperties = taskDescription.properties</span><br><span class="line">      task.setTaskMemoryManager(taskMemoryManager)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If this task has been killed before we deserialized it, let's quit now. Otherwise,</span></span><br><span class="line">      <span class="comment">// continue executing the task.</span></span><br><span class="line">      <span class="keyword">val</span> killReason = reasonIfKilled</span><br><span class="line">      <span class="keyword">if</span> (killReason.isDefined) &#123;</span><br><span class="line">        <span class="comment">// Throw an exception rather than returning, because returning within a try&#123;&#125; block</span></span><br><span class="line">        <span class="comment">// causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl</span></span><br><span class="line">        <span class="comment">// exception will be caught by the catch block, leading to an incorrect ExceptionFailure</span></span><br><span class="line">        <span class="comment">// for the task.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TaskKilledException</span>(killReason.get)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// The purpose of updating the epoch here is to invalidate executor map output status cache</span></span><br><span class="line">      <span class="comment">// in case FetchFailures have occurred. In local mode `env.mapOutputTracker` will be</span></span><br><span class="line">      <span class="comment">// MapOutputTrackerMaster and its cache invalidation is not based on epoch numbers so</span></span><br><span class="line">      <span class="comment">// we don't need to make any special calls here.</span></span><br><span class="line">      <span class="keyword">if</span> (!isLocal) &#123;</span><br><span class="line">        logDebug(<span class="string">"Task "</span> + taskId + <span class="string">"'s epoch is "</span> + task.epoch)</span><br><span class="line">        env.mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerWorker</span>].updateEpoch(task.epoch)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Run the actual task and measure its runtime.</span></span><br><span class="line">      taskStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      taskStartCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">      <span class="keyword">var</span> threwException = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">val</span> value = <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">        <span class="keyword">val</span> res = task.run(</span><br><span class="line">          taskAttemptId = taskId,</span><br><span class="line">          attemptNumber = taskDescription.attemptNumber,</span><br><span class="line">          metricsSystem = env.metricsSystem)</span><br><span class="line">        threwException = <span class="literal">false</span></span><br><span class="line">        res</span><br><span class="line">      &#125; &#123;</span><br><span class="line">        <span class="keyword">val</span> releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)</span><br><span class="line">        <span class="keyword">val</span> freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (freedMemory &gt; <span class="number">0</span> &amp;&amp; !threwException) &#123;</span><br><span class="line">          <span class="keyword">val</span> errMsg = <span class="string">s"Managed memory leak detected; size = <span class="subst">$freedMemory</span> bytes, TID = <span class="subst">$taskId</span>"</span></span><br><span class="line">          <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.unsafe.exceptionOnMemoryLeak"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logWarning(errMsg)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (releasedLocks.nonEmpty &amp;&amp; !threwException) &#123;</span><br><span class="line">          <span class="keyword">val</span> errMsg =</span><br><span class="line">            <span class="string">s"<span class="subst">$&#123;releasedLocks.size&#125;</span> block locks were not released by TID = <span class="subst">$taskId</span>:\n"</span> +</span><br><span class="line">              releasedLocks.mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</span><br><span class="line">          <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.storage.exceptionOnPinLeak"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logInfo(errMsg)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      task.context.fetchFailed.foreach &#123; fetchFailure =&gt;</span><br><span class="line">        <span class="comment">// uh-oh.  it appears the user code has caught the fetch-failure without throwing any</span></span><br><span class="line">        <span class="comment">// other exceptions.  Its *possible* this is what the user meant to do (though highly</span></span><br><span class="line">        <span class="comment">// unlikely).  So we will log an error and keep going.</span></span><br><span class="line">        logError(<span class="string">s"TID <span class="subst">$&#123;taskId&#125;</span> completed successfully though internally it encountered "</span> +</span><br><span class="line">          <span class="string">s"unrecoverable fetch failures!  Most likely this means user code is incorrectly "</span> +</span><br><span class="line">          <span class="string">s"swallowing Spark's internal <span class="subst">$&#123;classOf[FetchFailedException]&#125;</span>"</span>, fetchFailure)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> taskFinish = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> taskFinishCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If the task has been killed, let's fail it.</span></span><br><span class="line">      task.context.killTaskIfInterrupted()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> resultSer = env.serializer.newInstance()</span><br><span class="line">      <span class="keyword">val</span> beforeSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> valueBytes = resultSer.serialize(value)</span><br><span class="line">      <span class="keyword">val</span> afterSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Deserialization happens in two parts: first, we deserialize a Task object, which</span></span><br><span class="line">      <span class="comment">// includes the Partition. Second, Task.run() deserializes the RDD and function to be run.</span></span><br><span class="line">      task.metrics.setExecutorDeserializeTime(</span><br><span class="line">        (taskStartTime - deserializeStartTime) + task.executorDeserializeTime)</span><br><span class="line">      task.metrics.setExecutorDeserializeCpuTime(</span><br><span class="line">        (taskStartCpu - deserializeStartCpuTime) + task.executorDeserializeCpuTime)</span><br><span class="line">      <span class="comment">// We need to subtract Task.run()'s deserialization time to avoid double-counting</span></span><br><span class="line">      task.metrics.setExecutorRunTime((taskFinish - taskStartTime) - task.executorDeserializeTime)</span><br><span class="line">      task.metrics.setExecutorCpuTime(</span><br><span class="line">        (taskFinishCpu - taskStartCpu) - task.executorDeserializeCpuTime)</span><br><span class="line">      task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)</span><br><span class="line">      task.metrics.setResultSerializationTime(afterSerialization - beforeSerialization)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Expose task metrics using the Dropwizard metrics system.</span></span><br><span class="line">      <span class="comment">// Update task metrics counters</span></span><br><span class="line">      executorSource.<span class="type">METRIC_CPU_TIME</span>.inc(task.metrics.executorCpuTime)</span><br><span class="line">      ...</span><br><span class="line">      executorSource.<span class="type">METRIC_MEMORY_BYTES_SPILLED</span>.inc(task.metrics.memoryBytesSpilled)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Note: accumulator updates must be collected after TaskMetrics is updated</span></span><br><span class="line">      <span class="keyword">val</span> accumUpdates = task.collectAccumulatorUpdates()</span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> do not serialize value twice</span></span><br><span class="line">      <span class="keyword">val</span> directResult = <span class="keyword">new</span> <span class="type">DirectTaskResult</span>(valueBytes, accumUpdates)</span><br><span class="line">      <span class="keyword">val</span> serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">      <span class="keyword">val</span> resultSize = serializedDirectResult.limit()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// directSend = sending directly back to the driver</span></span><br><span class="line">      <span class="keyword">val</span> serializedResult: <span class="type">ByteBuffer</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (maxResultSize &gt; <span class="number">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class="line">          logWarning(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). Result is larger than maxResultSize "</span> +</span><br><span class="line">            <span class="string">s"(<span class="subst">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class="subst">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), "</span> +</span><br><span class="line">            <span class="string">s"dropping it."</span>)</span><br><span class="line">          ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](<span class="type">TaskResultBlockId</span>(taskId), resultSize))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class="line">          <span class="keyword">val</span> blockId = <span class="type">TaskResultBlockId</span>(taskId)</span><br><span class="line">          env.blockManager.putBytes(</span><br><span class="line">            blockId,</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),</span><br><span class="line">            <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">          logInfo(</span><br><span class="line">            <span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent via BlockManager)"</span>)</span><br><span class="line">          ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](blockId, resultSize))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logInfo(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent to driver"</span>)</span><br><span class="line">          serializedDirectResult</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FINISHED</span>, serializedResult)</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">TaskKilledException</span> =&gt;</span><br><span class="line">        logInfo(<span class="string">s"Executor killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>), reason: <span class="subst">$&#123;t.reason&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTime)</span><br><span class="line">        <span class="keyword">val</span> serializedTK = ser.serialize(<span class="type">TaskKilled</span>(t.reason, accUpdates, accums))</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, serializedTK)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">InterruptedException</span> | <span class="type">NonFatal</span>(_) <span class="keyword">if</span></span><br><span class="line">          task != <span class="literal">null</span> &amp;&amp; task.reasonIfKilled.isDefined =&gt;</span><br><span class="line">        <span class="keyword">val</span> killReason = task.reasonIfKilled.getOrElse(<span class="string">"unknown reason"</span>)</span><br><span class="line">        logInfo(<span class="string">s"Executor interrupted and killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>), reason: <span class="subst">$killReason</span>"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTime)</span><br><span class="line">        <span class="keyword">val</span> serializedTK = ser.serialize(<span class="type">TaskKilled</span>(killReason, accUpdates, accums))</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, serializedTK)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> <span class="keyword">if</span> hasFetchFailure &amp;&amp; !<span class="type">Utils</span>.isFatalError(t) =&gt;</span><br><span class="line">        <span class="keyword">val</span> reason = task.context.fetchFailed.get.toTaskFailedReason</span><br><span class="line">        <span class="keyword">if</span> (!t.isInstanceOf[<span class="type">FetchFailedException</span>]) &#123;</span><br><span class="line">          <span class="comment">// there was a fetch failure in the task, but some user code wrapped that exception</span></span><br><span class="line">          <span class="comment">// and threw something else.  Regardless, we treat it as a fetch failure.</span></span><br><span class="line">          <span class="keyword">val</span> fetchFailedCls = classOf[<span class="type">FetchFailedException</span>].getName</span><br><span class="line">          logWarning(<span class="string">s"TID <span class="subst">$&#123;taskId&#125;</span> encountered a <span class="subst">$&#123;fetchFailedCls&#125;</span> and "</span> +</span><br><span class="line">            <span class="string">s"failed, but the <span class="subst">$&#123;fetchFailedCls&#125;</span> was hidden by another "</span> +</span><br><span class="line">            <span class="string">s"exception.  Spark is handling this like a fetch failure and ignoring the "</span> +</span><br><span class="line">            <span class="string">s"other exception: <span class="subst">$t</span>"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">CausedBy</span>(cDE: <span class="type">CommitDeniedException</span>) =&gt;</span><br><span class="line">        <span class="keyword">val</span> reason = cDE.toTaskCommitDeniedReason</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// Attempt to exit cleanly by informing the driver of our failure.</span></span><br><span class="line">        <span class="comment">// If anything goes wrong (or this was a fatal exception), we will delegate to</span></span><br><span class="line">        <span class="comment">// the default uncaught exception handler, which will terminate the Executor.</span></span><br><span class="line">        logError(<span class="string">s"Exception in <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// SPARK-20904: Do not report failure to driver if if happened during shut down. Because</span></span><br><span class="line">        <span class="comment">// libraries may set up shutdown hooks that race with running tasks during shutdown,</span></span><br><span class="line">        <span class="comment">// spurious failures may occur and can result in improper accounting in the driver (e.g.</span></span><br><span class="line">        <span class="comment">// the task failure would not be ignored if the shutdown happened because of premption,</span></span><br><span class="line">        <span class="comment">// instead of an app issue).</span></span><br><span class="line">        <span class="keyword">if</span> (!<span class="type">ShutdownHookManager</span>.inShutdown()) &#123;</span><br><span class="line">          <span class="keyword">val</span> (accums, accUpdates) = collectAccumulatorsAndResetStatusOnFailure(taskStartTime)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> serializedTaskEndReason = &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates).withAccums(accums))</span><br><span class="line">            &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">              <span class="keyword">case</span> _: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">                <span class="comment">// t is not serializable so just send the stacktrace</span></span><br><span class="line">                ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates, <span class="literal">false</span>).withAccums(accums))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">          execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, serializedTaskEndReason)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logInfo(<span class="string">"Not reporting error to driver during JVM shutdown."</span>)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Don't forcibly exit unless the exception was inherently fatal, to avoid</span></span><br><span class="line">        <span class="comment">// stopping other tasks unnecessarily.</span></span><br><span class="line">        <span class="keyword">if</span> (!t.isInstanceOf[<span class="type">SparkOutOfMemoryError</span>] &amp;&amp; <span class="type">Utils</span>.isFatalError(t)) &#123;</span><br><span class="line">          uncaughtExceptionHandler.uncaughtException(<span class="type">Thread</span>.currentThread(), t)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      runningTasks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">hasFetchFailure</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    task != <span class="literal">null</span> &amp;&amp; task.context != <span class="literal">null</span> &amp;&amp; task.context.fetchFailed.isDefined</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Task</span>[<span class="type">T</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val stageId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val stageAttemptId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val partitionId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    @transient var localProperties: <span class="type">Properties</span> = new <span class="type">Properties</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    // <span class="type">The</span> default value is only used in tests.</span></span></span><br><span class="line"><span class="class"><span class="params">    serializedTaskMetrics: <span class="type">Array</span>[<span class="type">Byte</span>] =</span></span></span><br><span class="line"><span class="class"><span class="params">      <span class="type">SparkEnv</span>.get.closureSerializer.newInstance(</span>).<span class="title">serialize</span>(<span class="params"><span class="type">TaskMetrics</span>.registered</span>).<span class="title">array</span>(<span class="params"></span>),</span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">jobId</span></span>: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span>,</span><br><span class="line">    <span class="keyword">val</span> appId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>,</span><br><span class="line">    <span class="keyword">val</span> appAttemptId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>,</span><br><span class="line">    <span class="keyword">val</span> isBarrier: <span class="type">Boolean</span> = <span class="literal">false</span>) <span class="keyword">extends</span> <span class="type">Serializable</span> &#123;</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called by [[org.apache.spark.executor.Executor]] to run this task.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param taskAttemptId an identifier for this task attempt that is unique within a SparkContext.</span></span><br><span class="line"><span class="comment">   * @param attemptNumber how many times this task has been attempted (0 for the first attempt)</span></span><br><span class="line"><span class="comment">   * @return the result of the task along with updates of Accumulators.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(</span><br><span class="line">      taskAttemptId: <span class="type">Long</span>,</span><br><span class="line">      attemptNumber: <span class="type">Int</span>,</span><br><span class="line">      metricsSystem: <span class="type">MetricsSystem</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    <span class="type">SparkEnv</span>.get.blockManager.registerTask(taskAttemptId)</span><br><span class="line">    <span class="comment">// TODO SPARK-24874 Allow create BarrierTaskContext based on partitions, instead of whether</span></span><br><span class="line">    <span class="comment">// the stage is barrier.</span></span><br><span class="line">    <span class="keyword">val</span> taskContext = <span class="keyword">new</span> <span class="type">TaskContextImpl</span>(</span><br><span class="line">      stageId,</span><br><span class="line">      stageAttemptId, <span class="comment">// stageAttemptId and stageAttemptNumber are semantically equal</span></span><br><span class="line">      partitionId,</span><br><span class="line">      taskAttemptId,</span><br><span class="line">      attemptNumber,</span><br><span class="line">      taskMemoryManager,</span><br><span class="line">      localProperties,</span><br><span class="line">      metricsSystem,</span><br><span class="line">      metrics)</span><br><span class="line"></span><br><span class="line">    context = <span class="keyword">if</span> (isBarrier) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BarrierTaskContext</span>(taskContext)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      taskContext</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">InputFileBlockHolder</span>.initialize()</span><br><span class="line">    <span class="type">TaskContext</span>.setTaskContext(context)</span><br><span class="line">    taskThread = <span class="type">Thread</span>.currentThread()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (_reasonIfKilled != <span class="literal">null</span>) &#123;</span><br><span class="line">      kill(interruptThread = <span class="literal">false</span>, _reasonIfKilled)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">new</span> <span class="type">CallerContext</span>(</span><br><span class="line">      <span class="string">"TASK"</span>,</span><br><span class="line">      <span class="type">SparkEnv</span>.get.conf.get(<span class="type">APP_CALLER_CONTEXT</span>),</span><br><span class="line">      appId,</span><br><span class="line">      appAttemptId,</span><br><span class="line">      jobId,</span><br><span class="line">      <span class="type">Option</span>(stageId),</span><br><span class="line">      <span class="type">Option</span>(stageAttemptId),</span><br><span class="line">      <span class="type">Option</span>(taskAttemptId),</span><br><span class="line">      <span class="type">Option</span>(attemptNumber)).setCurrentContext()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      runTask(context)</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// Catch all errors; run task failure callbacks, and rethrow the exception.</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          context.markTaskFailed(e)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">            e.addSuppressed(t)</span><br><span class="line">        &#125;</span><br><span class="line">        context.markTaskCompleted(<span class="type">Some</span>(e))</span><br><span class="line">        <span class="keyword">throw</span> e</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// Call the task completion callbacks. If "markTaskCompleted" is called twice, the second</span></span><br><span class="line">        <span class="comment">// one is no-op.</span></span><br><span class="line">        context.markTaskCompleted(<span class="type">None</span>)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">            <span class="comment">// Release memory used by this thread for unrolling blocks</span></span><br><span class="line">            <span class="type">SparkEnv</span>.get.blockManager.memoryStore.releaseUnrollMemoryForThisTask(<span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br><span class="line">            <span class="type">SparkEnv</span>.get.blockManager.memoryStore.releaseUnrollMemoryForThisTask(</span><br><span class="line">              <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)</span><br><span class="line">            <span class="comment">// Notify any tasks waiting for execution memory to be freed to wake up and try to</span></span><br><span class="line">            <span class="comment">// acquire memory again. This makes impossible the scenario where a task sleeps forever</span></span><br><span class="line">            <span class="comment">// because there are no other tasks left to notify it. Since this is safe to do but may</span></span><br><span class="line">            <span class="comment">// not be strictly necessary, we should revisit whether we can remove this in the</span></span><br><span class="line">            <span class="comment">// future.</span></span><br><span class="line">            <span class="keyword">val</span> memoryManager = <span class="type">SparkEnv</span>.get.memoryManager</span><br><span class="line">            memoryManager.synchronized &#123; memoryManager.notifyAll() &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          <span class="comment">// Though we unset the ThreadLocal here, the context member variable itself is still</span></span><br><span class="line">          <span class="comment">// queried directly in the TaskRunner to check for FetchFailedExceptions.</span></span><br><span class="line">          <span class="type">TaskContext</span>.unset()</span><br><span class="line">          <span class="type">InputFileBlockHolder</span>.unset()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<h3 id="失败重试"><a href="#失败重试" class="headerlink" title="失败重试"></a>失败重试</h3><ol>
<li><code>spark.yarn.maxAppAttempts</code><br> YARN申请资源的重试次数。</li>
<li><code>spark.yarn.max.executor.failures</code><br> Spark应用程序的最大Executor失败次数，默认<code>numExecutors*2</code>。</li>
</ol>
<h2 id="Spark的存储管理"><a href="#Spark的存储管理" class="headerlink" title="Spark的存储管理"></a>Spark的存储管理</h2><p>为了实现与底层细节的解耦，Spark的存储基于BlockManager给计算部分提供服务。类似于Driver和Executor，BlockManager机制也分为BlockManagerMaster和BlockManager。Driver上的BlockManagerMaster对于存在与Executor上的BlockManager统一管理。BlockManager只是负责管理所在Executor上的Block。<br>BlockManagerMaster和BlockManager都是在SparkEnv中创建的，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Mapping from block manager id to the block manager's information.</span></span><br><span class="line"><span class="keyword">val</span> blockManagerInfo = <span class="keyword">new</span> concurrent.<span class="type">TrieMap</span>[<span class="type">BlockManagerId</span>, <span class="type">BlockManagerInfo</span>]()</span><br><span class="line"><span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(</span><br><span class="line">      rpcEnv,</span><br><span class="line">      isLocal,</span><br><span class="line">      conf,</span><br><span class="line">      listenerBus,</span><br><span class="line">      <span class="comment">// 是否使用ExternalShuffleService读取持久化在磁盘上的数据</span></span><br><span class="line">      <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_SERVICE_FETCH_RDD_ENABLED</span>)) &#123;</span><br><span class="line">        externalShuffleClient</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;, blockManagerInfo)),</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_HEARTBEAT_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterHeartbeatEndpoint</span>(rpcEnv, isLocal, blockManagerInfo)),</span><br><span class="line">  conf,</span><br><span class="line">  isDriver)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> blockTransferService =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">NettyBlockTransferService</span>(conf, securityManager, bindAddress, advertiseAddress,</span><br><span class="line">    blockManagerPort, numUsableCores, blockManagerMaster.driverEndpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  blockManagerMaster,</span><br><span class="line">  serializerManager,</span><br><span class="line">  conf,</span><br><span class="line">  memoryManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  securityManager,</span><br><span class="line">  externalShuffleClient)</span><br></pre></td></tr></table></figure>

<p>Driver节点和Executor节点的BlockManager之间的交互可以使用下图来描述，在此就不详细说明。<br><img src="/img/sparksql/blockmanager.png"></p>
<h3 id="BlockId和BlockInfo"><a href="#BlockId和BlockInfo" class="headerlink" title="BlockId和BlockInfo"></a>BlockId和BlockInfo</h3><p>抽象类<code>BlockId</code>被用来唯一标识一个Block，具有全局唯一的名字，通常和一个文件相对应。<code>BlockId</code>有着确定的命名规则，并且和它实际的类型有关。<br>如果它是用来Shuffle的<code>ShuffleBlockId</code>，那么他的命名就是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">String</span> = <span class="string">"shuffle_"</span> + shuffleId + <span class="string">"_"</span> + mapId + <span class="string">"_"</span> + reduceId</span><br></pre></td></tr></table></figure>

<p>抑或它是用来Broadcast的<code>BroadcastBlockId</code>，他的命名就是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"broadcast_"</span> + broadcastId + (<span class="keyword">if</span> (field == <span class="string">""</span>) <span class="string">""</span> <span class="keyword">else</span> <span class="string">"_"</span> + field)</span><br></pre></td></tr></table></figure>

<p>或者它是一个RDD，它的命名就是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"rdd_"</span> + rddId + <span class="string">"_"</span> + splitIndex</span><br></pre></td></tr></table></figure>

<p>通过在Spark.log里面跟踪这些block名字，我们可以了解到当前Spark任务的执行和存储情况。</p>
<p><code>BlockInfo</code>中的<code>level</code>项表示这个block的存储级别。</p>
<figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockInfoManager.scala</span></span><br><span class="line"><span class="keyword">private</span>[storage] <span class="class"><span class="keyword">class</span> <span class="title">BlockInfo</span></span>(</span><br><span class="line">    <span class="keyword">val</span> level: StorageLevel,</span><br><span class="line">    <span class="keyword">val</span> classTag: ClassTag[_],</span><br><span class="line">    <span class="keyword">val</span> tellMaster: <span class="built_in">Boolean</span>) &#123;</span><br></pre></td></tr></table></figure>

<h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><p>Spark提供了如下的持久化级别，其中选项为<code>useDisk</code>、<code>useMemory</code>、<code>useOffHeap</code>、<code>deserialized</code>、<code>replication</code>，分别表示是否采用磁盘、内存、堆外内存、反序列化以及持久化维护的副本数。其中反序列化为false时（好绕啊），会对对象进行序列化存储，能够节省一定空间，但同时会消耗计算资源。需要注意的是，<code>cache</code>操作是<code>persist</code>的一个特例，等于<code>MEMORY_ONLY</code>的persist。所有的广播对象都是<code>MEMORY_AND_DISK</code>的存储级别</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> <span class="keyword">extends</span> <span class="title">scala</span>.<span class="title">AnyRef</span> <span class="keyword">with</span> <span class="title">scala</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>) <span class="comment">// 默认存储类别</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>想在Spark任务完成之后检查每一个RDD的缓存状况是比较困难的，虽然在Spark EventLog中，我们也能看到在每一个RDD的RDD Info中有一个StorageLevel的条目。<code>RDDInfo</code>的源码建议我们可以通过<code>(Use Disk||Use Memory)&amp;&amp;NumberofCachedPartitions</code>这样的条件来判断一个RDD到底有没有被cache。但实际上，似乎EventLog里面的<code>NumberofCachedPartitions</code>、<code>Memory Size</code>、<code>Disk Size</code>永远是0，这可能是只能在执行过程中才能看到这些字段的值，毕竟WebUI的Storage标签就只在执行时能看到。不过<code>(Use Disk||Use Memory)</code>在cache调用的RDD上是true的，所以可以以这个RDD为根做一个BFS，将所有不需要计算的RDD找出来。</p>
<h3 id="Checkpoint"><a href="#Checkpoint" class="headerlink" title="Checkpoint"></a>Checkpoint</h3><h3 id="Save"><a href="#Save" class="headerlink" title="Save"></a>Save</h3><p>相对于持久化，这里指的是保存数据到文件或者数据表。</p>
<h4 id="RDD的overwrite问题"><a href="#RDD的overwrite问题" class="headerlink" title="RDD的overwrite问题"></a>RDD的overwrite问题</h4><p>一个蛋疼的事情是RDD的诸如<code>saveAsTextFile</code>不能够像DF的API一样直接指定<code>overwrite</code>为true，导致无法复写的情况。为此，需要借助于hdfs的API手动来判断是否exist。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> org.apache.hadoop.conf.<span class="type">Configuration</span>()</span><br><span class="line"><span class="comment">// 注意，可能需要手动指定集群，因为Spark的默认集群可能不对，</span></span><br><span class="line"><span class="comment">// 届时可能产生Wrong FS的错误</span></span><br><span class="line"><span class="keyword">val</span> fs = <span class="type">FileSystem</span>.get(<span class="keyword">new</span> java.net.<span class="type">URI</span>(<span class="string">"hdfs根"</span>), conf)</span><br><span class="line"><span class="comment">// val fs = FileSystem.get(sc.hadoopConfiguration)</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="keyword">new</span> org.apache.hadoop.fs.<span class="type">Path</span>(<span class="string">"需要删除的目录"</span>)</span><br><span class="line"><span class="keyword">if</span> (fs.exists(path))&#123;</span><br><span class="line">  fs.delete(path, <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="RDD的save问题"><a href="#RDD的save问题" class="headerlink" title="RDD的save问题"></a>RDD的save问题</h4><p><code>saveAsTextFile</code>会写到多个文件里面，如下所示，如果我们save到这个文件夹，那么会在下面创建<code>_SUCCESS</code>、<code>part-000000</code>这样的文件<br><img src="/img/sparksql/rdd.saveas.png"><br>那么我们读的时候，比较方便的是用下面的方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.read.text(path).rdd.collect().mkString(<span class="string">""</span>)</span><br></pre></td></tr></table></figure>

<p>但这样会导致读出来的string周围有中括号包起来。因此要用下面的办法去掉</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="keyword">if</span>(textRaw.endsWith(<span class="string">"]"</span>))&#123;</span><br><span class="line">  textRaw.substring(<span class="number">1</span>, textRaw.length - <span class="number">1</span>)</span><br><span class="line">&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">  textRaw</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>不过，我们有另一种办法，就是绕过spark，而直接用hadoop的api来读取。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">Configuration</span></span><br><span class="line"><span class="keyword">val</span> path = <span class="keyword">new</span> <span class="type">Path</span>(fileName)</span><br><span class="line"><span class="keyword">val</span> fileSystem = path.getFileSystem(conf)</span><br><span class="line"><span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">BufferedWriter</span>(<span class="keyword">new</span> <span class="type">OutputStreamWriter</span>(fileSystem.create(path)))</span><br><span class="line">writer.write(s)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h3 id="BlockInfoManager"><a href="#BlockInfoManager" class="headerlink" title="BlockInfoManager"></a>BlockInfoManager</h3><p><code>BlockInfoManager</code>用来管理Block的元信息，例如它维护了所有BlockId的BlockInfo信息<code>infos: mutable.HashMap[BlockId, BlockInfo]</code>。不过它最主要的功能还是为读写Block提供锁服务</p>
<h3 id="本地读Block"><a href="#本地读Block" class="headerlink" title="本地读Block"></a>本地读Block</h3><p>本地读方法位于BlockManager.scala中，从前叫<code>getBlockData</code>，现在叫<code>getLocalBlockData</code>，名字更易懂了。<code>getLocalBlockData</code>的主要内容就对Block的性质进行讨论，如果是Shuffle的，那么就借助于<code>ShuffleBlockResolver</code>。<br><code>ShuffleBlockResolver</code>是一个trait，它有两个子类<code>IndexShuffleBlockResolver</code>和<code>ExternalShuffleBlockResolver</code>，它们定义如何从一个logical shuffle block identifier（例如map、reduce或shuffle）中取回Block。这个类维护Block和文件的映射关系，维护index文件，向<code>BlockStore</code>提供抽象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockManager.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getLocalBlockData</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (blockId.isShuffle) &#123;</span><br><span class="line">    <span class="comment">// 如果这个BlockId是Shuffle的，那么就通过shuffleManager的shuffleBlockResolver来获取BlockData</span></span><br><span class="line">    shuffleManager.shuffleBlockResolver.getBlockData(blockId)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 否则使用getLocalBytes</span></span><br><span class="line">    getLocalBytes(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(blockData) =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BlockManagerManagedBuffer</span>(blockInfoManager, blockId, blockData, <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// If this block manager receives a request for a block that it doesn't have then it's</span></span><br><span class="line">        <span class="comment">// likely that the master has outdated block statuses for this block. Therefore, we send</span></span><br><span class="line">        <span class="comment">// an RPC so that this block is marked as being unavailable from this block manager.</span></span><br><span class="line">        reportBlockStatus(blockId, <span class="type">BlockStatus</span>.empty)</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BlockNotFoundException</span>(blockId.toString)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们看<code>getLocalBytes</code>函数，它带锁地调用<code>doGetLocalBytes</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLocalBytes</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Option</span>[<span class="type">BlockData</span>] = &#123;</span><br><span class="line">  logDebug(<span class="string">s"Getting local block <span class="subst">$blockId</span> as bytes"</span>)</span><br><span class="line">  assert(!blockId.isShuffle, <span class="string">s"Unexpected ShuffleBlockId <span class="subst">$blockId</span>"</span>)</span><br><span class="line">  blockInfoManager.lockForReading(blockId).map &#123; info =&gt; doGetLocalBytes(blockId, info) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的这一段代码会在spark.log中产生类似下面的Log，我们由此可以对Block的用途，存储级别等进行分析。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">19/11/26 17:24:52 DEBUG BlockManager: Getting local block broadcast_3_piece0 as bytes</span><br><span class="line">19/11/26 17:24:52 TRACE BlockInfoManager: Task -1024 trying to acquire read lock for broadcast_3_piece0</span><br><span class="line">19/11/26 17:24:52 TRACE BlockInfoManager: Task -1024 acquired read lock for broadcast_3_piece0</span><br><span class="line">19/11/26 17:24:52 DEBUG BlockManager: Level for block broadcast_3_piece0 is StorageLevel(disk, memory, 1 replicas)</span><br></pre></td></tr></table></figure>

<p><code>doGetLocalBytes</code>负责根据Block的存储级别，以最小的代价取到序列化后的数据。从下面的代码中可以看到，Spark认为序列化一个对象的开销是高于从磁盘中读取一个已经序列化之后的对象的开销的，因为它宁可从磁盘里面取也不愿意直接从内存序列化。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doGetLocalBytes</span></span>(blockId: <span class="type">BlockId</span>, info: <span class="type">BlockInfo</span>): <span class="type">BlockData</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> level = info.level</span><br><span class="line">  logDebug(<span class="string">s"Level for block <span class="subst">$blockId</span> is <span class="subst">$level</span>"</span>)</span><br><span class="line">  <span class="comment">// 如果内容是序列化的，先尝试读序列化的到内存和磁盘。</span></span><br><span class="line">  <span class="comment">// 如果内容是非序列化的，尝试序列化内存中的对象，最后抛出异常表示不存在</span></span><br><span class="line">  <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">    <span class="comment">// 因为内存中是非序列化的，尝试能不能先从磁盘中读到非序列化的。</span></span><br><span class="line">    <span class="keyword">if</span> (level.useDisk &amp;&amp; diskStore.contains(blockId)) &#123;</span><br><span class="line">      <span class="comment">// Note: Spark在这里故意不将block放到内存里面，因为这个if分支是处理非序列化块的，</span></span><br><span class="line">      <span class="comment">// 这个块可能被按照非序列化对象的形式存在内存里面，因此没必要在在内存里面存一份序列化了的。</span></span><br><span class="line">      diskStore.getBytes(blockId)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useMemory &amp;&amp; memoryStore.contains(blockId)) &#123;</span><br><span class="line">      <span class="comment">// 不在硬盘上，就序列化内存中的对象</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">ByteBufferBlockData</span>(serializerManager.dataSerializeWithExplicitClassTag(</span><br><span class="line">        blockId, memoryStore.getValues(blockId).get, info.classTag), <span class="literal">true</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      handleLocalReadFailure(blockId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果存在已经序列化的对象</span></span><br><span class="line">    <span class="keyword">if</span> (level.useMemory &amp;&amp; memoryStore.contains(blockId)) &#123;</span><br><span class="line">      <span class="comment">// 先找内存</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">ByteBufferBlockData</span>(memoryStore.getBytes(blockId).get, <span class="literal">false</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk &amp;&amp; diskStore.contains(blockId)) &#123;</span><br><span class="line">      <span class="comment">// 再找磁盘</span></span><br><span class="line">      <span class="keyword">val</span> diskData = diskStore.getBytes(blockId)</span><br><span class="line">      maybeCacheDiskBytesInMemory(info, blockId, level, diskData)</span><br><span class="line">        .map(<span class="keyword">new</span> <span class="type">ByteBufferBlockData</span>(_, <span class="literal">false</span>))</span><br><span class="line">        .getOrElse(diskData)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      handleLocalReadFailure(blockId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spark的内存管理"><a href="#Spark的内存管理" class="headerlink" title="Spark的内存管理"></a>Spark的内存管理</h2><p>在Spark 1.6之后，内存管理模式发生了大变化，从前版本的内存管理需要通过指定<code>spark.memory.useLegacyMode</code>来手动启用，因此在这里只对之后的进行论述。</p>
<h3 id="Spark内存布局"><a href="#Spark内存布局" class="headerlink" title="Spark内存布局"></a>Spark内存布局</h3><p>如下图所示，Spark的堆内存空间可以分为Spark托管区、用户区和保留区三块。<br><img src="/img/sparksql/mem_layout_16.png"><br>其中保留区占300MB，是固定的。托管区的大小由<code>spark.memory.fraction</code>节制，而<code>1 - spark.memory.fraction</code>的部分用户区。这个值越小，就越容易Spill或者Cache evict。这个设置的用途是将internal metadata、user data structures区分开来。从而减少对<em>稀疏的或者不常出现的大对象的大小</em>的不准确估计造成的影响（限定词有点多，是翻译的注释、、、）。默认<code>spark.memory.fraction</code>是0.6。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// package.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> <span class="type">MEMORY_FRACTION</span> = <span class="type">ConfigBuilder</span>(<span class="string">"spark.memory.fraction"</span>)</span><br><span class="line">  .doc(<span class="string">"..."</span>).doubleConf.createWithDefault(<span class="number">0.6</span>)</span><br></pre></td></tr></table></figure>

<p>Spark的托管区又分为Execution和Storage两个部分。其中Storage主要用来缓存RDD、Broadcast之类的对象，Execution被用来存Mapside的Shuffle数据。Storage和Execution共享的内存，<code>spark.storage.storageFraction</code>（现在应该已经改成了<code>spark.memory.storageFraction</code>）表示对eviction免疫的Storage部分的大小，它的值越大，Execution内存就越小，Task就越容易Spill。反之，Cache就越容易被evict。默认<code>spark.memory.storageFraction</code>是0.5。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// package.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">val</span> <span class="type">MEMORY_STORAGE_FRACTION</span> = <span class="type">ConfigBuilder</span>(<span class="string">"spark.memory.storageFraction"</span>)</span><br><span class="line">  .doc(<span class="string">"..."</span>).doubleConf.checkValue(v =&gt; v &gt;= <span class="number">0.0</span> &amp;&amp; v &lt; <span class="number">1.0</span>, <span class="string">"Storage fraction must be in [0,1)"</span>).createWithDefault(<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p>Storage可以借用任意多的Execution内存，直到Execution重新要回。此时被Cache的块会被从内存中evict掉（具体如何evict，根据每个Block的存储级别）。Execution也可以借用任意多的Storage的，但是Execution的借用不能被Storage驱逐，原因是因为实现起来很复杂。我们在稍后将看到，Spark没有一个统一的资源分配的入口。</p>
<p>除了堆内内存，Spark还可以使用堆外内存。为什么要有这个东西呢？原因是提高内存使用率、提高Shuffle时排序的效率等。由于Spark任务的性质，使用堆外内存能够更精细化地管理，而不需要通过JVM里面的GC，并且序列化数据的占用空间也可以被精确计算。此外，序列化也能节省内存开销。堆外内存在Spark 2.0之后由Tachyon迁移到了JDK Unsafe API实现。可通过配置<code>spark.memory.offHeap.enabled</code>参数启用堆外内存，并由<code>spark.memory.offHeap.size</code>参数设定堆外空间的大小。除了没有other 空间，堆外内存与堆内内存的划分方式相同，所有运行中的并发任务共享存储内存和执行内存。</p>
<h3 id="MemoryManager"><a href="#MemoryManager" class="headerlink" title="MemoryManager"></a>MemoryManager</h3><p>Spark中负责文件管理的类是<code>MemoryManager</code>，它是一个抽象类，被<code>SparkEnv</code>持有。在1.6版本后引入的<code>UnifiedMemoryManager</code>是它的一个实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkEnv.scala</span></span><br><span class="line"><span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> = <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)</span><br></pre></td></tr></table></figure>

<p><code>UnifiedMemoryManager</code>实现了诸如<code>acquireExecutionMemory</code>等方法来分配内存。通过在<code>acquireExecutionMemory</code>时传入一个<code>MemoryMode</code>可以告知是从堆内请求还是从堆外请求。需要注意的是，这类的函数并不像<code>malloc</code>一样直接去请求一段内存，并返回内存的地址，而是全局去维护每个Task所使用的内存大小。每一个Task在申请内存（new对象）之前都会去检查一下自己有没有超标，否则就去Spill。也就是说<code>MemoryManager</code>实际上是一个外挂式的内存管理系统，它不实际上托管内存，整个内存还是由JVM管理的。<br>对Task的Execution内存使用进行跟踪的这个机制被实现<code>ExecutionMemoryPool</code>中，如下面的代码所示。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionMemoryPool.scala </span></span><br><span class="line"><span class="comment">// 保存每一个Task所占用的内存大小</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> memoryForTask = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Long</span>, <span class="type">Long</span>]()</span><br></pre></td></tr></table></figure>

<p>当然，有<code>ExecutionMemoryPool</code>就也有<code>StorageMemoryPool</code>，他们都不出所料继承了<code>MemoryPool</code>。而以上这些Pool最后都被<code>MemoryManager</code>所持有。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// MemoryManager.scala</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> onHeapStorageMemoryPool = <span class="keyword">new</span> <span class="type">StorageMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> offHeapStorageMemoryPool = <span class="keyword">new</span> <span class="type">StorageMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)</span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> onHeapExecutionMemoryPool = <span class="keyword">new</span> <span class="type">ExecutionMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> offHeapExecutionMemoryPool = <span class="keyword">new</span> <span class="type">ExecutionMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)</span><br></pre></td></tr></table></figure>

<h3 id="请求内存的流程"><a href="#请求内存的流程" class="headerlink" title="请求内存的流程"></a>请求内存的流程</h3><p>我们知道，在Shuffle操作中有两个内存使用大户<code>ExecutorSorter</code>和<code>ExternalAppendOnlyMap</code>，都继承了<code>Spillable</code>，从而实现了在内存不足时进行Spill。我们查看对应的<code>maybeSpill</code>方法，它调用了自己父类<code>MemoryConsumer</code>中的<code>acquireExecutionMemory</code>方法。由于从代码注释上看<strong>似乎</strong><code>MemoryConsumer</code>包括它引用到的<code>TaskMemoryManager</code>类都与Tungsten有关，所以我们将在稍后进行研究。目前只是列明调用过程，因为如果其中涉及要向Spark托管内存请求分配，最终调用的还是<code>UnifiedMemoryManager</code>中的对应方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spillable.scala</span></span><br><span class="line"><span class="comment">// 在maybeSpill方法中</span></span><br><span class="line"><span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryConsumer.scala</span></span><br><span class="line">public long acquireMemory(long size) &#123;</span><br><span class="line">  long granted = taskMemoryManager.acquireExecutionMemory(size, <span class="keyword">this</span>);</span><br><span class="line">  used += granted;</span><br><span class="line">  <span class="keyword">return</span> granted;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// TaskMemoryManager.java</span></span><br><span class="line">public long acquireExecutionMemory(long required, <span class="type">MemoryConsumer</span> consumer) &#123;</span><br><span class="line">  assert(required &gt;= <span class="number">0</span>);</span><br><span class="line">  assert(consumer != <span class="literal">null</span>);</span><br><span class="line">  <span class="type">MemoryMode</span> mode = consumer.getMode();</span><br><span class="line">  synchronized (<span class="keyword">this</span>) &#123;</span><br><span class="line">    long got = memoryManager.acquireExecutionMemory(required, taskAttemptId, mode);</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Executor.scala</span></span><br><span class="line"><span class="comment">// TaskMemoryManager中的memoryManager，其实就是一个UnifiedMemoryManager</span></span><br><span class="line"><span class="keyword">val</span> taskMemoryManager = <span class="keyword">new</span> <span class="type">TaskMemoryManager</span>(env.memoryManager, taskId)</span><br></pre></td></tr></table></figure>

<p>下面，我们来看<code>acquireExecutionMemory</code>的详细实现。它前面会首先根据<code>memoryMode</code>选择使用的<code>MemoryPool</code>，是堆内的，还是堆外的。然后它会有个函数<code>maybeGrowExecutionPool</code>，用来处理在需要的情况下从Storage部分挤占一些内存回来。我们可以在稍后详看这个方法。现在，我们发现<code>acquireExecutionMemory</code>会往对应的<code>MemoryPool</code>发一个调用<code>acquireMemory</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UnifiedMemoryManager.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="keyword">private</span>[memory] <span class="function"><span class="keyword">def</span> <span class="title">acquireExecutionMemory</span></span>(</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 实际上是一个ExecutionMemoryPool</span></span><br><span class="line">  executionPool.acquireMemory(</span><br><span class="line">    numBytes, taskAttemptId, maybeGrowExecutionPool, () =&gt; computeMaxExecutionPoolSize)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MemoryManager.scala</span></span><br><span class="line"><span class="meta">@GuardedBy</span>(<span class="string">"this"</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> onHeapExecutionMemoryPool = <span class="keyword">new</span> <span class="type">ExecutionMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br></pre></td></tr></table></figure>

<p>由于我们讨论的场景就是请求堆内的执行内存，所以就进入<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/memory/ExecutionMemoryPool.scala" target="_blank" rel="noopener">ExecutionMemoryPool.scala</a>查看相关代码。在Spark中，会尝试保证每个Task能够得到合理份额的内存，而不是让某些Task的内存持续增大到一定的数量，然后导致其他人持续地Spill到Disk。<br>如果有N个任务，那么保证每个Task在Spill前可以获得至少<code>1 / 2N</code>的内存，并且最多只能获得<code>1 / N</code>。因为<code>N</code>是持续变化的，所以我们需要跟踪活跃Task集合，并且持续在等待Task集合中更新<code>1 / 2N</code>和<code>1 / N</code>的值。这个是借助于同步机制实现的，在1.6之前，是由<code>ShuffleMemoryManager</code>来仲裁的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExecutionMemoryPool.scala </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 保存每一个Task所占用的内存大小</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> memoryForTask = <span class="keyword">new</span> mutable.<span class="type">HashMap</span>[<span class="type">Long</span>, <span class="type">Long</span>]()</span><br><span class="line"><span class="keyword">private</span>[memory] <span class="function"><span class="keyword">def</span> <span class="title">acquireMemory</span></span>(</span><br><span class="line">    numBytes: <span class="type">Long</span>,</span><br><span class="line">    taskAttemptId: <span class="type">Long</span>,</span><br><span class="line">    maybeGrowPool: <span class="type">Long</span> =&gt; <span class="type">Unit</span> = (additionalSpaceNeeded: <span class="type">Long</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    computeMaxPoolSize: () =&gt; <span class="type">Long</span> = () =&gt; poolSize): <span class="type">Long</span> = lock.synchronized &#123;</span><br><span class="line">  assert(numBytes &gt; <span class="number">0</span>, <span class="string">s"invalid number of bytes requested: <span class="subst">$numBytes</span>"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> clean up this clunky method signature</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果我们没有Track到这个Task，那么就加到memoryForTask</span></span><br><span class="line">  <span class="keyword">if</span> (!memoryForTask.contains(taskAttemptId)) &#123;</span><br><span class="line">    memoryForTask(taskAttemptId) = <span class="number">0</span>L</span><br><span class="line">    <span class="comment">// 通知wait集合中的Task更新自己的numTasks</span></span><br><span class="line">    lock.notifyAll()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> simplify this to limit each task to its own slot</span></span><br><span class="line">  <span class="comment">// 尝试寻找，直到要么我们确定我们不愿意给它内存（因为超过1/N）了，</span></span><br><span class="line">  <span class="comment">// 或者我们有足够的内存提供。注意我们保证每个Task的1/2N的底线</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> numActiveTasks = memoryForTask.keys.size</span><br><span class="line">    <span class="keyword">val</span> curMem = memoryForTask(taskAttemptId)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 在每一次迭代中，首先尝试从Storage借用的内存中拿回部分内存。</span></span><br><span class="line">    <span class="comment">// 这是必要的，否则可能发生竞态，此时新的Storage Block会再把这个Task需要的执行内存拿回来。</span></span><br><span class="line">    maybeGrowPool(numBytes - memoryFree)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// maxPoolSize是内存池扩容之后可能的最大大小。</span></span><br><span class="line">    <span class="comment">// 通过这个值，可以计算所谓的1/N和1/2N具体有多大。在计算时必须考虑可能被释放的内存（例如evicting cached blocks），否则就会导致SPARK-12155的问题</span></span><br><span class="line">    <span class="keyword">val</span> maxPoolSize = computeMaxPoolSize()</span><br><span class="line">    <span class="keyword">val</span> maxMemoryPerTask = maxPoolSize / numActiveTasks</span><br><span class="line">    <span class="keyword">val</span> minMemoryPerTask = poolSize / (<span class="number">2</span> * numActiveTasks)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 最多再给这么多内存</span></span><br><span class="line">    <span class="keyword">val</span> maxToGrant = math.min(numBytes, math.max(<span class="number">0</span>, maxMemoryPerTask - curMem))</span><br><span class="line">    <span class="comment">// 实际上能给这么多内存</span></span><br><span class="line">    <span class="keyword">val</span> toGrant = math.min(maxToGrant, memoryFree)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 虽然我们尝试让每个Task尽可能得到1/2N的内存，</span></span><br><span class="line">    <span class="comment">// 但由于Task数量是动态变化的，可能在N增长前，老的Task就把内存吃完了</span></span><br><span class="line">    <span class="comment">// 所以如果我们给不了这么多内存的话，就让它睡在wait上面</span></span><br><span class="line">    <span class="keyword">if</span> (toGrant &lt; numBytes &amp;&amp; curMem + toGrant &lt; minMemoryPerTask) &#123;</span><br><span class="line">      logInfo(<span class="string">s"TID <span class="subst">$taskAttemptId</span> waiting for at least 1/2N of <span class="subst">$poolName</span> pool to be free"</span>)</span><br><span class="line">      lock.wait()</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      memoryForTask(taskAttemptId) += toGrant</span><br><span class="line">      <span class="keyword">return</span> toGrant</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="number">0</span>L  <span class="comment">// Never reached</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Tungsten内存管理机制"><a href="#Tungsten内存管理机制" class="headerlink" title="Tungsten内存管理机制"></a>Tungsten内存管理机制</h3><p>Tungsten不依赖于Java对象，所以堆内和堆外的内存分配都可以支持。序列化时间相比原生的要加快很多。其优化主要包含三点：</p>
<ol>
<li>Memory Management and Binary Processing</li>
<li>Cache-aware computation</li>
<li>Code generation<br> 这个是为了解决在Spark 2.0之前SparkSQL使用的<a href="https://www.iteblog.com/archives/2563.html" target="_blank" rel="noopener">Volcano</a>中大量的链式<code>next()</code>导致的性能（虚函数等）问题。</li>
</ol>
<p>在内存管理部分，能看到诸如<a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java" target="_blank" rel="noopener">TaskMemoryManager.java</a>的文件；在稍后的Shuffle部分，能看到诸如<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/java/org/apache/spark/sql/catalyst/expressions/codegen/UnsafeWriter.java" target="_blank" rel="noopener">UnsafeWriter.java</a>的文件。这些Java文件在实现上就有对Tungsten的使用，因为用到了sun.misc.Unsafe的API，所以使用Tungsten的shuffle又叫Unsafe shuffle。</p>
<p>在<code>MemoryManager</code>中持有了Tungsten内存管理机制的核心类<code>tungstenMemoryAllocator: MemoryAllocator</code>。并设置了<code>tungstenMemoryMode</code>指示其分配内存的默认位置，如果<code>MEMORY_OFFHEAP_ENABLED</code>是打开的且<code>MEMORY_OFFHEAP_SIZE</code>是大于0的，那么默认使用堆外内存。</p>
<h3 id="TaskMemoryManager"><a href="#TaskMemoryManager" class="headerlink" title="TaskMemoryManager"></a>TaskMemoryManager</h3><p><code>TaskMemoryManager</code>这个对象被用来管理一个Task的堆内和对外内存分配，因此它能够调度一个Task中各个组件的内存使用情况。当组件需要使用<code>TaskMemoryManager</code>提供的内存时，他们需要继承一个<code>MemoryConsumer</code>类，以便向<code>TaskMemoryManager</code>请求内存。<code>TaskMemoryManager</code>中集成了普通的内存分配机制和Tungsten内存分配机制。</p>
<h4 id="普通分配acquireExecutionMemory"><a href="#普通分配acquireExecutionMemory" class="headerlink" title="普通分配acquireExecutionMemory"></a>普通分配acquireExecutionMemory</h4><p>我们跟踪<code>TaskMemoryManager.acquireExecutionMemory</code>相关代码，它先尝试从<code>MemoryManager</code>直接请求内存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskMemoryManager.scala</span></span><br><span class="line">public long acquireExecutionMemory(long required, <span class="type">MemoryConsumer</span> consumer) &#123;</span><br><span class="line">  assert(required &gt;= <span class="number">0</span>);</span><br><span class="line">  assert(consumer != <span class="literal">null</span>);</span><br><span class="line">  <span class="type">MemoryMode</span> mode = consumer.getMode();</span><br><span class="line">  <span class="comment">// 如果我们在分配堆外内存的页，并且受到一个对堆内内存的请求，</span></span><br><span class="line">  <span class="comment">// 那么没必要去Spill，因为怎么说也只是Spill的堆外内存。</span></span><br><span class="line">  <span class="comment">// 不过现在改这个风险很大。。。。</span></span><br><span class="line">  synchronized (<span class="keyword">this</span>) &#123;</span><br><span class="line">    long got = memoryManager.acquireExecutionMemory(required, taskAttemptId, mode);</span><br></pre></td></tr></table></figure>

<p>如果请求不到，那么先尝试让同一个<code>TaskMemoryManager</code>上的其他的Consumer Spill，以减少Spill频率，从而减少Spill出来的小文件数量。主要是根据每个Consumer的内存使用排个序，从而避免重复对同一个Consumer进行Spill，导致产生很多小文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    <span class="keyword">if</span> (got &lt; required) &#123;</span><br><span class="line">      <span class="type">TreeMap</span>&lt;<span class="type">Long</span>, <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt;&gt; sortedConsumers = <span class="keyword">new</span> <span class="type">TreeMap</span>&lt;&gt;();</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">MemoryConsumer</span> c: consumers) &#123;</span><br><span class="line">        <span class="keyword">if</span> (c != consumer &amp;&amp; c.getUsed() &gt; <span class="number">0</span> &amp;&amp; c.getMode() == mode) &#123;</span><br><span class="line">          long key = c.getUsed();</span><br><span class="line">          <span class="type">List</span>&lt;<span class="type">MemoryConsumer</span>&gt; list =</span><br><span class="line">              sortedConsumers.computeIfAbsent(key, k -&gt; <span class="keyword">new</span> <span class="type">ArrayList</span>&lt;&gt;(<span class="number">1</span>));</span><br><span class="line">          list.add(c);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>现在，我们对排序得到的一系列<code>sortedConsumers</code>进行spill，一旦成功释放出内存，就立刻向MemoryManager去请求这些内存，相关代码没啥可看的，故省略。如果内存还是不够，就Spill自己，如果成功了，就向MemoryManager请求内存。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    <span class="comment">// call spill() on itself</span></span><br><span class="line">    <span class="keyword">if</span> (got &lt; required) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        long released = consumer.spill(required - got, consumer);</span><br><span class="line">        <span class="keyword">if</span> (released &gt; <span class="number">0</span>) &#123;</span><br><span class="line">          logger.debug(<span class="string">"Task &#123;&#125; released &#123;&#125; from itself (&#123;&#125;)"</span>, taskAttemptId,</span><br><span class="line">            <span class="type">Utils</span>.bytesToString(released), consumer);</span><br><span class="line">          got += memoryManager.acquireExecutionMemory(required - got, taskAttemptId, mode);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">ClosedByInterruptException</span> e) &#123;</span><br><span class="line">        ...</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    consumers.add(consumer);</span><br><span class="line">    logger.debug(<span class="string">"Task &#123;&#125; acquired &#123;&#125; for &#123;&#125;"</span>, taskAttemptId, <span class="type">Utils</span>.bytesToString(got), consumer);</span><br><span class="line">    <span class="keyword">return</span> got;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Tungsten分配allocatePage"><a href="#Tungsten分配allocatePage" class="headerlink" title="Tungsten分配allocatePage"></a>Tungsten分配allocatePage</h4><p><code>TaskMemoryManager</code>还有个<code>allocatePage</code>方法，用来获得<code>MemoryBlock</code>，这个是通过Tungsten机制分配的。<code>TaskMemoryManager</code>使用了类似操作系统中分页的机制来操控内存。每个“页”，也就是<code>MemoryBlock</code>对象，维护了一段堆内或者堆外的内存。页的总数由<code>PAGE_NUMBER_BITS</code>来决定，即对于一个64位的地址，高<code>PAGE_NUMBER_BITS</code>（默认13）位表示一个页，而后面的位表示在页内的偏移。当然，如果是堆外内存，那么这个64位就直接是内存地址了。有关使用分页机制的原因在<a href="https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java" target="_blank" rel="noopener">TaskMemoryManager.java</a>有介绍，我暂时没看懂。</p>
<p>需要注意的是，即使使用Tungsten分配，仍然不能绕开<code>UnifiedMemoryManager</code>机制的管理，所以我们看到在<code>allocatePage</code>方法中先要通过<code>acquireExecutionMemory</code>方法注册，请求到逻辑内存之后，再通过下面的方法请求物理内存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TaskMemoryManager.scala</span></span><br><span class="line">long acquired = acquireExecutionMemory(size, consumer);</span><br><span class="line"><span class="keyword">if</span> (acquired &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">&#125;</span><br><span class="line">page = memoryManager.tungstenMemoryAllocator().allocate(acquired);</span><br></pre></td></tr></table></figure>

<h2 id="Spark-Job执行流程分析"><a href="#Spark-Job执行流程分析" class="headerlink" title="Spark Job执行流程分析"></a>Spark Job执行流程分析</h2><h3 id="Job阶段"><a href="#Job阶段" class="headerlink" title="Job阶段"></a>Job阶段</h3><p>下面我们通过一个RDD上的Action操作count，查看Spark的Job是如何运行和调度的。特别注意的是，在SparkSQL中，Action操作有不同的执行流程，所以宜对比着看。<code>count</code>通过全局的<code>SparkContext.runJob</code>启动一个Job，这个函数转而调用<code>DAGScheduler.runJob</code>。<code>Utils.getIteratorSize</code>实际上就是遍历一遍迭代器，以便统计count。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br><span class="line"><span class="comment">// Utils.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIteratorSize</span></span>(iterator: <span class="type">Iterator</span>[_]): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    count += <span class="number">1</span>L</span><br><span class="line">    iterator.next()</span><br><span class="line">  &#125;</span><br><span class="line">  count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在参数列表里面的下划线<code>_</code>的作用是将方法转为函数，而方法和函数的定义和区别可参考<a href="http://www.calvinneo.com/2019/08/06/scala-lang/">我的另一篇文章</a>。<br>下面查看<code>runJob</code>函数。比较有趣的是<code>clean</code>函数，它调用<code>ClosureCleaner.clean</code>方法，这个方法用来清理<code>$outer</code>域中未被引用的变量。因为我们要将闭包<code>func</code>序列化，并从Driver发送到Executor上面。序列化闭包的过程就是为每一个闭包生成一个可序列化类，在生成时，会将这个闭包所引用的外部对象也序列化。容易发现，如果我们为了使用外部对象的某些字段，而序列化整个对象，那么开销是很大的，因此通过<code>clean</code>来清除不需要的部分以减少序列化开销。此外，<code>getCallSite</code>用来生成诸如<code>s&quot;$lastSparkMethod at $firstUserFile:$firstUserLine&quot;</code>这样的字符串，它实际上会回溯调用栈，找到第一个不是在Spark包中的函数，即<code>$lastSparkMethod</code>，它是导致一个RDD创建的函数，比如各种Transform操作、<code>sc.parallelize</code>等。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"SparkContext has been shutdown"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)</span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">    logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)</span><br><span class="line">  &#125;</span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br><span class="line">  <span class="comment">// CheckPoint机制</span></span><br><span class="line">  rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">clean</span></span>[<span class="type">F</span> &lt;: <span class="type">AnyRef</span>](f: <span class="type">F</span>, checkSerializable: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">F</span> = &#123;</span><br><span class="line">  <span class="type">ClosureCleaner</span>.clean(f, checkSerializable)</span><br><span class="line">  f</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们发现，传入的func只接受一个<code>Iterator[_]</code>参数，但是其形参声明却是接受<code>TaskContext</code>和<code>Iterator[T]</code>两个参数。这是为什么呢？这是因为<code>runJob</code>有不少重载函数，例如下面的这个</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面我们查看<code>DAGScheduler.runJob</code>函数，它实际上就是调用<code>submitJob</code>，然后等待Job执行的结果。由于Spark的<code>DAGScheduler</code>是基于事件循环的，它拥有一个<code>DAGSchedulerEventProcessLoop</code>类型的变量<code>eventProcessLoop</code>，不同的对象向它<code>post</code>事件，然后在它的<code>onReceive</code>循环中会依次对这些事件调用处理函数。<br>我们需要注意的是<code>partitions</code>不同于我们传入的<code>rdd.partitions</code>，前者是一个<code>Array[Int]</code>，后者是一个<code>Array[Partition]</code>。并且在逻辑意义上，前者表示需要计算的partition，对于如first之类的Action操作来说，它只是rdd的所有partition的一个子集，我们将在稍后的<code>submitMissingTasks</code>函数中继续看到这一点。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](...): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 下面就是在等了</span></span><br><span class="line">  <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">      <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">      exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">      <span class="keyword">throw</span> exception</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>], <span class="comment">// target RDD to run tasks on，就是被执行count的RDD</span></span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, <span class="comment">// 在RDD每一个partition上需要跑的函数</span></span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>, <span class="comment">// 被调用的位置</span></span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="comment">// 检查是否在一个不存在的分区上创建一个Task</span></span><br><span class="line">  <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">  partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>( <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> + <span class="string">"Total number of partitions: "</span> + maxPartitions)&#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// jobId是从后往前递增的</span></span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">  <span class="keyword">if</span> (partitions.isEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> time = clock.getTimeMillis()</span><br><span class="line">    <span class="comment">// listenerBus是一个LiveListenerBus对象，从DAGScheduler构造时得到，用来做event log</span></span><br><span class="line">    <span class="comment">// SparkListenerJobStart定义在SparkListener.scala文件中</span></span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerJobStart</span>(jobId, time, <span class="type">Seq</span>[<span class="type">Info</span>](), <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerJobEnd</span>(jobId, time, <span class="type">JobSucceeded</span>))</span><br><span class="line">    <span class="comment">// 如果partitions是空的，那么就直接返回</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  assert(partitions.nonEmpty)</span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">  <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">  <span class="comment">// 我们向eventProcessLoop提交一个JobSubmitted事件</span></span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">  waiter</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// DAGSchedulerEvent.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JobSubmitted</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    jobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    finalRDD: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line"><span class="class">    <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span> = <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DAGSchedulerEvent</span></span><br></pre></td></tr></table></figure>

<p>下面我们具体看看对<code>JobSubmitted</code>的响应</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(...) &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="comment">// 首先我们尝试创建一个`finalStage: ResultStage`，这是整个Job的最后一个Stage。</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// func: (TaskContext, Iterator[_]) =&gt; _</span></span><br><span class="line">    <span class="comment">// 下面的语句是可能抛BarrierJobSlotsNumberCheckFailed或者其他异常的，</span></span><br><span class="line">    <span class="comment">// 例如一个HadoopRDD所依赖的HDFS文件被删除了</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(...): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)</span><br><span class="line">  stageIdToStage(id) = stage</span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里<code>createResultStage</code>所返回的<code>ResultStage</code>继承了<code>Stage</code>类。<code>Stage</code>类有个<code>rdd</code>参数，对<code>ResultStage</code>而言就是<code>finalRDD</code>，对<code>ShuffleMapStage</code>而言就是<code>ShuffleDependency.rdd</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    shuffleDep: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>], jobId: <span class="type">Int</span>): <span class="type">ShuffleMapStage</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> rdd = shuffleDep.rdd</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>下面我们来看看<code>checkBarrierStageWithNumSlots</code>这个函数，因为它会抛出<code>BarrierJobSlotsNumberCheckFailed</code>这个异常，被<code>handleJobSubmitted</code>捕获。这个函数主要是为了检测是否有足够的slots去运行所有的barrier task。<a href="https://zhuanlan.zhihu.com/p/49628311" target="_blank" rel="noopener">屏障调度器</a>是Spark为了支持深度学习在2.4.0版本所引入的一个特性。它要求在barrier stage中同时启动所有的Task，当任意的task执行失败的时候，总是重启整个barrier stage。这么麻烦是因为Spark希望能够在Task中提供一个barrier以供显式同步。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkBarrierStageWithNumSlots</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> numPartitions = rdd.getNumPartitions</span><br><span class="line">  <span class="keyword">val</span> maxNumConcurrentTasks = sc.maxNumConcurrentTasks</span><br><span class="line">  <span class="keyword">if</span> (rdd.isBarrier() &amp;&amp; numPartitions &gt; maxNumConcurrentTasks) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BarrierJobSlotsNumberCheckFailed</span>(numPartitions, maxNumConcurrentTasks)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">BarrierJobSlotsNumberCheckFailed</span> =&gt;</span><br><span class="line">      <span class="comment">// If jobId doesn't exist in the map, Scala coverts its value null to 0: Int automatically.</span></span><br><span class="line">      <span class="comment">// barrierJobIdToNumTasksCheckFailures是一个ConcurrentHashMap，表示对每个BarrierJob上失败的Task数量</span></span><br><span class="line">      <span class="keyword">val</span> numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">        (_: <span class="type">Int</span>, value: <span class="type">Int</span>) =&gt; value + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">        messageScheduler.schedule(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, finalRDD, func,</span><br><span class="line">              partitions, callSite, listener, properties))</span><br><span class="line">          &#125;,</span><br><span class="line">          timeIntervalNumTasksCheck,</span><br><span class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Job failed, clear internal data.</span></span><br><span class="line">        barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</span><br><span class="line">      listener.jobFailed(e)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Job submitted, clear internal data.</span></span><br><span class="line">  barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>下面开始创建Job。<code>ActiveJob</code>表示在<code>DAGScheduler</code>里面运行的一个Job。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">  clearCacheLocs()</span><br><span class="line">  <span class="comment">// 在这里会打印四条日志，这个可以被用来在Spark.log里面定位事件</span></span><br><span class="line">  logInfo(<span class="string">"Got job %s (%s) with %d output partitions"</span>.format(</span><br><span class="line">    job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">  logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">" ("</span> + finalStage.name + <span class="string">")"</span>)</span><br><span class="line">  logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)</span><br><span class="line">  logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">  <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">  <span class="comment">// 从最后一个stage开始调用submitStage</span></span><br><span class="line">  submitStage(finalStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Job只负责向“叶子”Stage要结果，而之前Stage的运行是由<code>DAGScheduler</code>来调度的。这是因为若干Job可能共用同一个Stage的计算结果，所以将某个Stage强行归属到某个Job是不符合Spark设计逻辑的。我这么说的原因有一下两点</p>
<ol>
<li>在下面的论述中可以看到，在<code>getMissingParentStages</code>中会调用<code>getOrCreateShuffleMapStage</code>去取某个Stage。</li>
<li>根据<a href="https://stackoverflow.com/questions/47545529/can-one-stage-belong-to-different-jobs-in-spark" target="_blank" rel="noopener">爆栈网</a>，<code>Stage</code>中定义了一个<code>jobIds</code>，它是一个<code>HashSet</code>，也暗示了其可以被复用。 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Stage</span>(<span class="params">...</span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">/** Set of jobs that this stage belongs to. */</span></span><br><span class="line">    <span class="keyword">val</span> jobIds = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Int</span>]</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Stage阶段"><a href="#Stage阶段" class="headerlink" title="Stage阶段"></a>Stage阶段</h3><p>Stage是如何划分的呢？又是如何计算Stage之间的依赖的？我们继续查看<code>submitStage</code>这个函数，对于一个Stage，首先调用<code>getMissingParentStages</code>看看它的父Stage能不能直接用，也就是说这个Stage的rdd所依赖的<strong>所有</strong>父RDD能不能直接用，如果不行的话，就要先算父Stage的。在前面的论述里，我们知道，若干Job可能共用同一个Stage的计算结果，而不同的Stage也可能依赖同一个RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">	<span class="comment">// 找到这个stage所属的job</span></span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="comment">// 如果依赖之前的Stage，先列出来，并且按照id排序</span></span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">"missing: "</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">      	<span class="comment">// 运行这个Stage</span></span><br><span class="line">        logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      	<span class="comment">// 先提交所有的parent stage</span></span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面具体查看<code>getMissingParentStages</code>这个函数，可以看到，Stage的计算链是以最后一个RDD为树根逆着向上遍历得到的，而这个链条的终点要么是一个<code>ShuffleDependency</code>，要么是一个所有分区都被缓存了的RDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">  <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="comment">// 这里是个**DFS**，栈是手动维护的，主要是为了防止爆栈</span></span><br><span class="line">  waitingForVisit += stage.rdd</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">      visited += rdd</span><br><span class="line">      <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">      <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">        <span class="comment">// 如果这个RDD有没有被缓存的Partition，那么它就需要被计算</span></span><br><span class="line">        <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">          <span class="comment">// 我们检查这个RDD的所有依赖</span></span><br><span class="line">          dep <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">              <span class="comment">// 我们发现一个宽依赖，因此我们创建一个新的Shuffle Stage，并加入到missing中（如果不存在）</span></span><br><span class="line">              <span class="comment">// 由于是宽依赖，所以我们不需要向上找了</span></span><br><span class="line">              <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">              <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</span><br><span class="line">                missing += mapStage</span><br><span class="line">              &#125;</span><br><span class="line">            <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</span><br><span class="line">              <span class="comment">// 如果是一个窄依赖，就加入到waitingForVisit中</span></span><br><span class="line">              <span class="comment">// prepend是在头部加，+=是在尾部加</span></span><br><span class="line">              waitingForVisit.prepend(narrowDep.rdd)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    visit(waitingForVisit.remove(<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line">  missing.toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Task阶段"><a href="#Task阶段" class="headerlink" title="Task阶段"></a>Task阶段</h3><p>下面是重头戏<code>submitMissingTasks</code>，这个方法负责生成TaskSet，并且将它提交给TaskScheduler低层调度器。<br><code>partitionsToCompute</code>计算有哪些分区是待计算的。根据Stage类型的不同，<code>findMissingPartitions</code>的计算方法也不同。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">  logDebug(<span class="string">"submitMissingTasks("</span> + stage + <span class="string">")"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// ResultStage.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> job = activeJob.get</span><br><span class="line">  (<span class="number">0</span> until job.numPartitions).filter(id =&gt; !job.finished(id))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ActiveJob.scala</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = finalStage <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="comment">// 对于ResultStage，不一定得到当前rdd的所有分区，例如first()和lookup()的Action，</span></span><br><span class="line">  <span class="comment">// 因此这里是r.partitions而不是r.rdd.partitions</span></span><br><span class="line">  <span class="keyword">case</span> r: <span class="type">ResultStage</span> =&gt; r.partitions.length</span><br><span class="line">  <span class="keyword">case</span> m: <span class="type">ShuffleMapStage</span> =&gt; m.rdd.partitions.length</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ShuffleMapStage.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  mapOutputTrackerMaster</span><br><span class="line">    .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class="line">    .getOrElse(<span class="number">0</span> until numPartitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapOutputTrackerMaster.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(shuffleId: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">  shuffleStatuses.get(shuffleId).map(_.findMissingPartitions())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个<code>outputCommitCoordinator</code>是由<code>SparkEnv</code>维护的<code>OutputCommitCoordinator</code>对象，它决定到底谁有权利向HDFS写数据。在Executor上的请求会通过他持有的Driver的<code>OutputCommitCoordinatorEndpoint</code>的引用发送给Driver处理</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">  <span class="comment">// with this Stage</span></span><br><span class="line">  <span class="keyword">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class="line"></span><br><span class="line">  runningStages += stage</span><br><span class="line">  <span class="comment">// 在检测Tasks是否serializable之前，就要SparkListenerStageSubmitted，</span></span><br><span class="line">  <span class="comment">// 如果不能serializable，那就在这**之后**给一个SparkListenerStageCompleted</span></span><br><span class="line"></span><br><span class="line">  stage <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(</span><br><span class="line">        stage = s.id, maxPartitionId = s.rdd.partitions.length - <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>用<code>getPreferredLocs</code>计算每个分区的最佳计算位置，它实际上是调用<code>getPreferredLocsInternal</code>这个函数。这个函数是一个关于<code>visit: HashSet[(RDD[_], Int)]</code>的递归函数，visit用<code>(rdd, partition)</code>元组唯一描述一个分区。<code>getPreferredLocs</code>的计算逻辑是这样的：</p>
<ol>
<li>如果已经visit过了，就返回Nil</li>
<li>如果是被cached的，通过<code>getCacheLocs</code>返回cache的位置</li>
<li>如果RDD有自己的偏好位置，例如输入RDD，那么使用<code>rdd.preferredLocations</code>返回它的偏好位置</li>
<li>如果还没返回，但RDD有窄依赖，那么遍历它的所有依赖项，返回第一个具有位置偏好的依赖项的值</li>
</ol>
<p>理论上，一个最优的位置选取应该尽可能靠近数据源以减少网络传输，但目前版本的Spark还没有实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">          (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">        &#125;.toMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      <span class="comment">// 如果有非致命异常就创建一个新的Attempt，并且abortStage（这还不致命么）</span></span><br><span class="line">      stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">      listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">      abortStage(stage, <span class="string">s"Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>"</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>下面，我们开始attempt这个Stage，我们需要将RDD对象和依赖通过<code>closureSerializer</code>序列化成<code>taskBinaryBytes</code>，然后广播得到<code>taskBinary</code>。当广播变量过大时，会产生一条<code>Broadcasting large task binary with size</code>的INFO。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果没有Task要执行，实际上就是skip了，那么就没有Submission Time这个字段</span></span><br><span class="line">  <span class="keyword">if</span> (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">    stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">  &#125;</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> 也许可以将`taskBinary`放到Stage里面以避免对它序列化多次。</span></span><br><span class="line">  <span class="comment">// 一堆注释看不懂</span></span><br><span class="line">  <span class="keyword">var</span> taskBinary: <span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Byte</span>]] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> partitions: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">var</span> taskBinaryBytes: <span class="type">Array</span>[<span class="type">Byte</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// taskBinaryBytes and partitions are both effected by the checkpoint status. We need</span></span><br><span class="line">    <span class="comment">// this synchronization in case another concurrent job is checkpointing this RDD, so we get a</span></span><br><span class="line">    <span class="comment">// consistent view of both variables.</span></span><br><span class="line">    <span class="type">RDDCheckpointData</span>.synchronized &#123;</span><br><span class="line">      taskBinaryBytes = stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class="type">AnyRef</span>))</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          <span class="comment">// 注意这里的stage.func已经被ClosureCleaner清理过了</span></span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class="type">AnyRef</span>))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      partitions = stage.rdd.partitions</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 广播</span></span><br><span class="line">    taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="comment">// In the case of a failure during serialization, abort the stage.</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">"Task not serializable: "</span> + e.toString, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>下面，我们根据Stage的类型生成Task。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">          stage.pendingPartitions += id</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">            stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>我们将生成的<code>tasks</code>包装成一个<code>TaskSet</code>，并且提交给<code>taskScheduler</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> (tasks.nonEmpty) &#123;</span><br><span class="line">    logInfo(<span class="string">s"Submitting <span class="subst">$&#123;tasks.size&#125;</span> missing tasks from <span class="subst">$stage</span> (<span class="subst">$&#123;stage.rdd&#125;</span>) (first 15 "</span> +</span><br><span class="line">      <span class="string">s"tasks are for partitions <span class="subst">$&#123;tasks.take(15).map(_.partitionId)&#125;</span>)"</span>)</span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">      tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br></pre></td></tr></table></figure>

<p>如果tasks是空的，说明任务就已经完成了，打上DEBUG日志，并且调用<code>submitWaitingChildStages</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">// Because we posted SparkListenerStageSubmitted earlier, we should mark</span></span><br><span class="line">    <span class="comment">// the stage as completed here in case there are no tasks to run</span></span><br><span class="line">    markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">s"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; "</span> +</span><br><span class="line">            <span class="string">s"(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,"</span> +</span><br><span class="line">            <span class="string">s"available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,"</span> +</span><br><span class="line">            <span class="string">s"partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span>)</span><br><span class="line">        markMapStageJobsAsFinished(stage)</span><br><span class="line">      <span class="keyword">case</span> stage : <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        logDebug(<span class="string">s"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    submitWaitingChildStages(stage)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><p>Shuffle机制是Spark Core的核心内容。在Stage和Stage之间，Spark需要Shuffle数据。这个流程包含上一个Stage上的Shuffle Write，中间的数据传输，以及下一个Stage的Shuffle Read。如下图所示<br><img src="/img/sparksql/shuffle.png"></p>
<p>Shuffle类操作常常发生在宽依赖的RDD之间，这类算子需要将多个节点上的数据拉取到同一节点上进行计算，其中存在大量磁盘IO、序列化和网络传输开销，它们可以分为以下几点来讨论。<br>当Spark中的某个节点故障之后，常常需要重算RDD中的某几个分区。对于窄依赖而言，父RDD的一个分区只对应一个子RDD分区，因此丢失子RDD的分区，重算整个父RDD分区是必要的。而对于宽依赖而言，父RDD会被多个子RDD使用，而可能当前丢失的子RDD只使用了父RDD中的某几个分区的数据，而我们仍然要重新计算整个父RDD，这造成了计算资源的浪费。<br>当使用Aggregate类（如<code>groupByKey</code>）或者Join类这种Shuffle算子时，如果选择的<code>key</code>上的数据是倾斜(skew)的，会导致部分节点上的负载增大。对于这种情况除了可以增加Executor的内存，还可以重新选择分区函数（例如在之前的key上加盐）来平衡分区。<br>Shuffle Read操作容易产生OOM，其原因是尽管在<code>BlockStoreShuffleReader</code>中会产生外部排序的<code>resultIter</code>，但在这之前，<code>ExternalAppendOnlyMap</code>先要从BlockManager拉取数据<code>(k, v)</code>到自己的<code>currentMap</code>中，如果这里的<code>v</code>很大，那么就会导致Executor的OOM问题。可以从<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a>的文档中佐证这一点。在<code>Dataset</code>中并没有<code>reduceByKey</code>，原因可能<a href="https://stackoverflow.com/questions/38383207/rolling-your-own-reducebykey-in-spark-dataset" target="_blank" rel="noopener">与Catalyst Optimizer的优化</a>有关，但考虑到<code>groupByKey</code>还是比较坑的，感觉这个举措并不明智。</p>
<h3 id="Map-side-combine"><a href="#Map-side-combine" class="headerlink" title="Map side combine"></a>Map side combine</h3><p>Map side combine指的是将聚合操作下推到每个计算节点，在这些节点上预先聚合(Aggregate)，将预聚合的结果拉到Driver上进行最终的聚合。这有点类似于Hadoop的Combine操作，目的是为了减少通信，以及为了通信产生的序列化反序列化开销。</p>
<p>Map side combine可以体现在一些新的算子的替换，例如<code>groupByKey -&gt; reduceByKey</code>。</p>
<h3 id="Shuffle考古"><a href="#Shuffle考古" class="headerlink" title="Shuffle考古"></a>Shuffle考古</h3><p>在Spark0.8版本前，Spark只有Hash Based Shuffle的机制。在这种方式下，假定Shuffle Write阶段（有的也叫Map阶段）有<code>W</code>个Task，在Shuffle Read阶段（有的也叫Reduce阶段）有<code>R</code>个Task，那么就会产生<code>W*R</code>个文件，分别表示从<code>W</code>中某个Task传递到<code>R</code>中的某个Task。这样的坏处是对文件系统产生很大压力，并且IO也差（随机读写）。由于这些文件是先全量在内存里面构造，再dump到磁盘上，所以Shuffle在Write阶段就很可能OOM。</p>
<p>为了解决这个问题，在Spark 0.8.1版本加入了File Consolidation，以求将<code>W</code>个Task的输出尽可能合并。现在，Executor上的每一个<a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">执行单位</a>都生成自己独一份的文件。假定所有的Executor总共有<code>C</code>个核心，每个Task占用<code>T</code>个核心，那么总共有<code>C/T</code>个执行单位。考虑极端情况，如果<code>C==T</code>，那么任务实际上是串行的，所以写一个文件就行了。因此，最终会生成<code>C/T*R</code>个文件。</p>
<p>但这个版本仍然没有解决OOM的问题。虽然对于reduce这类操作，比如<code>count</code>，因为是来一个combine一个，所以只要你的V不是数组，一般都没有较大的内存问题。但有的时候我们可能会强行把结果concat成一个数组。考虑执行<code>groupByKey</code>这样的操作，在Read阶段，每个Task需要得到得到自己负责的key对应的所有value，而Shuffle Write产生的是若干很大的文件，里面的key是杂乱无章的。如果我们需要得到一个key对应的所有value，那么我们就需要遍历这个文件，将key和对应的value全部存放在一个结构比如HashMap中，并进行合并。因此，我们必须保证这个HashMap足够大。既然如此，我们很容易想到一个基于外部排序的方案，我们为什么不能对key进行外排呢？确实在Hadoop MapReduce中会做归并排序，因此Reducer侧的数据按照key组织好的了。但Spark在下一个版本才这么做。</p>
<p>在Spark 0.9版本之后，引入了<code>ExternalAppendOnlyMap</code>，通过这个结构，SparkShuffle在combine的时候如果内存不够，就能Spill到磁盘，并在Spill的时候进行排序。当然，<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">内存还是要能承载一个KV的</a>，我们将在稍后的源码分析中深入研究这个问题。</p>
<p>终于在Spark1.1版本之后引入了Sorted Based Shuffle。此时，Shuffle Write阶段会按照Partition ID以及key对记录进行排序。同时将全部结果写到一个数据文件中，同时生成一个索引文件，Shuffle Read的Task可以通过该索引文件获取相关的数据。</p>
<p>在Spark 1.5，<a href="https://community.cloudera.com/t5/Community-Articles/What-is-Tungsten-for-Apache-Spark/ta-p/248445" target="_blank" rel="noopener">Tungsten</a>内存管理机制成为了Spark的默认选项。如果关闭<code>spark.sql.tungsten.enabled</code>，Spark将采用基于Kryo序列化的列式存储格式。</p>
<h3 id="常见对象关系简介"><a href="#常见对象关系简介" class="headerlink" title="常见对象关系简介"></a>常见对象关系简介</h3><p><img src="/img/sparksql/shuffle-comp.png"></p>
<h3 id="ShuffleManager-SortShuffleManager"><a href="#ShuffleManager-SortShuffleManager" class="headerlink" title="ShuffleManager/SortShuffleManager"></a>ShuffleManager/SortShuffleManager</h3><p><code>ShuffleManager</code>是一个Trait，它的两个实现就是<code>org.apache.spark.shuffle.hash.HashShuffleManager</code>和<br><code>org.apache.spark.shuffle.sort.SortShuffleManager</code>。</p>
<p>如果partition的数量小于<code>spark.shuffle.sort.bypassMergeThreshold</code>，并且我们不需要做map side combine，那么就使用BypassMergeSortShuffleHandle。输出<code>numPartitions</code>个文件，并且在最后merge起来。这么做可以避免普通流程中对Spill的文件进行序列化和反序列化的过程。不好的是需要同时打开多个文件，并且导致很多内存分配。</p>
<p>如果可以进行序列化，就使用SerializedShuffleHandle。</p>
<p>否则就使用BaseShuffleHandle。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SortShuffleManager</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">ShuffleManager</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Obtains a [[ShuffleHandle]] to pass to tasks.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      numMaps: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Otherwise, buffer map outputs in a deserialized form:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>shouldBypassMergeSort主要判断下面几点：</p>
<ol>
<li>是否有Map Side Combine</li>
<li>Partition的数量是否小于bypassMergeThreshold</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">SortShuffleWriter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">shouldBypassMergeSort</span></span>(conf: <span class="type">SparkConf</span>, dep: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="comment">// We cannot bypass sorting if we need to do map-side aggregation.</span></span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bypassMergeThreshold: <span class="type">Int</span> = conf.getInt(<span class="string">"spark.shuffle.sort.bypassMergeThreshold"</span>, <span class="number">200</span>)</span><br><span class="line">      dep.partitioner.numPartitions &lt;= bypassMergeThreshold</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>canUseSerializedShuffle主要判断下面几点：</p>
<ol>
<li>是否支持序列化文件</li>
<li>是否允许Map-side Combine</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">object</span> <span class="title">SortShuffleManager</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The maximum number of shuffle output partitions that SortShuffleManager supports when</span></span><br><span class="line"><span class="comment">   * buffering map outputs in a serialized form. This is an extreme defensive programming measure,</span></span><br><span class="line"><span class="comment">   * since it's extremely unlikely that a single shuffle produces over 16 million output partitions.</span></span><br><span class="line"><span class="comment">   * */</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> =</span><br><span class="line">    <span class="type">PackedRecordPointer</span>.<span class="type">MAXIMUM_PARTITION_ID</span> + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Helper method for determining whether a shuffle should use an optimized serialized shuffle</span></span><br><span class="line"><span class="comment">   * path or whether it should fall back to the original path that operates on deserialized objects.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">canUseSerializedShuffle</span></span>(dependency: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> shufId = dependency.shuffleId</span><br><span class="line">    <span class="keyword">val</span> numPartitions = dependency.partitioner.numPartitions</span><br><span class="line">    <span class="keyword">if</span> (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123;</span><br><span class="line">      log.debug(<span class="string">s"Can't use serialized shuffle for shuffle <span class="subst">$shufId</span> because the serializer, "</span> +</span><br><span class="line">        <span class="string">s"<span class="subst">$&#123;dependency.serializer.getClass.getName&#125;</span>, does not support object relocation"</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency.mapSideCombine) &#123;</span><br><span class="line">      log.debug(<span class="string">s"Can't use serialized shuffle for shuffle <span class="subst">$shufId</span> because we need to do "</span> +</span><br><span class="line">        <span class="string">s"map-side aggregation"</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPartitions &gt; <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span>) &#123;</span><br><span class="line">      log.debug(<span class="string">s"Can't use serialized shuffle for shuffle <span class="subst">$shufId</span> because it has more than "</span> +</span><br><span class="line">        <span class="string">s"<span class="subst">$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> partitions"</span>)</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      log.debug(<span class="string">s"Can use serialized shuffle for shuffle <span class="subst">$shufId</span>"</span>)</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Shuffle完，产生多少个分区呢？这取决于具体的Partitioner，默认是200个。如果指定了Partitioner，通常是有产生Shuffle的时候计算的。例如coalesce会产生一个包装ShuffledRDD的CoalescedRDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> <span class="type">SHUFFLE_PARTITIONS</span> = buildConf(<span class="string">"spark.sql.shuffle.partitions"</span>)</span><br><span class="line">  .doc(<span class="string">"The default number of partitions to use when shuffling data for joins or aggregations."</span>)</span><br><span class="line">  .intConf</span><br><span class="line">  .createWithDefault(<span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Shuffle-Read端源码分析"><a href="#Shuffle-Read端源码分析" class="headerlink" title="Shuffle Read端源码分析"></a>Shuffle Read端源码分析</h3><p>Shuffle Read一般位于一个Stage的开始，这时候上一个Stage会给我们留下一个ShuffledRDD。在它的<code>compute</code>方法中会首先取出<code>shuffleManager: ShuffleManager</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  <span class="keyword">val</span> dep = dependencies.head.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="keyword">val</span> metrics = context.taskMetrics().createTempShuffleReadMetrics()</span><br><span class="line">  <span class="type">SparkEnv</span>.get.shuffleManager <span class="comment">// 由SparkEnv维护的ShuffleManager</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>接着，我们调用<code>shuffleManager.getReader</code>方法返回一个<code>BlockStoreShuffleReader</code>，它用来读取<code>[split.index, split.index + 1)</code>这个区间内的Shuffle数据。接着，它会调用<code>SparkEnv.get.mapOutputTracker</code>的<code>getMapSizesByExecutorId</code>方法。<code>getMapSizesByExecutorId</code>返回一个迭代器<code>Iterator[(BlockManagerId, Seq[(BlockId, Long, Int)])]</code>，表示对于某个<code>BlockManagerId</code>，它所存储的Shuffle Write中间结果，包括<code>BlockId</code>、大小和index。<br>具体实现上，这个方法首先从传入的<code>dep.shuffleHandle</code>中获得当前Shuffle过程的唯一标识<code>shuffleId</code>，然后它会从自己维护的<code>shuffleStatuses</code>中找到<code>shuffleId</code>对应的<code>MapStatus</code>，它应该有<code>endPartition-startPartition</code>这么多个。接着，对这些<code>MapStatus</code>，调用<code>convertMapStatuses</code>获得迭代器。在<code>compute</code>中，实际上就只取当前<code>split</code>这一个Partition的Shuffle元数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    .getReader(dep.shuffleHandle, split.index, split.index + <span class="number">1</span>, context, metrics) <span class="comment">// 返回一个BlockStoreShuffleReader</span></span><br><span class="line">    .read().asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>ShuffleManager</code>通过调用<code>BlockStoreShuffleReader.read</code>返回一个迭代器<code>Iterator[(K, C)]</code>。在<code>BlockStoreShuffleReader.read</code>方法中，首先得到一个<code>ShuffleBlockFetcherIterator</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">  <span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">    ...</span><br><span class="line">    ) <span class="comment">// 返回一个ShuffleBlockFetcherIterator</span></span><br><span class="line">    .toCompletionIterator <span class="comment">// 返回一个Iterator[(BlockId, InputStream)]</span></span><br></pre></td></tr></table></figure>

<p><code>ShuffleBlockFetcherIterator</code>用<code>fetchUpToMaxBytes()</code>和 <code>fetchLocalBlocks()</code>分别读取remote和local的Block。在拉取远程数据的时候，会统计<code>bytesInFlight</code>、<code>reqsInFlight</code>等信息，并使用<code>maxBytesInFlight</code>和<code>maxReqsInFlight</code>节制。同时，为了允许5个并发同时拉取数据，还会设置<code>targetRemoteRequestSize = math.max(maxBytesInFlight / 5, 1L)</code>去请求每次拉取数据的最大大小。通过<code>ShuffleBlockFetcherIterator.splitLocalRemoteBytes</code>，现在改名叫<code>partitionBlocksByFetchMode</code>函数，可以将blocks分为Local和Remote的。关于两个函数的具体实现，将单独讨论。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a key/value iterator for each stream</span></span><br><span class="line"><span class="keyword">val</span> recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">  serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Update the context task metrics for each record read.</span></span><br><span class="line"><span class="comment">// CompletionIterator相比普通的Iterator的区别就是在结束之后会调用一个completion函数</span></span><br><span class="line"><span class="comment">// CompletionIterator通过它伴生对象的apply方法创建，传入第二个参数即completionFunction</span></span><br><span class="line"><span class="keyword">val</span> metricIter = <span class="type">CompletionIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>), <span class="type">Iterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)]](</span><br><span class="line">  recordIter.map &#123; record =&gt;</span><br><span class="line">    readMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">    record</span><br><span class="line">  &#125;,</span><br><span class="line">  context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line"><span class="comment">// An interruptible iterator must be used here in order to support task cancellation</span></span><br><span class="line"><span class="keyword">val</span> interruptibleIter = <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)](context, metricIter)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>经过一系列转换，我们得到一个<code>interruptibleIter</code>。接下来，根据是否有Map Side Combine对它进行聚合。这里的<code>dep</code>来自于<code>BaseShuffleHandle</code>对象，它是一个<code>ShuffleDependency</code>。在前面Spark的任务调度中已经提到，<code>ShuffleDependency</code>就是宽依赖。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> aggregatedIter: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = <span class="keyword">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="comment">// We are reading values that are already combined</span></span><br><span class="line">      <span class="keyword">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">      dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class="line">      <span class="comment">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class="line">      <span class="comment">// type to the combined type C</span></span><br><span class="line">      <span class="keyword">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Nothing</span>)]]</span><br><span class="line">      dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>这里的<code>aggregator</code>是<code>Aggregator[K, V, C]</code>，这里的<code>K</code>、<code>V</code>和<code>C</code>与熟悉<code>combineByKey</code>的是一样的。需要注意的是，在combine的过程中借助了<code>ExternalAppendOnlyMap</code>，这是之前提到的在Spark 0.9中引入的重要特性。通过调用<code>insertAll</code>方法能够将<code>interruptibleIter</code>内部的数据添加到<code>ExternalAppendOnlyMap</code>中，并在之后更新<code>MemoryBytesSpilled</code>、<code>DiskBytesSpilled</code>、<code>PeakExecutionMemory</code>三个统计维度，这也是我们在Event Log中所看到的统计维度。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Aggregator.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    mergeValue: (<span class="type">C</span>, <span class="type">V</span></span>) <span class="title">=&gt;</span> <span class="title">C</span>,</span></span><br><span class="line"><span class="class">    <span class="title">mergeCombiners</span></span>: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">combineValuesByKey</span></span>(</span><br><span class="line">      iter: <span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ExternalAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](createCombiner, mergeValue, mergeCombiners)</span><br><span class="line">    combiners.insertAll(iter)</span><br><span class="line">    updateMetrics(context, combiners)</span><br><span class="line">    combiners.iterator</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">combineCombinersByKey</span></span>(</span><br><span class="line">      iter: <span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]],</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ExternalAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class="line">    <span class="comment">// 同上</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Update task metrics after populating the external map. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateMetrics</span></span>(context: <span class="type">TaskContext</span>, map: <span class="type">ExternalAppendOnlyMap</span>[_, _, _]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Option</span>(context).foreach &#123; c =&gt;</span><br><span class="line">      c.taskMetrics().incMemoryBytesSpilled(map.memoryBytesSpilled)</span><br><span class="line">      c.taskMetrics().incDiskBytesSpilled(map.diskBytesSpilled)</span><br><span class="line">      c.taskMetrics().incPeakExecutionMemory(map.peakMemoryUsedBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在获得Aggregate迭代器之后，最后一步，我们要进行排序，这时候就需要用到<code>ExternalSorter</code>这个对象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line">...</span><br><span class="line">  <span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">      <span class="keyword">val</span> sorter = <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">      sorter.insertAll(aggregatedIter)</span><br><span class="line">      context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">      context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">      context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">      <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">      context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">        sorter.stop()</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      aggregatedIter</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="Spillable"><a href="#Spillable" class="headerlink" title="Spillable"></a>Spillable</h3><p>从常见对象关系简介图中可以发现，其实<code>Spillable</code>是一个核心类，它定义了内存不够时的溢出行为。查看定义，发现它继承了<code>MemoryConsumer</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Spillable</span>[<span class="type">C</span>](<span class="params">taskMemoryManager: <span class="type">TaskMemoryManager</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">MemoryConsumer</span>(<span class="params">taskMemoryManager</span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure>

<p>另一点有趣的是这个<code>C</code>没有任何诸如上下界的约束，我以为Spark这边会至少给能Spill的容器一点约束啥的。<br>在这里，我们先来分析一下它的几个主要方法。</p>
<p><code>maybeSpill</code>是Spillable的主要逻辑，负责调用其他的抽象方法。<br>我们将在单独的章节论述<code>SizeTracker</code>如何估计集合大小，先看具体的Spill过程，可以梳理出<code>shouldSpill==true</code>的情况：</p>
<ol>
<li><code>elementsRead % 32 == 0</code></li>
<li><code>currentMemory &gt;= myMemoryThreshold</code>，其中后者默认值为<code>spark.shuffle.spill.initialMemoryThreshold = 5 * 1024 * 1024</code>，随着内存的分配会不断增大。前者为当前估计的Collection的内存大小。</li>
<li>通过<code>acquireMemory</code>请求的内存不足以扩展到<code>2 * currentMemory</code>的大小，关于这一步骤已经在内存管理部分详细说明了，在这就不详细说了</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spillable.scala</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="comment">// 调用对应MemoryConsumer的acquireMemory方法，先尝试获得内存</span></span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// 如果当前的Collection的内存大于myMemoryThreshold，就说明内存没有被分配足够，这时候要启动spill流程。</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line"><span class="comment">// MemoryConsumer.scala</span></span><br><span class="line">public long acquireMemory(long size) &#123;</span><br><span class="line">  long granted = taskMemoryManager.acquireExecutionMemory(size, <span class="keyword">this</span>);</span><br><span class="line">  used += granted;</span><br><span class="line">  <span class="keyword">return</span> granted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面就是真正Spill的过程了，其实就是调用可能被重载的<code>spill</code>函数。注意<code>_memoryBytesSpilled</code>就是我们在Event Log里面看到的Memory Spill的统计量，他表示在Spill之后我们能够释放多少内存</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spillable.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span> <span class="comment">// 统计Spill的次数</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    spill(collection) <span class="comment">// 这个方法有对应的重载</span></span><br><span class="line">    _elementsRead = <span class="number">0</span> <span class="comment">// 重置强制Spill计数器_elementsRead</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p><code>protected def spill(collection: C): Unit</code> 是由子类自己实现的逻辑。</p>
<hr>
<p><code>override def spill(size: Long, trigger: MemoryConsumer): Long</code> 是来自<code>MemoryConsumer</code>的接口，会调用<code>forceSpill</code>。</p>
<hr>
<p><code>protected def forceSpill(): Boolean</code><br>这个完全由子类来实现。<br>一个容易想到的问题是，<code>spill</code>和<code>forceSpill</code>有啥区别呢？前者嘛，肯定是被<code>maybeSpill</code>调用的，而后者，根据注释，是会被<code>TaskMemoryManager</code>调用的。当这个任务没有足够多的内存的时候，会调用<code>override def spill(size: Long, trigger: MemoryConsumer): Long</code>这个方法，而这个方法会调用<code>forceSpill</code>。</p>
<hr>
<p><code>logSpillage</code>函数。其实按道理，只要用到继承了<code>Spillalbe</code>的类，那么就会在Spark.log里面看到相应的log字符串，但我观察了一下，并没有看到在Shuffle密集任务里面看到有过多的Spill。只有观察到<code>UnsafeExternalSorter</code>里面有<code>Thread 102 spilling sort data of 1664.0 MB to disk(0 time so far)</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@inline</span> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">logSpillage</span></span>(size: <span class="type">Long</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> threadId = <span class="type">Thread</span>.currentThread().getId</span><br><span class="line">  logInfo(<span class="string">"Thread %d spilling in-memory map of %s to disk (%d time%s so far)"</span></span><br><span class="line">    .format(threadId, org.apache.spark.util.<span class="type">Utils</span>.bytesToString(size),</span><br><span class="line">      _spillCount, <span class="keyword">if</span> (_spillCount &gt; <span class="number">1</span>) <span class="string">"s"</span> <span class="keyword">else</span> <span class="string">""</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<p>当然，<code>Spillable</code>也不是一上来就Spill的，也会有个先申请内存的过程。这体现了在<code>maybeSpill</code>中，会先尝试调用自己<code>MemoryConsumer</code>基类的<code>acquireMemory</code>方法尝试获得足够数量的内存。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Initial threshold for the size of a collection before we start tracking its memory usage</span></span><br><span class="line"><span class="comment">// For testing only</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> initialMemoryThreshold: <span class="type">Long</span> =</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(<span class="type">SHUFFLE_SPILL_INITIAL_MEM_THRESHOLD</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Force this collection to spill when there are this many elements in memory</span></span><br><span class="line"><span class="comment">// For testing only</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> numElementsForceSpillThreshold: <span class="type">Int</span> =</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.get(<span class="type">SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Threshold for this collection's size in bytes before we start tracking its memory usage</span></span><br><span class="line"><span class="comment">// To avoid a large number of small spills, initialize this to a value orders of magnitude &gt; 0</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">var</span> myMemoryThreshold = initialMemoryThreshold</span><br></pre></td></tr></table></figure>

<h3 id="SizeTracker"><a href="#SizeTracker" class="headerlink" title="SizeTracker"></a>SizeTracker</h3><p>上面讲解了<code>Spillable</code>的特点，在这一章节中，继续介绍<code>Spillable</code>过程中用到的<code>SizeTracker</code>的实现。我们知道<strong>非序列化对象</strong>在内存存储上是<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">不连续的</a>，我们需要通过遍历迭代器才能知道对象的具体大小，而这个开销是比较大的。因此通过<code>SizeTracker</code>我们可以得到一个内存空间占用的估计值，从来用来判定是否需要Spill。<br>首先在每次集合更新之后，会调用<code>afterUpdate</code>，当到达采样的阈值<code>nextSampleNum</code>之后，会<code>takeSample</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">afterUpdate</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  numUpdates += <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> (nextSampleNum == numUpdates) &#123;</span><br><span class="line">    takeSample()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意，这里不是每一次都要<code>takeSample</code>一次，原因是这个计算开销还是蛮大的（主要是下面要讲的estimate方法）。我们查看定义</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A general interface for collections to keep track of their estimated sizes in bytes.</span></span><br><span class="line"><span class="comment"> * We sample with a slow exponential back-off using the SizeEstimator to amortize the time,</span></span><br><span class="line"><span class="comment"> * as each call to SizeEstimator is somewhat expensive (order of a few milliseconds).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">SizeTracker</span> </span>&#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">SizeTracker</span>._</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Controls the base of the exponential which governs the rate of sampling.</span></span><br><span class="line"><span class="comment">   * E.g., a value of 2 would mean we sample at 1, 2, 4, 8, ... elements.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SAMPLE_GROWTH_RATE</span> = <span class="number">1.1</span></span><br></pre></td></tr></table></figure>

<p>在开头，提到一个exponential back-off。这里的exponential back-off实际上就是每次更新后，随着<code>numUpdates</code>的增大，会更新<code>nextSampleNum</code>，导致调用的次数也会越来越不频繁。而这个<code>nextSampleNum</code>的值是<code>numUpdates*SAMPLE_GROWTH_RATE</code>，默认值是1.1。<br><code>takeSample</code>函数中第一句话就涉及多个对象，一个一个来看。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  samples.enqueue(<span class="type">Sample</span>(<span class="type">SizeEstimator</span>.estimate(<span class="keyword">this</span>), numUpdates))</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p><code>SizeEstimator.estimate</code>的实现类似去做一个state队列上的BFS。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">estimate</span></span>(obj: <span class="type">AnyRef</span>, visited: <span class="type">IdentityHashMap</span>[<span class="type">AnyRef</span>, <span class="type">AnyRef</span>]): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> state = <span class="keyword">new</span> <span class="type">SearchState</span>(visited)</span><br><span class="line">  state.enqueue(obj)</span><br><span class="line">  <span class="keyword">while</span> (!state.isFinished) &#123;</span><br><span class="line">    visitSingleObject(state.dequeue(), state)</span><br><span class="line">  &#125;</span><br><span class="line">  state.size</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>visitSingleObject</code>来具体做这个BFS，会特殊处理Array类型。我们不处理反射，因为反射包里面会引用到很多全局反射对象，这个对象又会应用到很多全局的大对象。同理，我们不处理ClassLoader，因为它里面会应用到整个REPL。反正ClassLoaders和Classes是所有对象共享的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">visitSingleObject</span></span>(obj: <span class="type">AnyRef</span>, state: <span class="type">SearchState</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> cls = obj.getClass</span><br><span class="line">  <span class="keyword">if</span> (cls.isArray) &#123;</span><br><span class="line">    visitArray(obj, cls, state)</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (cls.getName.startsWith(<span class="string">"scala.reflect"</span>)) &#123;</span><br><span class="line"></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (obj.isInstanceOf[<span class="type">ClassLoader</span>] || obj.isInstanceOf[<span class="type">Class</span>[_]]) &#123;</span><br><span class="line">    <span class="comment">// Hadoop JobConfs created in the interpreter have a ClassLoader.</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    obj <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">KnownSizeEstimation</span> =&gt;</span><br><span class="line">        state.size += s.estimatedSize</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="keyword">val</span> classInfo = getClassInfo(cls)</span><br><span class="line">        state.size += alignSize(classInfo.shellSize)</span><br><span class="line">        <span class="keyword">for</span> (field &lt;- classInfo.pointerFields) &#123;</span><br><span class="line">          state.enqueue(field.get(obj))</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后我们创建一个<code>Sample</code>，并且放到队列<code>samples</code>中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">SizeTracker</span> </span>&#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Sample</span>(<span class="params">size: <span class="type">Long</span>, numUpdates: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">&#125;</span></span><br></pre></td></tr></table></figure>

<p>下面的主要工作就是计算一个<code>bytesPerUpdate</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  ...</span><br><span class="line">  <span class="comment">// Only use the last two samples to extrapolate</span></span><br><span class="line">  <span class="comment">// 如果sample太多了，就删除掉一些</span></span><br><span class="line">  <span class="keyword">if</span> (samples.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">    samples.dequeue()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> bytesDelta = samples.toList.reverse <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> latest :: previous :: tail =&gt;</span><br><span class="line">      (latest.size - previous.size).toDouble / (latest.numUpdates - previous.numUpdates)</span><br><span class="line">    <span class="comment">// If fewer than 2 samples, assume no change</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  bytesPerUpdate = math.max(<span class="number">0</span>, bytesDelta)</span><br><span class="line">  nextSampleNum = math.ceil(numUpdates * <span class="type">SAMPLE_GROWTH_RATE</span>).toLong</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们统计到上次估算之后经历的update数量，并乘以<code>bytesPerUpdate</code>，即可得到总大小</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimateSize</span></span>(): <span class="type">Long</span> = &#123;</span><br><span class="line">  assert(samples.nonEmpty)</span><br><span class="line">  <span class="keyword">val</span> extrapolatedDelta = bytesPerUpdate * (numUpdates - samples.last.numUpdates)</span><br><span class="line">  (samples.last.size + extrapolatedDelta).toLong</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="AppendOnlyMap"><a href="#AppendOnlyMap" class="headerlink" title="AppendOnlyMap"></a>AppendOnlyMap</h3><h4 id="赋值"><a href="#赋值" class="headerlink" title="赋值"></a>赋值</h4><p>下面的代码是<code>AppendOnlyMap.changeValue</code>的实现，它接受一个<code>updateFunc</code>用来更新一个指定<code>K</code>的值。<code>updateFunc</code>接受第一个布尔值，用来表示是不是首次出现这个<code>key</code>。我们需要注意，<code>AppendOnlyMap</code>里面<code>null</code>是一个合法的键，但同时<code>null</code>又作为它里面的哈希表的默认填充，因此它里面有个对<code>null</code>特殊处理的过程。也就是说，如果<code>key</code>是null，会提前将它替换为一个<code>nullValue</code>的值，这个<code>key</code>不会存放到哈希表<code>data</code>里面。<a href="https://stackoverflow.com/questions/10749010/if-an-int-cant-be-null-what-does-null-asinstanceofint-mean" target="_blank" rel="noopener">这里介绍一下</a><code>null.asInstanceOf[V]</code>的花里胡哨的语法，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"><span class="comment">// 这里的nullValue和haveNullValue是用来单独处理k为null的情况的，下面会详细说明</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> haveNullValue = <span class="literal">false</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> nullValue: <span class="type">V</span> = <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeValue</span></span>(key: <span class="type">K</span>, updateFunc: (<span class="type">Boolean</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">V</span> = &#123;</span><br><span class="line">  <span class="comment">// updateFunc就是从insertAll传入的update</span></span><br><span class="line">  assert(!destroyed, destructionMessage)</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">  <span class="keyword">if</span> (k.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!haveNullValue) &#123;</span><br><span class="line">      <span class="comment">// 如果这时候还没有null的这个key，就新创建一个</span></span><br><span class="line">      incrementSize()</span><br><span class="line">    &#125;</span><br><span class="line">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class="line">    haveNullValue = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">return</span> nullValue</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">1</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 乘以2的原因是他按照K1 V1 K2 V2这样放的</span></span><br><span class="line">    <span class="keyword">val</span> curKey = data(<span class="number">2</span> * pos)</span><br><span class="line">    <span class="keyword">if</span> (curKey.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">      <span class="comment">// 如果对应的key不存在，就新创建一个</span></span><br><span class="line">      <span class="comment">// 这也是为什么前面要单独处理null的原因，这里的null被用来做placeholder了</span></span><br><span class="line">      <span class="comment">// 可以看到，第一个参数传的false，第二个是花里胡哨的null</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">false</span>, <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>]) </span><br><span class="line">      data(<span class="number">2</span> * pos) = k</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      incrementSize()</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (k.eq(curKey) || k.equals(curKey)) &#123; <span class="comment">// 又是从Java继承下来的花里胡哨的特性</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">true</span>, data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 再散列</span></span><br><span class="line">      <span class="keyword">val</span> delta = i</span><br><span class="line">      pos = (pos + delta) &amp; mask</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>] <span class="comment">// Never reached but needed to keep compiler happy</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面，我们看看<code>incrementSize</code>的实现，这是一个很经典的以2为底的递增的内存分配。当目前元素数量达到<code>(LOAD_FACTOR * capacity)</code>后，就考虑扩容。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Increase table size by 1, rehashing if necessary */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">incrementSize</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  curSize += <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> (curSize &gt; growThreshold) &#123;</span><br><span class="line">    growTable()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> <span class="type">LOAD_FACTOR</span> = <span class="number">0.7</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> capacity = nextPowerOf2(initialCapacity)</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> mask = capacity - <span class="number">1</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> curSize = <span class="number">0</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> growThreshold = (<span class="type">LOAD_FACTOR</span> * capacity).toInt</span><br></pre></td></tr></table></figure>

<h4 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h4><p>先来看看<code>destructiveSortedIterator</code>的实现，相比它提供的另一个<code>iterator</code>方法，这个方法同样返回一个<code>Iterator</code>，但是经过排序的。<a href="https://stackoverflow.com/questions/16682861/what-is-destructive" target="_blank" rel="noopener">这里destructive的意思是inplace的，会改变原来的容器的状态</a>，因此它不需要使用额外的内存。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* Return an iterator of the map in sorted order. This provides a way to sort the map without</span></span><br><span class="line"><span class="comment">* using additional memory, at the expense of destroying the validity of the map.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">destructiveSortedIterator</span></span>(keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>]): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</span><br><span class="line">  destroyed = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">var</span> keyIndex, newIndex = <span class="number">0</span></span><br><span class="line">  <span class="comment">// 下面这个循环将哈希表里面散乱的KV对压缩到最前面</span></span><br><span class="line">  <span class="keyword">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class="line">    <span class="keyword">if</span> (data(<span class="number">2</span> * keyIndex) != <span class="literal">null</span>) &#123;</span><br><span class="line">      data(<span class="number">2</span> * newIndex) = data(<span class="number">2</span> * keyIndex)</span><br><span class="line">      data(<span class="number">2</span> * newIndex + <span class="number">1</span>) = data(<span class="number">2</span> * keyIndex + <span class="number">1</span>)</span><br><span class="line">      newIndex += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    keyIndex += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 因为nullValue不会存放到哈希表data里面，所以这里除了netIndex，如果说有null的话，还要额外加上1。</span></span><br><span class="line">  assert(curSize == newIndex + (<span class="keyword">if</span> (haveNullValue) <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[<span class="type">K</span>, <span class="type">AnyRef</span>]).sort(data, <span class="number">0</span>, newIndex, keyComparator)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 这是一个经典的iterator的实现，在后面我们也会看到非常类似的写法</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] &#123;</span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> nullValueReady = haveNullValue</span><br><span class="line">    <span class="comment">// 如果没遍历完newIndex，或者nullValue还没遍历到，那么都有next。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): (<span class="type">K</span>, <span class="type">V</span>) = &#123;</span><br><span class="line">      <span class="keyword">if</span> (nullValueReady) &#123;</span><br><span class="line">        <span class="comment">// 每次都优先返回nullValue</span></span><br><span class="line">        nullValueReady = <span class="literal">false</span></span><br><span class="line">        (<span class="literal">null</span>.asInstanceOf[<span class="type">K</span>], nullValue)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> item = (data(<span class="number">2</span> * i).asInstanceOf[<span class="type">K</span>], data(<span class="number">2</span> * i + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ExternalAppendOnlyMap"><a href="#ExternalAppendOnlyMap" class="headerlink" title="ExternalAppendOnlyMap"></a>ExternalAppendOnlyMap</h3><p>我们查看<code>ExternalAppendOnlyMap</code>的实现。<code>ExternalAppendOnlyMap</code>拥有一个<code>currentMap</code>管理在内存中存储的键值对们。和一个<code>DiskMapIterator</code>的数组<code>spilledMaps</code>，表示Spill到磁盘上的键值对们。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span>[collection] <span class="keyword">var</span> currentMap = <span class="keyword">new</span> <span class="type">SizeTrackingAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> spilledMaps = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DiskMapIterator</span>]</span><br></pre></td></tr></table></figure>

<h4 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h4><p>下面，我们来看<code>insertAll</code>这个方法，这个方法也就是将一些KV对，加入<code>ExternalAppendOnlyMap</code>中。<br>这里的<code>currentMap</code>是一个<code>SizeTrackingAppendOnlyMap</code>。这个东西实际上就是一个<code>AppendOnlyMap</code>，不过给它加上了统计数据大小的功能，主要是借助于<code>SizeTracker</code>中<code>afterUpdate</code>和<code>resetSamples</code>两个方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(entries: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (currentMap == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(</span><br><span class="line">      <span class="string">"Cannot insert new elements into a map after calling iterator"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 我们复用update函数，从而避免每一次都创建一个新的闭包（编程环境这么恶劣的么。。。）</span></span><br><span class="line">  <span class="keyword">var</span> curEntry: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">val</span> update: (<span class="type">Boolean</span>, <span class="type">C</span>) =&gt; <span class="type">C</span> = (hadVal, oldVal) =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (hadVal) </span><br><span class="line">      <span class="comment">// 如果不是第一个V，就merge，类似于reduce的func</span></span><br><span class="line">      <span class="comment">// mergeValue: (C, V) =&gt; C,</span></span><br><span class="line">      mergeValue(oldVal, curEntry._2) </span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="comment">// 如果是第一个V，就新建一个C，类似于return函数</span></span><br><span class="line">      <span class="comment">// createCombiner: V =&gt; C,</span></span><br><span class="line">      createCombiner(curEntry._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (entries.hasNext) &#123;</span><br><span class="line">    curEntry = entries.next()</span><br><span class="line">    <span class="comment">// SizeTracker的特性</span></span><br><span class="line">    <span class="keyword">val</span> estimatedSize = currentMap.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">      _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(currentMap, estimatedSize)) &#123;</span><br><span class="line">      <span class="comment">// 如果发生了Spill，就重新创建一个currentMap</span></span><br><span class="line">      currentMap = <span class="keyword">new</span> <span class="type">SizeTrackingAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// key: K, updateFunc: (Boolean, C) =&gt; C</span></span><br><span class="line">    currentMap.changeValue(curEntry._1, update)</span><br><span class="line">    addElementsRead()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出，在<code>insertAll</code>中主要做了两件事情：</p>
<ol>
<li>遍历<code>curEntry &lt;- entries</code>，并通过传入的<code>update</code>函数进行Combine<br> 在内部存储上，<code>AppendOnlyMap</code>，包括后面将看到的一些其他KV容器，都倾向于将<code>(K, V)</code>对放到哈希表的相邻两个位置，这样的好处应该是避免访问时再进行一次跳转。有关<code>changeValue</code>的实现，我们已经在<code>AppendOnlyMap</code>上进行了讨论。</li>
<li>估计<code>currentMap</code>的当前大小，并调用<code>currentMap.maybeSpill</code>向磁盘Spill。Spill相关的过程，我们已经在<code>Spillable</code>相关章节进行了说明</li>
</ol>
<h4 id="读出"><a href="#读出" class="headerlink" title="读出"></a>读出</h4><p>下面查看<code>ExternalAppendOnlyMap.iterator</code>这个方法，可以发现如果<code>spilledMaps</code>都是空的，也就是没有Spill的话，就返回内存里面<code>currentMap</code>的<code>iterator</code>，否则就返回一个<code>ExternalIterator</code>。<br>对于第一种情况，会用<code>SpillableIterator</code>包裹一下。这个类在很多地方有定义，包括<code>ExternalAppendOnlyMap.scala</code>，<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala" target="_blank" rel="noopener">ExternalSorter.scala</a>里面。在当前使用的实现中，它实际上就是封装了一下<code>Iterator</code>，使得能够spill，转换成<code>CompletionIterator</code>等。我们稍后来看一下这个迭代器的实现。<br>对于第二种情况，<code>ExternalIterator</code>比较有趣，将在稍后进行讨论。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// 如果没有发生Spill</span></span><br><span class="line">    destructiveIterator(currentMap.iterator)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果发生了Spill</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalIterator</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">destructiveIterator</span></span>(inMemoryIterator: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  readingIterator = <span class="keyword">new</span> <span class="type">SpillableIterator</span>(inMemoryIterator)</span><br><span class="line">  readingIterator.toCompletionIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而<code>currentMap.iterator</code>实际上就是一个朴素无华的迭代器的实现。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nextValue</span></span>(): (<span class="type">K</span>, <span class="type">V</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span> (pos == <span class="number">-1</span>) &#123;    <span class="comment">// Treat position -1 as looking at the null value</span></span><br><span class="line">    <span class="keyword">if</span> (haveNullValue) &#123;</span><br><span class="line">      <span class="keyword">return</span> (<span class="literal">null</span>.asInstanceOf[<span class="type">K</span>], nullValue)</span><br><span class="line">    &#125;</span><br><span class="line">    pos += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> (pos &lt; capacity) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!data(<span class="number">2</span> * pos).eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">      <span class="keyword">return</span> (data(<span class="number">2</span> * pos).asInstanceOf[<span class="type">K</span>], data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">    &#125;</span><br><span class="line">    pos += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Spill细节和SpillableIterator的实现"><a href="#Spill细节和SpillableIterator的实现" class="headerlink" title="Spill细节和SpillableIterator的实现"></a>Spill细节和SpillableIterator的实现</h4><p>而Spill实际上是走的<code>spillMemoryIteratorToDisk</code>函数</p>
<h4 id="ExternalIterator"><a href="#ExternalIterator" class="headerlink" title="ExternalIterator"></a>ExternalIterator</h4><p>下面我们来看<code>ExternalAppendOnlyMap</code>中<code>ExternalIterator</code>的实现。它是一个典型的外部排序的实现，有一个PQ用来merge。不过这次的迭代器换成了<code>destructiveSortedIterator</code>，sorted的意思就是我们都是排序的了。这个道理也是显而易见的，不sort一下，我们怎么和硬盘上的数据做聚合呢？</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="keyword">val</span> mergeHeap = <span class="keyword">new</span> mutable.<span class="type">PriorityQueue</span>[<span class="type">StreamBuffer</span>]</span><br><span class="line"><span class="keyword">val</span> sortedMap = destructiveIterator(currentMap.destructiveSortedIterator(keyComparator))</span><br><span class="line"><span class="comment">// 我们得到一个Array的迭代器</span></span><br><span class="line"><span class="keyword">val</span> inputStreams = (<span class="type">Seq</span>(sortedMap) ++ spilledMaps).map(it =&gt; it.buffered)</span><br><span class="line"></span><br><span class="line">inputStreams.foreach &#123; it =&gt;</span><br><span class="line">  <span class="keyword">val</span> kcPairs = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br><span class="line">  <span class="comment">// 读完所有具有所有相同hash(key)的序列，并创建一个StreamBuffer</span></span><br><span class="line">  <span class="comment">// 需要注意的是，由于哈希碰撞的原因，里面可能有多个key</span></span><br><span class="line">  readNextHashCode(it, kcPairs)</span><br><span class="line">  <span class="keyword">if</span> (kcPairs.length &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    mergeHeap.enqueue(<span class="keyword">new</span> <span class="type">StreamBuffer</span>(it, kcPairs))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>destructiveSortedIterator</code>的实现已经在<code>AppendOnlyMap</code>中进行了介绍。<br>下面我们来看看实现的<code>ExternalAppendOnlyMap.next()</code>接口函数，它是外部排序中的一个典型的归并过程。我们需要注意的是<code>minBuffer</code>是一个<code>StreamBuffer</code>，维护一个<code>hash(K), V</code>的<code>ArrayBuffer</code>，类似<code>H1 V1 H1 V2 H2 V3</code>这样的序列，而不是我们想的<code>(K, V)</code>流。因此其中是可能有哈希碰撞的。我们从<code>mergeHeap</code>中<code>dequeue</code>出来的<code>StreamBuffer</code>是当前<code>hash(K)</code>最小的所有<code>K</code>的集合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): (<span class="type">K</span>, <span class="type">C</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span> (mergeHeap.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// 如果堆是空的，就再见了</span></span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Select a key from the StreamBuffer that holds the lowest key hash</span></span><br><span class="line">  <span class="comment">// mergeHeap选择所有StreamBuffer中最小hash的，作为minBuffer</span></span><br><span class="line">  <span class="keyword">val</span> minBuffer = mergeHeap.dequeue()</span><br><span class="line">  <span class="comment">// minPairs是一个ArrayBuffer[T]，表示这个StreamBuffer维护的所有KV对</span></span><br><span class="line">  <span class="keyword">val</span> minPairs = minBuffer.pairs</span><br><span class="line">  <span class="keyword">val</span> minHash = minBuffer.minKeyHash</span><br><span class="line">  <span class="comment">// 从一个ArrayBuffer[T]中移出Index为0的项目</span></span><br><span class="line">  <span class="keyword">val</span> minPair = removeFromBuffer(minPairs, <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 得到非哈希的 (minKey, minCombiner)</span></span><br><span class="line">  <span class="keyword">val</span> minKey = minPair._1</span><br><span class="line">  <span class="keyword">var</span> minCombiner = minPair._2</span><br><span class="line">  assert(hashKey(minPair) == minHash)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// For all other streams that may have this key (i.e. have the same minimum key hash),</span></span><br><span class="line">  <span class="comment">// merge in the corresponding value (if any) from that stream</span></span><br><span class="line">  <span class="keyword">val</span> mergedBuffers = <span class="type">ArrayBuffer</span>[<span class="type">StreamBuffer</span>](minBuffer)</span><br><span class="line">  <span class="keyword">while</span> (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.minKeyHash == minHash) &#123;</span><br><span class="line">    <span class="keyword">val</span> newBuffer = mergeHeap.dequeue()</span><br><span class="line">    <span class="comment">// 如果newBuffer的key和minKey相等的话（考虑哈希碰撞），就合并</span></span><br><span class="line">    minCombiner = mergeIfKeyExists(minKey, minCombiner, newBuffer)</span><br><span class="line">    mergedBuffers += newBuffer</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Repopulate each visited stream buffer and add it back to the queue if it is non-empty</span></span><br><span class="line">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class="line">    <span class="keyword">if</span> (buffer.isEmpty) &#123;</span><br><span class="line">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!buffer.isEmpty) &#123;</span><br><span class="line">      mergeHeap.enqueue(buffer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  (minKey, minCombiner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="ExternalSorter"><a href="#ExternalSorter" class="headerlink" title="ExternalSorter"></a>ExternalSorter</h3><p><code>ExternalSorter</code>的作用是对输入的<code>(K, V)</code>进行排序，以产生新的<code>(K, C)</code>对，排序过程中可选择进行combine，否则输出的<code>C == V</code>。需要注意的是<code>ExternalSorter</code>不仅被用在Shuffle Read端，也被用在了Shuffle Write端，所以在后面会提到Map-side combine的概念。<code>ExternalSorter</code>具有如下的参数，在给定<code>ordering</code>之后，<code>ExternalSorter</code>就会按照它来排序。在Spark源码中建议如果希望进行Map-side combining的话，就指定<code>ordering</code>，否则就可以设置<code>ordering</code>为<code>null</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    context: <span class="type">TaskContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    ordering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Spillable</span>[<span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]](<span class="params">context.taskMemoryManager(</span>))</span></span><br></pre></td></tr></table></figure>

<p>由于<code>ExternalSorter</code>支持有combine和没有combine的两种模式，因此对应设置了两个对象。<code>map = new PartitionedAppendOnlyMap[K, C]</code>，以及<code>buffer = new PartitionedPairBuffer[K, C]</code>。其中，<code>PartitionedAppendOnlyMap</code>就是一个<code>SizeTrackingAppendOnlyMap</code>，支持按key进行combine。<code>PartitionedPairBuffer</code>则继承了<code>WritablePartitionedPairCollection</code>，由于不需要按照key进行combine，所以它的实现接近于一个Array。</p>
<p>相比之前的aggregator，<code>ExternalSorter</code>不仅能aggregate，还能sort。<code>ExternalSorter</code>在Shuffle Read和Write都有使用，而<code>ExternalAppendOnlyMap</code>只有在Shuffle Read中使用。所以为啥不直接搞一个<code>ExternalSorter</code>而是还要在前面垫一个<code>ExternalAppendOnlyMap</code>呢？为此，我们总结比较一下这两者：<br>首先，在<code>insertAll</code>时，<code>ExternalAppendOnlyMap</code>是一定要做combine的，而<code>ExternalSorter</code>可以选择是否做combine，为此还有<code>PartitionedAppendOnlyMap</code>和<code>PartitionedPairBuffer</code>两种数据结构。<br>其次，在做排序时，<code>ExternalAppendOnlyMap</code>默认对内存中的对象不进行排序，只有当要Spill的时候才会返回<code>AppendOnlyMap.destructiveSortedIterator</code>的方式将内存里面的东西有序写入磁盘。在返回迭代器时，如果没有发生Spill，那么<code>ExternalAppendOnlyMap</code>返回没有经过排序的<code>currentMap</code>，否则才通过<code>ExternalIterator</code>进行排序。而对<code>ExternalSorter</code>而言排序与否在于有没有指定<code>ordering</code>。如果进行排序的话，那么它会首先考虑Partition，再考虑Key。</p>
<h4 id="插入-1"><a href="#插入-1" class="headerlink" title="插入"></a>插入</h4><p><code>ExternalSorter.insertAll</code>方法和之前看到的<code>ExternalAppendOnlyMap</code>方法是大差不差的，他也会对可以聚合的特征进行聚合，并且TODO上还说如果聚合之后的reduction factor不够明显，就停止聚合。根据是否定义了<code>aggregator</code>，会分别采用之前提到的map和buffer来承载加入的数据。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalSorter.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn't high</span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      kv = records.next()</span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Stick values into our buffer</span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      <span class="keyword">val</span> kv = records.next()</span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])</span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>容易看出，这里面用到<code>maybeSpillCollection</code>来尝试管理内存，这个应该是和<code>spill</code>相关的，我们检查一下其实现，发现其实就是一个代理，对于是否使用map的情况，进行了分类讨论。而<code>maybeSpill</code>就是<code>Spillable</code>里面的定义了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;</span><br><span class="line">    estimatedSize = map.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    estimatedSize = buffer.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Shuffle-Write端源码分析"><a href="#Shuffle-Write端源码分析" class="headerlink" title="Shuffle Write端源码分析"></a>Shuffle Write端源码分析</h3><p>Shuffle Write端的实现主要依赖<code>ShuffleManager</code>中的<code>ShuffleWriter</code>对象，目前使用的<code>ShuffleManager</code>是<code>SortShuffleManager</code>，因此只讨论它。它是一个抽象类，主要有<code>SortShuffleWriter</code>、<code>UnsafeShuffleWriter</code>、<code>BypassMergeSortShuffleWriter</code>等实现。</p>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/** Write a sequence of records to this task's output */</span></span><br><span class="line">  <span class="meta">@throws</span>[<span class="type">IOException</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Close this writer, passing along whether the map completed */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(success: <span class="type">Boolean</span>): <span class="type">Option</span>[<span class="type">MapStatus</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>SortShuffleWriter</code>的实现可以说很简单了，就是将<code>records</code>放到一个<code>ExternalSorter</code>里面，然后创建一个<code>ShuffleMapOutputWriter</code>。<code>shuffleExecutorComponents</code>实际上是一个<code>LocalDiskShuffleExecutorComponents</code>。<code>ShuffleMapOutputWriter</code>是一个Java接口，实际上被创建的是<code>LocalDiskShuffleMapOutputWriter</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SortShuffleWriter</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果不需要进行mapSideCombine，那么我们传入空的aggregator和ordering，</span></span><br><span class="line">    <span class="comment">// 我们在map端不负责对key进行排序，统统留给reduce端吧</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don't bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter(</span><br><span class="line">    dep.shuffleId, mapId, dep.partitioner.numPartitions)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>紧接着，调用<code>ExternalSorter.writePartitionedMapOutput</code>将自己维护的<code>map</code>或者<code>buffer</code>（根据是否有Map Side Aggregation）写到<code>mapOutputWriter</code>提供的<code>partitionWriter</code>里面。其过程用到了一个<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala" target="_blank" rel="noopener">叫<code>destructiveSortedWritablePartitionedIterator</code>的迭代器</a>，相比<code>destructiveSortedIterator</code>，它是多了Writable和Partitioned两个词。前者的意思是我可以写到文件，后者的意思是我先按照partitionId排序，然后在按照给定的Comparator排序。<br>接着就是<code>commitAllPartitions</code>，这个函数调用<code>writeIndexFileAndCommit</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// </span></span><br><span class="line">  ...</span><br><span class="line">  sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)</span><br><span class="line">  <span class="keyword">val</span> partitionLengths = mapOutputWriter.commitAllPartitions()</span><br></pre></td></tr></table></figure>

<p><code>MapStatus</code>被用来保存Shuffle Write操作的metadata。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths, mapId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LocalDiskShuffleMapOutputWriter.java</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">public long[] commitAllPartitions() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  cleanUp();</span><br><span class="line">  <span class="type">File</span> resolvedTmp = outputTempFile != <span class="literal">null</span> &amp;&amp; outputTempFile.isFile() ? outputTempFile : <span class="literal">null</span>;</span><br><span class="line">  blockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, resolvedTmp);</span><br><span class="line">  <span class="keyword">return</span> partitionLengths;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>writeIndexFileAndCommit</code>负责为传入的文件<code>dataTmp</code>创建一个索引文件，并原子地提交。注意到，到当前版本，每一个执行单元只会生成一份数据文件和一份索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// IndexShuffleBlockResolver.java</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeIndexFileAndCommit</span></span>(shuffleId: <span class="type">Int</span>, mapId: <span class="type">Long</span>, lengths: <span class="type">Array</span>[<span class="type">Long</span>], dataTmp: <span class="type">File</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure>

<p>根据<code>writeIndexFileAndCommit</code>的注释，<code>getBlockData</code>会来读它写的块，这个<code>getBlockData</code>同样位于我们先前介绍过的<code>IndexShuffleBlockResolver</code>类中。</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>下面我们来看看<code>BypassMergeSortShuffleWriter</code>的实现。它到底Bypass了什么东西呢？其实是sort和aggregate。</p>
<h3 id="UnsafeShuffleWriter"><a href="#UnsafeShuffleWriter" class="headerlink" title="UnsafeShuffleWriter"></a>UnsafeShuffleWriter</h3><p><code>SortShuffleManager</code>还有一个子类是<code>UnsafeShuffleWriter</code>。<br><code>UnsafeShuffleWriter</code>使用<code>ShuffleExternalSorter</code>进行排序，而<code>SortShuffleWriter</code>使用<code>ExternalSorter</code>对象。<br><code>UnsafeShuffleWriter</code>使用<code>TaskMemoryManager</code>作内存分配，而<code>SortShuffleWriter</code>没有明确指定。</p>
<h3 id="fetchLocalBlocks和fetchUpToMaxBytes的实现"><a href="#fetchLocalBlocks和fetchUpToMaxBytes的实现" class="headerlink" title="fetchLocalBlocks和fetchUpToMaxBytes的实现"></a>fetchLocalBlocks和fetchUpToMaxBytes的实现</h3><p>简单说明一下<code>fetchLocalBlocks</code>和<code>fetchUpToMaxBytes</code>的实现</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ShuffleBlockFetcherIterator.scala</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> localBlocks = scala.collection.mutable.<span class="type">LinkedHashSet</span>[<span class="type">BlockId</span>]()</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">fetchLocalBlocks</span></span>() &#123;</span><br><span class="line">  logDebug(<span class="string">s"Start fetching local blocks: <span class="subst">$&#123;localBlocks.mkString(", ")&#125;</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> iter = localBlocks.iterator</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = iter.next()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> buf = blockManager.getBlockData(blockId)</span><br><span class="line">      ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (blockId.isShuffle) &#123;</span><br><span class="line">    <span class="comment">// 需要通过ShuffleBlockResolver来获取</span></span><br><span class="line">    shuffleManager.shuffleBlockResolver.getBlockData(blockId.asInstanceOf[<span class="type">ShuffleBlockId</span>])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    getLocalBytes(blockId) <span class="keyword">match</span> &#123;</span><br></pre></td></tr></table></figure>

<h2 id="Spark分布式部署方式"><a href="#Spark分布式部署方式" class="headerlink" title="Spark分布式部署方式"></a>Spark分布式部署方式</h2><h3 id="Spark自有部署方式"><a href="#Spark自有部署方式" class="headerlink" title="Spark自有部署方式"></a>Spark自有部署方式</h3><p>最常用的其实是单机模式也就是<code>spark-submit --master local</code>，这里local是默认选项。在程序执行过程中，只会生成一个SparkSubmit进程，不会产生Master和Worker节点，也不依赖Hadoop。当然，Windows里面可能需要winutils这个工具的，但也是直接下载，而不需要装Hadoop。<br>在集群化上，Spark可以部署在<a href="https://www.jianshu.com/p/65a3476757a5" target="_blank" rel="noopener">On Yarn和On Mesos、K8S和Standalone</a>上面，而又分别对应了Cluster和Client两种deploy mode。</p>
<p>首先是Spark自带Cluster Manager的Standalone Client模式，也是我们最常用的集群测试模式，需要启动Master和Slave节点，但仍然不依赖Hadoop。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master spark://localhost:7077  --class org.apache.spark.examples.SparkPi ./examples/jars/spark-examples_2.11-2.4.4.jar 100</span><br></pre></td></tr></table></figure>

<p>下面一种是Spark自带Cluster Manager的Standalone Cluster模式，一字之差，还是有不同的，用下面的方式启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master spark://wl1:6066 --deploy-mode cluster <span class="comment"># 默认cluster</span></span><br></pre></td></tr></table></figure>

<p>上面两种的配置一般修改Spark的spark-defaults.conf和spark-env.sh也就可以了，不涉及hadoop。<br>此外，还有Connection Reset的情况，这个需要打开Connection Reset。</p>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>Spark跑在yarn上面，这个还依赖hadoop集群，但Spark不需要自己提供Master和Worker了。Yarn同样提供了Cluster和Client两种模式，如下所示</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --master yarn-cluster</span><br><span class="line">./bin/spark-submit --master yarn-client</span><br></pre></td></tr></table></figure>

<p>Yarn Cluster就是通常使用的部署方式，此时Spark Driver是运行在Yarn的ApplicationMaster上的，而Client方式的Driver是在任务提交机上面运行，<a href="https://www.zybuluo.com/sasaki/note/252413" target="_blank" rel="noopener">ApplicationMaster只负责向ResourceManager申请Executor需要的资源</a>。</p>
<p>我们在Spark的WebUI中常常看到诸如Container、ApplicationMaster、ResourceMaster、NodeManager这些东西，其实他们都是Yarn里面的常见概念。具体的联系可以结合下面的图来看，ResourceMaster是YARN集群的Master，负责管理整个集群的资源，而NodeManager就是YARN集群的Slave，每个Node上面都会跑一个NodeManager。而每个Node上面又可以有很多个Container。<br><img src="/img/sparksql/yarn.png"><br>对应到Spark中，<a href="https://books.google.com.hk/books?id=YO13CgAAQBAJ&pg=PA8&lpg=PA8&dq=yarn+is+applicationmaster+a+special+container&source=bl&ots=yBDaV4bLbq&sig=ACfU3U2lki4VB2c9LQQtYWW8Hw0VgWE4HA&hl=zh-CN&sa=X&ved=2ahUKEwjgrqnz5sHqAhUTBZQKHcQQCQAQ6AEwAHoECAYQAQ#v=onepage&q=yarn%20is%20applicationmaster%20a%20special%20container&f=false" target="_blank" rel="noopener">一般来说一个Driver或一个Executor跑在Yarn的一个Container里面</a>，而ApplicationMaster是一个特殊的Container，一般为后缀<code>_00001</code>的container。<a href="https://www.zybuluo.com/sasaki/note/252413" target="_blank" rel="noopener">每个SparkContext对应一个ApplicationMaster，每个Executor对应一个Container</a>。</p>
<p>YARN视角的架构：</p>
<ol>
<li>ResourceMaster<ol>
<li>NodeManager<ol>
<li>Container<br> ApplicationMaster：类似于Spark Driver，对应一个SparkContext</li>
<li>Container<br> 普通的Spark Executor</li>
<li>Container<br> 普通的Spark Executor</li>
</ol>
</li>
<li>NodeManager<ol>
<li>Container</li>
<li>Container</li>
</ol>
</li>
</ol>
</li>
</ol>
<h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><p>SparkSQL由4个项目组成，分别为Spark Core、Spark Catalyst、Spark Hive和Spark Hive ThriftServer。我们主要和前两者打交道，Core中就是SparkSQL的核心，包括Dataset等类的实现。Catalyst是Spark的水晶优化器。</p>
<h2 id="DataFrame和Dataset"><a href="#DataFrame和Dataset" class="headerlink" title="DataFrame和Dataset"></a>DataFrame和Dataset</h2><p>我们可以将RDD看为一个分布式的容器<code>M[T]</code>，我们对<code>T</code>是未知的。而事实上我们处理数据集往往就是个来自HBase或者其他数据仓库大宽表。如果使用RDD会导致很多的拆箱和装箱的操作。并且由于<code>T</code>是一个黑盒，Spark也很难对RDD的计算进行优化。为此，Spark推出了SparkSQL来解决这个问题。而SparkSQL的一个核心机制就是DataFrame和Dataset。</p>
<p>在Spark1.0中，DataFrame可以看做<code>RDD[Row]</code>，但在Spark2.0对Dataset和DataFrame进行了统一，DataFrame可以看做一个<code>Dataset[Row]</code>，所以，我们主要以Dataset来研究对象。</p>
<h3 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h3><p>借助于Scala提供的implicit机制，我们可以从<code>Seq</code>创建<code>DataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">DoubleType</span>, <span class="type">LongType</span>, <span class="type">IntegerType</span>, <span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions.&#123;concat, lit, udf&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.catalyst.expressions.&#123;<span class="type">GenericRow</span>, <span class="type">GenericRowWithSchema</span>&#125;</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span>, money: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">import</span> <span class="title">spark</span>.<span class="title">implicits</span>.<span class="title">_</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">df</span> </span>= <span class="type">Seq</span>((<span class="string">"Calvin"</span>, <span class="number">18</span>, <span class="number">1000</span>)).toDF(<span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"money"</span>)</span><br></pre></td></tr></table></figure>

<p>但是，我们不能从一个<code>DataFrame</code>通过map到一个<code>Row</code>的方式得到另一个<code>DataFrame</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df2 = df.map(row =&gt; <span class="type">Row</span>(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>)))</span><br><span class="line"></span><br><span class="line">&lt;console&gt;:<span class="number">32</span>: error: <span class="type">Unable</span> to find encoder <span class="keyword">for</span> <span class="class"><span class="keyword">type</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">Row</span>. <span class="title">An</span> <span class="title">implicit</span> <span class="title">Encoder</span>[org.apache.spark.sql.<span class="type">Row</span>] <span class="title">is</span> <span class="title">needed</span> <span class="title">to</span> <span class="title">store</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">sql</span>.<span class="title">Row</span> <span class="title">instances</span> <span class="title">in</span> <span class="title">a</span> <span class="title">Dataset</span>. <span class="title">Primitive</span> <span class="title">types</span> (<span class="params"><span class="type">Int</span>, <span class="type">String</span>, etc</span>) <span class="title">and</span> <span class="title">Product</span> <span class="title">types</span> (<span class="params">case classes</span>) <span class="title">are</span> <span class="title">supported</span> <span class="title">by</span> <span class="title">importing</span> <span class="title">spark</span>.<span class="title">implicits</span>.<span class="title">_</span>  <span class="title">Support</span> <span class="title">for</span> <span class="title">serializing</span> <span class="title">other</span> <span class="title">types</span> <span class="title">will</span> <span class="title">be</span> <span class="title">added</span> <span class="title">in</span> <span class="title">future</span> <span class="title">releases</span>.</span></span><br><span class="line"><span class="class">       <span class="title">val</span> <span class="title">df2</span> </span>= df.map(row =&gt; <span class="type">Row</span>(row.getAs[<span class="type">String</span>](<span class="string">"name"</span>)))</span><br></pre></td></tr></table></figure>

<p>我们还可以通过<code>createDataFrame</code>从RDD创建一个<code>DataFrame</code>，因为RDD没有schema，所以我们要显式提供一个schema。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> schemaString=<span class="string">"name,age,money"</span></span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>(schemaString.split(<span class="string">","</span>).map(fieldName =&gt; <span class="type">StructField</span>(fieldName, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"><span class="keyword">val</span> sqlcontext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"><span class="keyword">val</span> r = sc.parallelize(<span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Calvin"</span>, <span class="number">18</span>, <span class="number">1000</span>))).map(a =&gt; <span class="type">Row</span>(a.name, a.age, a._3))</span><br><span class="line"><span class="keyword">val</span> df = sqlcontext.createDataFrame(r, schema)</span><br></pre></td></tr></table></figure>

<p>而这个schema可以通过下面的方法得到</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> schema = <span class="keyword">new</span> <span class="type">StructType</span>()</span><br><span class="line">    .add(<span class="type">StructField</span>(<span class="string">"string1"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)) <span class="comment">// 可能为null</span></span><br><span class="line">    .add(<span class="type">StructField</span>(<span class="string">"long1"</span>, <span class="type">LongType</span>)) <span class="comment">// 不可能为null</span></span><br></pre></td></tr></table></figure>

<p>这里的<code>StructType</code>和<code>StringType</code>等都是SparkSQL所提供的ADT，它们都继承<code>AbstractDataType</code>，例如<code>LongType</code>的<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/AbstractDataType.scala" target="_blank" rel="noopener">继承链是<code>LongType-&gt;IntegralType-&gt;NumericType-&gt;AtomicType-&gt;DataType-&gt;AbstractDataType</code></a>。<code>StructField</code>是<code>add</code>方法的参数，用来描述一个类型。包含四个成员，如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">StructField</span>(<span class="params"><span class="type">String</span> name,</span></span></span><br><span class="line"><span class="class"><span class="params">           <span class="type">DataType</span> dataType,</span></span></span><br><span class="line"><span class="class"><span class="params">           boolean nullable,</span></span></span><br><span class="line"><span class="class"><span class="params">           <span class="type">Metadata</span> metadata</span>)</span></span><br></pre></td></tr></table></figure>

<p>当然，还有一些从外部数据源构造DataFrame的工具，如下所示，这里的路径是Driver本地路径</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark.read.json(...)</span><br><span class="line">spark.read.csv(...)</span><br></pre></td></tr></table></figure>

<p>此外，Spark还可以从Hive、MySQL、HBase、Avro, Parquet, Kafka等数据源中读取数据。</p>
<h3 id="从RDD到DF-DS"><a href="#从RDD到DF-DS" class="headerlink" title="从RDD到DF/DS"></a>从RDD到DF/DS</h3><p>RDD可以通过<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala" target="_blank" rel="noopener">下面代码中的一个隐式转换</a> 得到一个<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala" target="_blank" rel="noopener">DatasetHolder</a>，接着借助于<code>DatasetHolder</code>中提供的<code>toDS</code>和<code>toDF</code>来实现到<code>DataFrame</code>和<code>Dataset</code>的转换。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">implicit</span> <span class="function"><span class="keyword">def</span> <span class="title">rddToDatasetHolder</span></span>[<span class="type">T</span> : <span class="type">Encoder</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">DatasetHolder</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="type">DatasetHolder</span>(_sqlContext.createDataset(rdd))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其实在上面文件里面还定义了一系列隐式转换所需要的<code>Encoder</code>，例如对于大多数的case class都需要调用<code>newProductArrayEncoder</code>。有关这部分的进一步说明，可以查看<a href="https://github.com/summerDG/spark-code-analysis/blob/master/analysis/sql/spark_sql_parser.md" target="_blank" rel="noopener">文章</a></p>
<p>同样，从Dataset/DataFrame到RDD可以通过调用<code>.rdd</code>方法来轻松得到。不过这个操作是Action么？在爆栈网上有<a href="https://stackoverflow.com/questions/44708629/is-dataset-rdd-an-action-or-transformation" target="_blank" rel="noopener">相关讨论1，认为不是Action但有开销</a>；和<a href="https://stackoverflow.com/questions/55885145/whats-the-overhead-of-converting-an-rdd-to-a-dataframe-and-back-again" target="_blank" rel="noopener">相关讨论2，认为是无开销的</a>。我们查看具体代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> objectType = exprEnc.deserializer.dataType</span><br><span class="line">    rddQueryExecution.toRdd.mapPartitions &#123; rows =&gt;</span><br><span class="line">        rows.map(_.get(<span class="number">0</span>, objectType).asInstanceOf[<span class="type">T</span>])</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="从DF到DS"><a href="#从DF到DS" class="headerlink" title="从DF到DS"></a>从DF到DS</h3><p>从DF到DS的转换需要指定一个Encoder</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds = df.as[<span class="type">Person</span>]</span><br></pre></td></tr></table></figure>

<h3 id="从DS到DF"><a href="#从DS到DF" class="headerlink" title="从DS到DF"></a>从DS到DF</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@scala</span>.annotation.varargs</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">toDF</span></span>(colNames: <span class="type">String</span>*): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">    require(schema.size == colNames.size,</span><br><span class="line">      <span class="string">"The number of columns doesn't match.\n"</span> +</span><br><span class="line">        <span class="string">s"Old column names (<span class="subst">$&#123;schema.size&#125;</span>): "</span> + schema.fields.map(_.name).mkString(<span class="string">", "</span>) + <span class="string">"\n"</span> +</span><br><span class="line">        <span class="string">s"New column names (<span class="subst">$&#123;colNames.size&#125;</span>): "</span> + colNames.mkString(<span class="string">", "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> newCols = logicalPlan.output.zip(colNames).map &#123; <span class="keyword">case</span> (oldAttribute, newName) =&gt;</span><br><span class="line">      <span class="type">Column</span>(oldAttribute).as(newName)</span><br><span class="line">    &#125;</span><br><span class="line">    select(newCols : _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h2><p><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala" target="_blank" rel="noopener">Row</a>是SparkSQL的基石。它实际上是一个<code>trait</code>，我们经常使用是它的子类<code>GenericRow</code>和<code>GenericRowWithSchema</code>，而<code>Row</code>的内部实现则是<code>InternalRow</code>。<br><code>GenericRow</code>是<code>Row</code>从<code>apply</code>创建时的默认构造。它没有schema。在<code>GenericRowWithSchema</code>中重新实现了<code>filedIndex</code>这个函数，允许我们使用<code>row.getAs[String](&quot;colName&quot;)</code>这样的方法。在上面的讨论中已经提到，我们不能从一个<code>DataFrame</code>通过map到一个<code>Row</code>的方式得到另一个<code>DataFrame</code>；反而可以从一个<code>Seq</code>得到。其原因就是因为<code>DataFrame</code>有schema而<code>Row</code>没有。我们通过下面的实验来检查从一个<code>Seq</code>到<code>DataFrame</code>的转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>((<span class="string">"Calvin"</span>, <span class="number">18</span>, <span class="number">1000</span>)).toDF(<span class="string">"name"</span>, <span class="string">"age"</span>, <span class="string">"money"</span>)</span><br></pre></td></tr></table></figure>

<p><code>GenericRow</code>是<code>Row</code>从<code>apply</code>创建时的默认构造。它没有schema。在<code>GenericRowWithSchema</code>中重新实现了<code>filedIndex</code>这个函数，允许我们使用<code>row.getAs[String](&quot;colName&quot;)</code>这样的方法。如果经常使用SparkSQL的API会发现我们不能从一个<code>DataFrame</code>通过map到一个<code>Row</code>的方式得到另一个<code>DataFrame</code>；反而可以从一个<code>Seq</code>得到，我们甚至可以通过<code>Seq(...).toDF(columns)</code>方法来得到一个其原因就是因为<code>DataFrame</code>有schema而<code>Row</code>没有。我们通过下面的实验来检查从一个<code>Seq</code>到<code>DataFrame</code>的转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">1</span>)).toDS</span><br><span class="line"><span class="keyword">val</span> ds2 = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Neo"</span>, <span class="number">23</span>, <span class="number">1</span>)).toDS</span><br><span class="line"><span class="keyword">val</span> dfj = ds.union(ds2)</span><br><span class="line"><span class="keyword">val</span> dsj_fail = dfj.toDS <span class="comment">// 注意DataFrame没有toDS方法，toDS是由RDD转DS用的</span></span><br><span class="line"><span class="keyword">val</span> dsj = dfj.as[<span class="type">Person</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> ds3 = <span class="type">Seq</span>((<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">1</span>)).toDS</span><br><span class="line"><span class="keyword">val</span> ds4 = <span class="type">Seq</span>((<span class="string">"Neo"</span>, <span class="number">23</span>, <span class="number">1</span>)).toDS</span><br><span class="line"><span class="keyword">val</span> dfj2 = ds3.union(ds4)</span><br></pre></td></tr></table></figure>

<p>有关<code>Row</code>和<code>GenericRowWithSchema</code>之间的转换，我们可以进行下面的实验</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 复用之前的头部</span></span><br><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">100</span>), <span class="type">Person</span>(<span class="string">"Neo"</span>, <span class="number">33</span>, <span class="number">300</span>)).toDF</span><br><span class="line"></span><br><span class="line"><span class="comment">// df的schema</span></span><br><span class="line">scala&gt; df.schema</span><br><span class="line">res2: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(name,<span class="type">StringType</span>,<span class="literal">true</span>), <span class="type">StructField</span>(age,<span class="type">LongType</span>,<span class="literal">false</span>), <span class="type">StructField</span>(money,<span class="type">LongType</span>,<span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第1行的schema</span></span><br><span class="line">scala&gt; df.take(<span class="number">1</span>)(<span class="number">0</span>).schema</span><br><span class="line">res3: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(name,<span class="type">StringType</span>,<span class="literal">true</span>), <span class="type">StructField</span>(age,<span class="type">LongType</span>,<span class="literal">false</span>), <span class="type">StructField</span>(money,<span class="type">LongType</span>,<span class="literal">false</span>))</span><br></pre></td></tr></table></figure>

<p><code>DataFrame</code>里面的<code>Row</code>不是单纯的<code>Row</code>，而是<code>GenericRowWithSchema</code>，相比之前的Row，要多了Schema。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查看type，发现是GenericRowWithSchema而不是Row</span></span><br><span class="line">scala&gt; df.take(<span class="number">1</span>)(<span class="number">0</span>).getClass.getSimpleName</span><br><span class="line">res5: <span class="type">String</span> = <span class="type">GenericRowWithSchema</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加一列</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> r = <span class="type">Row</span>.unapplySeq(df.take(<span class="number">1</span>)(<span class="number">0</span>)).get.toArray ++ <span class="type">Seq</span>(<span class="string">"SZ"</span>)</span><br><span class="line">r: <span class="type">Array</span>[<span class="type">Any</span>] = <span class="type">Array</span>(<span class="type">Calvin</span>, <span class="number">22</span>, <span class="number">100</span>, <span class="type">SZ</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对应增加一列schema</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> nsch = df.take(<span class="number">1</span>)(<span class="number">0</span>).schema.add(<span class="type">StructField</span>(<span class="string">"addr"</span>,<span class="type">StringType</span>,<span class="literal">true</span>))</span><br><span class="line">nsch: org.apache.spark.sql.types.<span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(name,<span class="type">StringType</span>,<span class="literal">true</span>), <span class="type">StructField</span>(age,<span class="type">LongType</span>,<span class="literal">false</span>), <span class="type">StructField</span>(money,<span class="type">LongType</span>,<span class="literal">false</span>), <span class="type">StructField</span>(addr,<span class="type">StringType</span>,<span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个新SchemaRow</span></span><br><span class="line">scala&gt; <span class="keyword">val</span> row_sch = <span class="keyword">new</span> <span class="type">GenericRowWithSchema</span>(r, nsch)</span><br><span class="line">row_sch: org.apache.spark.sql.catalyst.expressions.<span class="type">GenericRowWithSchema</span> = [<span class="type">Calvin</span>,<span class="number">22</span>,<span class="number">100</span>,<span class="type">SZ</span>]</span><br></pre></td></tr></table></figure>

<p>只有<code>GenericRowWithSchema</code>有，因此我们可以创建一个<a href="https://stackoverflow.com/questions/33934615/how-to-introduce-the-schema-in-a-row-in-spark" target="_blank" rel="noopener">GenericRowWithSchema</a>，其实现在<code>org.apache.spark.sql.catalyst.expressions.{GenericRow, GenericRowWithSchema}</code>。</p>
<h3 id="为什么不能在map函数中返回Row"><a href="#为什么不能在map函数中返回Row" class="headerlink" title="为什么不能在map函数中返回Row"></a>为什么不能在map函数中返回Row</h3><p>熟悉map函数的人往往比较熟悉下面的函数签名</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">fmap</span> :: <span class="type">Functor</span> f =&gt; (a -&gt; b) -&gt; f a -&gt; f b</span><br></pre></td></tr></table></figure>

<p>因此容易写出下面的代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df.map&#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Row</span>(...) =&gt; <span class="type">Row</span>(...)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>乍一看，很有道理啊，<code>f</code>就是<code>Dataset</code>，<code>a</code>和<code>b</code>都是Row，是一个很准确的代码了，但一编译发现少什么Encoder，这是怎么回事呢？<a href="https://stackoverflow.com/questions/39433419/encoder-error-while-trying-to-map-dataframe-row-to-updated-row" target="_blank" rel="noopener">这篇文章给了答案</a>。</p>
<h2 id="Column"><a href="#Column" class="headerlink" title="Column"></a>Column</h2><p>从上文中可以看到，<code>DataFrame</code>中的数据依旧是按照行组织的，通过外挂了一个schema，我们能够有效地识别列。在这种情况下对行的改动是容易的，但是如何对列进行改动呢？一般有两种办法</p>
<h3 id="借助于withColumn"><a href="#借助于withColumn" class="headerlink" title="借助于withColumn"></a>借助于withColumn</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">100</span>), <span class="type">Person</span>(<span class="string">"Neo"</span>, <span class="number">33</span>, <span class="number">300</span>)).toDF</span><br><span class="line"><span class="comment">// 通过cast函数进行类型转换，concat函数进行字符串连接</span></span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, concat($<span class="string">"name"</span>, $<span class="string">"age"</span>.cast(<span class="type">StringType</span>))).show()</span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, $<span class="string">"age"</span>+$<span class="string">"money"</span>).show()</span><br></pre></td></tr></table></figure>

<p>当然，在<code>$</code>表达式之外，我们还可以使用udf，甚至<a href="https://stackoverflow.com/questions/53191271/how-to-do-conditional-withcolumn-in-a-spark-dataframe" target="_blank" rel="noopener">带条件地进行withColumn</a></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 除了$表达式，还可以使用udf</span></span><br><span class="line"><span class="keyword">val</span> addMoneyUDF = udf((age: <span class="type">Long</span>, money: <span class="type">Long</span>) =&gt; age + money)</span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, addMoneyUDF($<span class="string">"age"</span>, $<span class="string">"money"</span>))</span><br></pre></td></tr></table></figure>

<p>特别需要注意的是<code>withColumn</code>是存在性能开销的。如果我们在代码里频繁（例如使用一个for循环）withColumn，那么就可能出现一个Job结束，而下一个Job迟迟不开始的情况。如果我们将日志等级设置为TRACE，可以看到代码中也存在了很多Batch Resolution的情况。这是因为较深层次的依赖会导致SparkSQL不能分清到底需要缓存哪些数据以用来恢复，因此只能全部缓存。另外<a href="https://medium.com/@manuzhang/the-hidden-cost-of-spark-withcolumn-8ffea517c015" target="_blank" rel="noopener">文章</a>中还表示会造成大量的Analyzes开销。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Analyzer.scala</span></span><br><span class="line"><span class="type">Batch</span>(<span class="string">"Resolution"</span>, fixedPoint,</span><br><span class="line">  <span class="type">ResolveTableValuedFunctions</span> ::</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ResolveCatalogs</span>(catalogManager) ::</span><br><span class="line">  <span class="type">ResolveInsertInto</span> ::</span><br><span class="line">  <span class="type">ResolveRelations</span> ::</span><br><span class="line">  <span class="type">ResolveReferences</span> ::</span><br><span class="line">...</span><br><span class="line">  <span class="type">ResolveRandomSeed</span> ::</span><br><span class="line">  <span class="type">TypeCoercion</span>.typeCoercionRules(conf) ++</span><br><span class="line">  extendedResolutionRules : _*),</span><br></pre></td></tr></table></figure>

<p>此外，伴随着<code>withColumn</code>的是UDF或者UDAF的使用，在Spark the definitive一书中指出，这类的算子容易导致OOM等问题。该书中还指出UDF会强迫将数据转换成JVM里面的对象，并且在一次查询中可能重复转换多次，这会产生很大的性能开销。</p>
<h2 id="KeyValueGroupedDataset和RelationalGroupedDataset"><a href="#KeyValueGroupedDataset和RelationalGroupedDataset" class="headerlink" title="KeyValueGroupedDataset和RelationalGroupedDataset"></a>KeyValueGroupedDataset和RelationalGroupedDataset</h2><p>不同于RDD的相关方法，DataFrame系列的<code>groupBy</code>和<code>groupByKey</code>会返回两个<a href="https://www.imooc.com/article/258924?block_id=tuijian_wz" target="_blank" rel="noopener">不同的类型</a>，<code>RelationalGroupedDataset</code>和<code>KeyValueGroupedDataset</code>。一般来说，虽然<code>groupByKey</code>更为灵活，能够生成自定义的key用来group，但<code>KeyValueGroupedDataset</code>只提供相对较少的操作，所以最好还是使用<code>groupby</code>。另外，在group操作之后就没有诸如union的操作，我们需要再显式map回<code>DataFrame</code>。</p>
<h2 id="SparkSQL语法和用法"><a href="#SparkSQL语法和用法" class="headerlink" title="SparkSQL语法和用法"></a>SparkSQL语法和用法</h2><h3 id="SparkSQL和DataFrame的交互"><a href="#SparkSQL和DataFrame的交互" class="headerlink" title="SparkSQL和DataFrame的交互"></a>SparkSQL和DataFrame的交互</h3><p>一个简单的问题是，在SQL中引用一个DataFrame呢？一个简单的做法是创建一个视图，即通过<code>createOrReplaceTempView</code>。</p>
<h2 id="SparkSQL的上下文"><a href="#SparkSQL的上下文" class="headerlink" title="SparkSQL的上下文"></a>SparkSQL的上下文</h2><p><code>SparkSQL</code>的上下文通过<code>SQLContext</code>维护，它由一个SparkSession持有，并指向其所有者，以及所有者维护的SparkContext。在Spark 2.0之后，大部分<code>SparkSQL</code>的逻辑工作被迁移到了<code>SparkSession</code>中，所以这个类可以被看做是一个兼容性的封装。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQLContext</span> <span class="title">private</span>[sql](<span class="params">val sparkSession: <span class="type">SparkSession</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">sessionState</span></span>: <span class="type">SessionState</span> = sparkSession.sessionState</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">sharedState</span></span>: <span class="type">SharedState</span> = sparkSession.sharedState</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">conf</span></span>: <span class="type">SQLConf</span> = sessionState.conf</span><br></pre></td></tr></table></figure>

<h3 id="SharedState"><a href="#SharedState" class="headerlink" title="SharedState"></a>SharedState</h3><p><code>SharedState</code>保存不同session的共享状态，<a href="https://www.cnblogs.com/SpeakSoftlyLove/p/7485449.html" target="_blank" rel="noopener">包括下面几个对象</a></p>
<ol>
<li>   <code>warehousePath</code></li>
<li>   <code>conf</code>, <code>hadoopConf</code></li>
<li><code>cacheManager</code><br> 这是SQLContext 的支持类，会自动保存query的查询结果。这样子查询在执行过程中，就可以使用这些查询结果。</li>
<li>   <code>statusStore</code></li>
<li>   <code>externalCatalog</code></li>
<li><code>globalTempViewManager</code><br> 一个线程安全的类，用来管理global temp view，并提供create、update、remove等原子操作来管理这些view。</li>
</ol>
<h3 id="SessionState"><a href="#SessionState" class="headerlink" title="SessionState"></a>SessionState</h3><p><code>SessionState</code>则包含了这个Session中相关的组件</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[sql] <span class="class"><span class="keyword">class</span> <span class="title">SessionState</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    sharedState: <span class="type">SharedState</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SQLConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val experimentalMethods: <span class="type">ExperimentalMethods</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val functionRegistry: <span class="type">FunctionRegistry</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val udfRegistration: <span class="type">UDFRegistration</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    catalogBuilder: (</span>) <span class="title">=&gt;</span> <span class="title">SessionCatalog</span>,</span></span><br><span class="line"><span class="class">    <span class="title">val</span> <span class="title">sqlParser</span></span>: <span class="type">ParserInterface</span>, <span class="comment">// 是一个ParserInterface实现</span></span><br><span class="line">    analyzerBuilder: () =&gt; <span class="type">Analyzer</span>,</span><br><span class="line">    optimizerBuilder: () =&gt; <span class="type">Optimizer</span>,</span><br><span class="line">    <span class="keyword">val</span> planner: <span class="type">SparkPlanner</span>,</span><br><span class="line">    <span class="keyword">val</span> streamingQueryManager: <span class="type">StreamingQueryManager</span>,</span><br><span class="line">    <span class="keyword">val</span> listenerManager: <span class="type">ExecutionListenerManager</span>,</span><br><span class="line">    resourceLoaderBuilder: () =&gt; <span class="type">SessionResourceLoader</span>,</span><br><span class="line">    createQueryExecution: <span class="type">LogicalPlan</span> =&gt; <span class="type">QueryExecution</span>,</span><br><span class="line">    createClone: (<span class="type">SparkSession</span>, <span class="type">SessionState</span>) =&gt; <span class="type">SessionState</span>,</span><br><span class="line">    <span class="keyword">val</span> columnarRules: <span class="type">Seq</span>[<span class="type">ColumnarRule</span>]) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The following fields are lazy to avoid creating the Hive client when creating SessionState.</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> catalog: <span class="type">SessionCatalog</span> = catalogBuilder()</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> analyzer: <span class="type">Analyzer</span> = analyzerBuilder()</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> optimizer: <span class="type">Optimizer</span> = optimizerBuilder()</span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> resourceLoader: <span class="type">SessionResourceLoader</span> = resourceLoaderBuilder()</span><br></pre></td></tr></table></figure>

<p>构建<code>SparkSession</code>时，Spark内部会构造<code>SessionState</code>，<code>SessionState</code>会构造Parser，Analyzer，Catalog，Optimizer，Planner还有逻辑计划转化为执行计划的方法。<code>SessionState</code>的具体构建如下所示</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * State isolated across sessions, including SQL configurations, temporary tables, registered</span></span><br><span class="line"><span class="comment"> * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].</span></span><br><span class="line"><span class="comment"> * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This is internal to Spark and there is no guarantee on interface stability.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @since 2.2.0</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Unstable</span></span><br><span class="line"><span class="meta">@transient</span></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> sessionState: <span class="type">SessionState</span> = &#123;</span><br><span class="line">  parentSessionState</span><br><span class="line">    .map(_.clone(<span class="keyword">this</span>))</span><br><span class="line">    .getOrElse &#123;</span><br><span class="line">      <span class="keyword">val</span> state = <span class="type">SparkSession</span>.instantiateSessionState(</span><br><span class="line">        <span class="type">SparkSession</span>.sessionStateClassName(sparkContext.conf),</span><br><span class="line">        self)</span><br><span class="line">      initialSessionOptions.foreach &#123; <span class="keyword">case</span> (k, v) =&gt; state.conf.setConfString(k, v) &#125;</span><br><span class="line">      state</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里的<code>SparkSession.sessionStateClassName(sparkContext.conf)</code>具体有两个取值，在使用Hive的时候，是<code>org.apache.spark.sql.hive.HiveSessionStateBuilder</code>，否则是<code>org.apache.spark.sql.internal.SessionStateBuilder</code>，作为in-memory。<br><code>instantiateSessionState</code>会具体构建<code>sessionState</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">instantiateSessionState</span></span>(</span><br><span class="line">    className: <span class="type">String</span>,</span><br><span class="line">    sparkSession: <span class="type">SparkSession</span>): <span class="type">SessionState</span> = &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`</span></span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(className)</span><br><span class="line">    <span class="keyword">val</span> ctor = clazz.getConstructors.head</span><br><span class="line">    ctor.newInstance(sparkSession, <span class="type">None</span>).asInstanceOf[<span class="type">BaseSessionStateBuilder</span>].build()</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s"Error while instantiating '<span class="subst">$className</span>':"</span>, e)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>而最终是通过一个<code>BaseSessionStateBuilder</code>的子类来构建的，我们以<code>HiveSessionStateBuilder</code>为例介绍。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// HiveSessionStateBuilder.scala and BaseSessionStateBuilder.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build</span></span>(): <span class="type">SessionState</span> = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">SessionState</span>(</span><br><span class="line">    session.sharedState,</span><br><span class="line">    conf,</span><br><span class="line">    experimentalMethods,</span><br><span class="line">    functionRegistry,</span><br><span class="line">    udfRegistration,</span><br><span class="line">    () =&gt; catalog,</span><br><span class="line">    sqlParser,</span><br><span class="line">    () =&gt; analyzer,</span><br><span class="line">    () =&gt; optimizer,</span><br><span class="line">    planner,</span><br><span class="line">    () =&gt; streamingQueryManager,</span><br><span class="line">    listenerManager,</span><br><span class="line">    () =&gt; resourceLoader,</span><br><span class="line">    createQueryExecution,</span><br><span class="line">    createClone,</span><br><span class="line">    columnarRules)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="SparkSQL的架构总览"><a href="#SparkSQL的架构总览" class="headerlink" title="SparkSQL的架构总览"></a>SparkSQL的架构总览</h2><p>总而言之，SparkSQL的解析与运行流程类似于一般SQL的解析与运行流程，包含：</p>
<ol>
<li>将SQL解析得到一个逻辑计划，它是一颗AST。SparkSQL的执行目标就是树根的值，在计算过程中，父节点的计算依赖于子节点的计算结果。通过Analyzer去Resolve，通过Optimizer去优化</li>
<li>将逻辑计划转换为物理计划。首先需要为逻辑计划中的节点选择一个最优的物理计划（同样的逻辑计划可能对应多个物理计划），然后需要生成一个可执行的执行计划。</li>
<li>调用执行计划生成的RDD的action方法提交一个Job</li>
</ol>
<h3 id="LogicPlan类"><a href="#LogicPlan类" class="headerlink" title="LogicPlan类"></a>LogicPlan类</h3><p>逻辑计划的对应实现是<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala" target="_blank" rel="noopener">Logical Plan</a>继承了<code>QueryPlan[LogicalPlan]</code>。自己又拥有三个子类<code>BinaryNode</code>/<code>UnaryNode</code>和<code>LeafNode</code>，然后有产生了<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala" target="_blank" rel="noopener">OrderPreservingUnaryNode</a>等子类。这些Node被<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala" target="_blank" rel="noopener">另一些子类</a>所继承，这些<code>basicLogicalOperators</code><a href="https://blog.csdn.net/oopsoom/article/details/38274621" target="_blank" rel="noopener">描述了包括Project/Filter/Sample/Union/Join/Limit等操作</a>。</p>
<h3 id="SparkPlan类"><a href="#SparkPlan类" class="headerlink" title="SparkPlan类"></a>SparkPlan类</h3><p>物理计划的对应实现<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala" target="_blank" rel="noopener">是<code>SparkPlan</code></a>，和<code>LogicalPlan</code>一样，他同样继承了<code>QueryPlan[SparkPlan]</code>。例如逻辑计划<code>Project</code>就可能产生<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/basicPhysicalOperators.scala" target="_blank" rel="noopener">一个<code>ProjectExec</code></a>的物理计划。</p>
<h3 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h3><p>首先祭出一张图。</p>
<p><img src="/img/sparksql/spark-sql-detail.png"></p>
<p>从图中可以看到，SparkSQL首先会对SQL进行Parse，得到一个Unresolved LogicalPlan。这里Unresolved的意思是诸如变量名和表名这些东西是不确定的。Catalog就是描述了<code>SQLContext</code>里面的诸如表之类的对象。<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="noopener">在生产环境中，一般由 Hive Metastore提供Catalog 服务</a>。</p>
<p>在<a href="https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis" target="_blank" rel="noopener">Analyzer</a>的阶段借助于Catalog来决议得到LogicalPlan，将Unresolved LogicalPlan决议为Logical Plan。</p>
<p>通过Optimizer，对Logical Plan进行优化。Catalyst主要做的是RBO，但诸如华为等公司和机构也有提出过CBO的方案。</p>
<p>逻辑计划不能被直接执行，它需要通过<code>QueryPlanner.plan</code>得到一系列物理计划，并选择其中一个。<code>QueryPlanner</code>是一个抽象类，它有可以有许多子类实现，这些子类负责将一系列<code>strategies</code>应用到输入的Logical Plan上，订得到一系列<code>candidates: Seq[PhysicalPlan]</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// QueryPlanner.scala</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">QueryPlanner</span>[<span class="type">PhysicalPlan</span> &lt;: <span class="type">TreeNode</span>[<span class="type">PhysicalPlan</span>]] </span>&#123;</span><br><span class="line">  <span class="comment">/** A list of execution strategies that can be used by the planner */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">strategies</span></span>: <span class="type">Seq</span>[<span class="type">GenericStrategy</span>[<span class="type">PhysicalPlan</span>]]</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">plan</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Iterator</span>[<span class="type">PhysicalPlan</span>] = &#123;</span><br><span class="line">    <span class="comment">// Obviously a lot to do here still...</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Collect physical plan candidates.</span></span><br><span class="line">    <span class="keyword">val</span> candidates = strategies.iterator.flatMap(_(plan))</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>

<p>在得到物理计划后，会调用<code>prepareForExecution</code>得到一个可执行的<code>executedPlan</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// QueryExecution.scala</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">prepareForExecution</span></span>(plan: <span class="type">SparkPlan</span>): <span class="type">SparkPlan</span> = &#123;</span><br><span class="line">  preparations.foldLeft(plan) &#123; <span class="keyword">case</span> (sp, rule) =&gt; rule.apply(sp) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在Spark2.0前，SparkSQL的主要是采用的Volcano查询引擎模型。Volcano是一种<a href="https://zhuanlan.zhihu.com/p/41562506" target="_blank" rel="noopener">经典的基于行的流式迭代模型(Row-Based Streaming Iterator Model)</a>，它在例如Oracle、SQL Server、MySQL等方面都有使用。在Volcano模型中，查询计划树由这些算子组成。这些算子可以看做迭代器，每一次的<code>next()</code>调用，会返回一行(<code>Row</code>)。这<code>next()</code>调用实际上作用在下层算子上，它们把这个下层的输出<strong>看做</strong>一个表。Volcano具有一些性能方面的缺点，例如<code>next()</code>调用深度可能很深，而每次调用都是虚的，所以有很大的查阅虚表的开销，这给编译器做inline，或者CPU做分支预测都带来了困难。此外，Volcano有很好的pipeline性能，能节约内存，但每获得一次数据，都需要最顶层驱动一次，这雪上加霜。因此在Spark2.0之后加入了WholeStageCodegen机制和ExpressionCodegen机制。</p>
<h2 id="SparkSQL的解析流程"><a href="#SparkSQL的解析流程" class="headerlink" title="SparkSQL的解析流程"></a>SparkSQL的解析流程</h2><h2 id="SparkSQL-API的执行流程"><a href="#SparkSQL-API的执行流程" class="headerlink" title="SparkSQL API的执行流程"></a>SparkSQL API的执行流程</h2><p>和RDD一样，Dataset同样只在Action操作才会计算。假设现在已经生成了物理计划，我们选取最典型的<code>count()</code>来研究，以查看物理计划是如何执行的。可以看到，<code>count</code>操作实际上会执行<code>plan.executeCollect()</code>，而这里的<code>plan</code>是一个<code>SparkPlan</code>，<code>qe</code>是一个<code>QueryExecution</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Dataset.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = withAction(<span class="string">"count"</span>, groupBy().count().queryExecution) &#123; plan =&gt;</span><br><span class="line">  plan.executeCollect().head.getLong(<span class="number">0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withAction</span></span>[<span class="type">U</span>](name: <span class="type">String</span>, qe: <span class="type">QueryExecution</span>)(action: <span class="type">SparkPlan</span> =&gt; <span class="type">U</span>) = &#123;</span><br><span class="line">  <span class="type">SQLExecution</span>.withNewExecutionId(sparkSession, qe, <span class="type">Some</span>(name)) &#123;</span><br><span class="line">    qe.executedPlan.foreach &#123; plan =&gt;</span><br><span class="line">      plan.resetMetrics()</span><br><span class="line">    &#125;</span><br><span class="line">    action(qe.executedPlan)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withNewExecutionId</span></span>[<span class="type">U</span>](body: =&gt; <span class="type">U</span>): <span class="type">U</span> = &#123;</span><br><span class="line">  <span class="type">SQLExecution</span>.withNewExecutionId(sparkSession, queryExecution)(body)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>QueryExecution</code>用来描述整个SQL执行的上下文，从如下示例中可以看出，它维护了从Unsolved Logical Plan到Physical Plan的整个转换流程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> ds = <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">1</span>)).toDS</span><br><span class="line">ds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint ... <span class="number">1</span> more field]</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> fds = ds.filter(p =&gt; p.age&gt;<span class="number">1</span>)</span><br><span class="line">fds: org.apache.spark.sql.<span class="type">Dataset</span>[<span class="type">Person</span>] = [name: string, age: bigint ... <span class="number">1</span> more field]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.queryExecution</span><br><span class="line">res9: org.apache.spark.sql.execution.<span class="type">QueryExecution</span> =</span><br><span class="line">== <span class="type">Parsed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">LocalRelation</span> [name#<span class="number">3</span>, age#<span class="number">4</span>L, money#<span class="number">5</span>L]</span><br><span class="line"></span><br><span class="line">== <span class="type">Analyzed</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line">name: string, age: bigint, money: bigint</span><br><span class="line"><span class="type">LocalRelation</span> [name#<span class="number">3</span>, age#<span class="number">4</span>L, money#<span class="number">5</span>L]</span><br><span class="line"></span><br><span class="line">== <span class="type">Optimized</span> <span class="type">Logical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">LocalRelation</span> [name#<span class="number">3</span>, age#<span class="number">4</span>L, money#<span class="number">5</span>L]</span><br><span class="line"></span><br><span class="line">== <span class="type">Physical</span> <span class="type">Plan</span> ==</span><br><span class="line"><span class="type">LocalTableScan</span> [name#<span class="number">3</span>, age#<span class="number">4</span>L, money#<span class="number">5</span>L]</span><br></pre></td></tr></table></figure>

<p>那么，<code>count()</code>做的就是对<code>qe</code>做一些手脚，然后调用<code>qe.executedPlan.executeCollect().head.getLong(0)</code>。于是我们查看<code>executeCollect()</code>这个方法，他实际上就是execute和collect两部分。execute部分实际上是对<code>getByteArrayRdd</code>的一个调用，得到一个RDD。而collect部分就是调用<code>byteArrayRdd.collect()</code>，这个操作会<strong>触发RDD的Action</strong>操作，从而提交一个Job。整个函数最终返回一个<code>ArrayBuffer[InternalRow]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkPlan.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">executeCollect</span></span>(): <span class="type">Array</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line">  <span class="comment">// byteArrayRdd是一个RDD[(Long, Array[Byte])]</span></span><br><span class="line">  <span class="keyword">val</span> byteArrayRdd = getByteArrayRdd()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> results = <span class="type">ArrayBuffer</span>[<span class="type">InternalRow</span>]()</span><br><span class="line">  byteArrayRdd</span><br><span class="line">    .collect()</span><br><span class="line">    .foreach &#123; countAndBytes =&gt;</span><br><span class="line">    decodeUnsafeRows(countAndBytes._2).foreach(results.+=)</span><br><span class="line">  &#125;</span><br><span class="line">  results.toArray</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>getByteArrayRdd</code>的作用是将一系列<code>UnsafeRow</code>打包成一个<code>Array[Byte]</code>以方便序列化，这个<code>Array[Byte]</code>的结构是<code>[size] [bytes of UnsafeRow] [size] [bytes of UnsafeRow] ... [-1]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getByteArrayRdd</span></span>(n: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Array</span>[<span class="type">Byte</span>])] = &#123;</span><br><span class="line">  execute() <span class="comment">// 得到一个RDD[InternalRow]了</span></span><br><span class="line">  .mapPartitionsInternal &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](<span class="number">4</span> &lt;&lt; <span class="number">10</span>)  <span class="comment">// 4K</span></span><br><span class="line">    <span class="keyword">val</span> codec = <span class="type">CompressionCodec</span>.createCodec(<span class="type">SparkEnv</span>.get.conf)</span><br><span class="line">    <span class="keyword">val</span> bos = <span class="keyword">new</span> <span class="type">ByteArrayOutputStream</span>()</span><br><span class="line">    <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(codec.compressedOutputStream(bos))</span><br><span class="line">    <span class="comment">// `iter.hasNext` may produce one row and buffer it, we should only call it when the limit is</span></span><br><span class="line">    <span class="comment">// not hit.</span></span><br><span class="line">    <span class="keyword">while</span> ((n &lt; <span class="number">0</span> || count &lt; n) &amp;&amp; iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> row = iter.next().asInstanceOf[<span class="type">UnsafeRow</span>]</span><br><span class="line">      out.writeInt(row.getSizeInBytes)</span><br><span class="line">      row.writeToStream(out, buffer)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    out.writeInt(<span class="number">-1</span>)</span><br><span class="line">    out.flush()</span><br><span class="line">    out.close()</span><br><span class="line">    <span class="type">Iterator</span>((count, bos.toByteArray))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看到<code>getByteArrayRdd</code>中调用了<code>execute</code>方法，<code>execute</code>继而调用<code>doExecute</code>，得到一个<code>RDD[InternalRow]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = executeQuery &#123;</span><br><span class="line">  <span class="keyword">if</span> (isCanonicalizedPlan) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"A canonicalized plan is not supposed to be executed."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  doExecute()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>]</span><br></pre></td></tr></table></figure>

<p>由于SparkPlan是一个抽象类，所以这里的<code>doExecute()</code>没有看到实现，具体的实现根据其操作对象的不同分布在<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala" target="_blank" rel="noopener">objects.scala</a>上。</p>
<p>那么<code>execute()</code>调用链的终点是什么呢？显然它一定是一个<code>LeafNode</code>的子类。而通过上面的解析可以看到，我们最终得到的一个物理计划是一个<code>LocalTableScanExec</code>，它继承于<code>LeafNodeExec</code>。</p>
<h2 id="map操作分析"><a href="#map操作分析" class="headerlink" title="map操作分析"></a>map操作分析</h2><p>在Dataset中，同样提供了诸如<code>map</code>之类的算子，不过它们的实现是从<code>Dataset</code>和<code>DataFrame</code>之间的变换了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span> : <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapElements</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span> : <span class="type">Encoder</span>, <span class="type">U</span> : <span class="type">Encoder</span>](</span><br><span class="line">      func: <span class="type">AnyRef</span>,</span><br><span class="line">      child: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> deserialized = <span class="type">CatalystSerde</span>.deserialize[<span class="type">T</span>](child)</span><br><span class="line">    <span class="keyword">val</span> mapped = <span class="type">MapElements</span>(</span><br><span class="line">      func,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].clsTag.runtimeClass,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].schema,</span><br><span class="line">      <span class="type">CatalystSerde</span>.generateObjAttr[<span class="type">U</span>],</span><br><span class="line">      deserialized)</span><br><span class="line">    <span class="type">CatalystSerde</span>.serialize[<span class="type">U</span>](mapped)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MapElements</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    func: <span class="type">AnyRef</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    argumentClass: <span class="type">Class</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    argumentSchema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    outputObjAttr: <span class="type">Attribute</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    child: <span class="type">LogicalPlan</span></span>) <span class="keyword">extends</span> <span class="title">ObjectConsumer</span> <span class="keyword">with</span> <span class="title">ObjectProducer</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">TypedFilter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span> : <span class="type">Encoder</span>](func: <span class="type">AnyRef</span>, child: <span class="type">LogicalPlan</span>): <span class="type">TypedFilter</span> = &#123;</span><br><span class="line">    <span class="type">TypedFilter</span>(</span><br><span class="line">      func,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].clsTag.runtimeClass,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].schema,</span><br><span class="line">      <span class="type">UnresolvedDeserializer</span>(encoderFor[<span class="type">T</span>].deserializer),</span><br><span class="line">      child)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Spark性能调优"><a href="#Spark性能调优" class="headerlink" title="Spark性能调优"></a>Spark性能调优</h1><p>一般来说，Spark可能出现瓶颈的地方有内存、网络和CPU，对于上面的这些问题，宜分为Driver和Executor两块进行考虑<br>内存方面的向硬盘的溢写、从gc.log中看到的GC的猛增、节点的未响应和OOM。<br>网络问题的主要场景是诸如Shuffle类的操作涉及在多个节点上传输，节点之间Connection reset by peer。</p>
<h2 id="Spark常见性能问题和选项"><a href="#Spark常见性能问题和选项" class="headerlink" title="Spark常见性能问题和选项"></a>Spark常见性能问题和选项</h2><h3 id="总体列表"><a href="#总体列表" class="headerlink" title="总体列表"></a>总体列表</h3><table>
<thead>
<tr>
<th align="center">诊断</th>
<th align="center">现象</th>
<th align="center">解决方案</th>
</tr>
</thead>
<tbody><tr>
<td align="center">Executor内存不足</td>
<td align="center">Driver端ExecutorLostFailure，Executor端gc.log显示大量GC和FullGC</td>
<td align="center">需要考虑Shuffle Read数据过大，或者数据倾斜。对于前者，可以考虑增加分区数或者换个Partitioner，增加Executor内存，增加Executor数量，减少Executor上的Task并行度，提前Filter，使用序列化。</td>
</tr>
<tr>
<td align="center">Executor内存不足</td>
<td align="center">Local Bytes Read+Remote Bytes Read很大</td>
<td align="center">考虑是Shuffle Read的问题，同上。需要注意的是当使用groupBy系列算子时，可能一个KV对就很大的，所以增加Executor内存会更保险</td>
</tr>
<tr>
<td align="center">Driver内存不足</td>
<td align="center">Driver端gc.log显示大量GC和FullGC，spark.log中DAGScheduler相关log显示collect算子耗时过长</td>
<td align="center">考虑增大Driver内存，避免collect大量数据</td>
</tr>
<tr>
<td align="center">Driver内存不足</td>
<td align="center">Driver端gc.log显示大量GC和FullGC</td>
<td align="center">减少UDF的使用，减少诸如withColumn的使用</td>
</tr>
<tr>
<td align="center">Driver内存不足</td>
<td align="center">Driver端gc.log显示大量GC和FullGC，Driver的spark.log中出现大量<code>BlockManagerInfo: Added broadcast</code>，并且剩余内存较少，Executor的spark.log中出现<code>TorrentBroadcast: Reading broadcast</code>事件且耗时过长</td>
<td align="center">减少broadcast的数据量</td>
</tr>
<tr>
<td align="center">数据倾斜</td>
<td align="center">部分Task Retry比较多</td>
<td align="center">repartition</td>
</tr>
<tr>
<td align="center">数据倾斜</td>
<td align="center">少数Task耗时显著高于平均值</td>
<td align="center">考虑换个Partitioner，扩大<code>spark.shuffle.file.buffer</code>、<code>spark.reducer.maxSizeInFlight</code>、<code>spark.shuffle.memoryFraction</code>，打开<code>spark.shuffle.consolidateFiles</code></td>
</tr>
<tr>
<td align="center">分区过多</td>
<td align="center">Task执行的时间都很短，但整个Stage耗时较长</td>
<td align="center">使用<code>coalesce</code>减少分区数</td>
</tr>
<tr>
<td align="center">分区过少</td>
<td align="center">Task执行的时间都很长</td>
<td align="center">使用<code>repartition</code>增加分区数</td>
</tr>
<tr>
<td align="center">Shuffle Spill</td>
<td align="center">大部分Task执行时间长，但计算量不大</td>
<td align="center">增加partition数量，增大Executor内存</td>
</tr>
<tr>
<td align="center">Shuffle Write过大</td>
<td align="center">部分节点<a href="https://juejin.im/post/5dc3ed336fb9a04a7847f25c" target="_blank" rel="noopener">出现<code>FetchFailedException</code>错误</a></td>
<td align="center"></td>
</tr>
<tr>
<td align="center">RDD重复计算</td>
<td align="center">通过Eventlog，追踪Cached的RDD。在同Stage中一个RDD被Cache，那么它的子RDD也会被Cache</td>
<td align="center">持久化该RDD或使用SparkSQL改写</td>
</tr>
<tr>
<td align="center">HDFS IO过大</td>
<td align="center"></td>
<td align="center"><a href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank" rel="noopener">当并行Task过多时，会导致HDFS读取瓶颈</a></td>
</tr>
<tr>
<td align="center"></td>
<td align="center">RM界面显示Tracking URL:    UNASSIGNED</td>
<td align="center">目前App还没启动运行</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">RM界面显示State:    ACCEPTED</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">计算开销较大</td>
<td align="center"></td>
<td align="center"><a href="https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/ch19.html#s4c4---performance-tuning" target="_blank" rel="noopener">减少UDF和UDAF的使用</a>；减少不必要的排序</td>
</tr>
<tr>
<td align="center">集群缺少资源</td>
<td align="center">状态是ACCEPTED而不是RUNNING。Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources</td>
<td align="center">修改资源</td>
</tr>
</tbody></table>
<h3 id="替代性算子"><a href="#替代性算子" class="headerlink" title="替代性算子"></a>替代性算子</h3><p>为了避免由于Shuffle操作导致的性能问题，常用的解决方案是使用map-side-combine的算子。这个思路就是先将聚合操作下推到每个节点本地，再将每个节点上的聚合结果拉到同一节点上进行聚合，这样能够显著减少通信量。这种方法的常见实践就是采用如下所示的一些替代性算子:</p>
<table>
<thead>
<tr>
<th align="center">原算子</th>
<th align="center">替代算子</th>
<th align="center">备注</th>
</tr>
</thead>
<tbody><tr>
<td align="center">groupByKey</td>
<td align="center">reduceByKey/aggregateByKey</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">reduceByKey</td>
<td align="center">aggregateByKey</td>
<td align="center"><a href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank" rel="noopener">当reduceByKey的输入和输出不一致时，创建临时对象（例如从T变为List[T]）有额外开销</a></td>
</tr>
<tr>
<td align="center">aggregate</td>
<td align="center">treeAggregate</td>
<td align="center"><a href="/2020/05/15/spark-ml-lib/">根据实验，treeAggregate具有更好的效率</a></td>
</tr>
<tr>
<td align="center">foreach</td>
<td align="center">foreachPartitions</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">filter</td>
<td align="center">filter+coalesce</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">repartition+sort</td>
<td align="center">repartitionAndSortWithinPartitions</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">repartition</td>
<td align="center">coalesce</td>
<td align="center">如果目标分区数量小于当前分区数量</td>
</tr>
<tr>
<td align="center">flatMap-join-groupBy</td>
<td align="center">cogroup</td>
<td align="center"><a href="https://blog.cloudera.com/how-to-tune-your-apache-spark-jobs-part-1/" target="_blank" rel="noopener">避免pack和unpack group操作</a></td>
</tr>
</tbody></table>
<p>需要注意的是，减少Shuffle未必就是好的，例如对于<a href="https://www.cnblogs.com/jiangxiaoxian/p/9539760.html" target="_blank" rel="noopener">coalesce而言</a>，如果产生文件过少，很可能导致Executor空转，并且某些Executor OOM的问题，这就类似于说不患寡而患不均。</p>
<h3 id="使用DataFrame-API"><a href="#使用DataFrame-API" class="headerlink" title="使用DataFrame API"></a>使用DataFrame API</h3><p>使用DataFrame API在下面的几个方面有性能优势。</p>
<ol>
<li>Spark Catalyst优化器能够进行优化</li>
<li>Tungsten优化器的引入带来了三点性能提升（见上文）</li>
</ol>
<h3 id="有关Persist的优化方案"><a href="#有关Persist的优化方案" class="headerlink" title="有关Persist的优化方案"></a>有关Persist的优化方案</h3><p>根据<a href="https://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose" target="_blank" rel="noopener">RDD Programming Guide</a>，即使是RDD（DF的话有优化器另说）Spark也可能会自动做persist，具体是发生在Shuffle过程中，这样可以避免在某个Node失败之后还要重新计算全部。但是对于肯定需要复用的数据，显式persist并没有坏处。这里需要注意的是我们要尽量提高RDD的复用程度。<br>一般来说，如果内存中能够全部放下对象，选择默认的<code>MEMORY_ONLY</code>级别能够最大程度利用CPU，否则就需要考虑使用序列化的<code>MEMORY_ONLY_SER</code>存储。当内存再不够时，就需要考虑将其持久化到磁盘上，但这会带来较高的时间代价。虽然在Spark的较新版本中，通过Unsafe Shuffle可以直接对序列化之后的对象进行sort shuffle，但这不是通用的。</p>
<h3 id="一些Case"><a href="#一些Case" class="headerlink" title="一些Case"></a>一些Case</h3><h4 id="Task-Retry"><a href="#Task-Retry" class="headerlink" title="Task Retry"></a>Task Retry</h4><div style="display:none">
LolLegsTsFeature.scala
</div>

<p>我们可以看到，在优化前Job4耗时1.6h，并且Fail了不少。<br><img src="/img/sparksql/case1/ori_job_4.png"><br>点进去看一下，发现这个Job里面7重试了两次，花了1h了。<br><img src="/img/sparksql/case1/ori_stage_4.png"><br>点进去这个Stage看看，他实际上就是一个<code>rdd.join</code>函数的调用。<br><img src="/img/sparksql/case1/stage7.png"><br>我们可以看到，某个任务的时间达到了1.0h，而其他任务的耗时都在2min左右，因此可以认为这里分区有问题。<br><img src="/img/sparksql/case1/tasks2.png"><br>因此，我们在这里重新repartition了一下，现在运行时间缩小到了44min。<br><img src="/img/sparksql/case1/new_job_4.png"></p>
<h2 id="Spark日志"><a href="#Spark日志" class="headerlink" title="Spark日志"></a>Spark日志</h2><p>Spark会记录Event log，并在History Server或者Spark UI中供访问调试使用。</p>
<h3 id="HistoryServer"><a href="#HistoryServer" class="headerlink" title="HistoryServer"></a>HistoryServer</h3><p>Spark提供了History Server以保存<a href="https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_EventLog.md" target="_blank" rel="noopener">Event Log</a>，以便追踪历史任务的性能。History Server部署在18080，可以使用WebUI，也可以使用18080的<code>/api/vi/application</code>的api来请求json版本。<br>这种方式需要在运行前手动<code>export SPARK_MASTER_HOST=localhost</code>（会被诸如start-master.sh等文件访问修改）或者<code>sh ./sbin/start-master.sh -h localhost &amp;&amp; ./sbin/start-slave.sh spark://localhost:7077 </code>可以通过<code>-h</code>指定localhost。不然可能Slave会连不上localhost，因为他会访问你的电脑名字，例如<code>CALVINNEO-MB0:7077</code>而不是localhost。<br>在<code>spark-defaults.conf</code>中，有关Event Log的配置项有两种，一个是在HDFS上，一个是在硬盘上。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 硬盘</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://localhost:9000/user/spark/appHist</span><br><span class="line"># HDFS</span><br><span class="line">spark.history.fs.logDirectory      .../spark-2.4.4-bin-hadoop2.7/conf/history/spark-events</span><br></pre></td></tr></table></figure>

<p>这个在磁盘上，供给History Server用，但是实际上和HDFS的内容是一样的。需要注意的是，一旦<code>spark.eventLog.enabled</code>被设置为<code>True</code>，就需要保证9000是可以访问的，不然可能会报错。</p>
<h3 id="spark-log"><a href="#spark-log" class="headerlink" title="spark log"></a>spark log</h3><p>在每个节点的spark.log记载了这个节点的Spark日志，其中日志从低到高有TRACE、DEBUG、INFO、WARN、ERROR、FATAL等级别。INFO是默认级别。</p>
<h3 id="gc-log"><a href="#gc-log" class="headerlink" title="gc log"></a>gc log</h3><p>在每个节点的gc.log上记载有这个节点JVM的GC情况</p>
<h2 id="常用调试方法"><a href="#常用调试方法" class="headerlink" title="常用调试方法"></a>常用调试方法</h2><ol>
<li><p>查看RDD的分区数</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.partitions.size</span><br></pre></td></tr></table></figure></li>
<li><p>查看RDD的logical plan</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.toDebugString</span><br></pre></td></tr></table></figure></li>
<li><p>查看queryExecution</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d.queryExecution</span><br></pre></td></tr></table></figure></li>
<li><p>查看schema</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d.printSchema</span><br></pre></td></tr></table></figure></li>
<li><p>查看查询计划</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.explain</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Spark-on-YARN的配置参数"><a href="#Spark-on-YARN的配置参数" class="headerlink" title="Spark on YARN的配置参数"></a>Spark on YARN的配置参数</h2><p>在Spark1.2之后，Spark on YARN就已经支持动态资源分配了，当然这个机制在后面的版本中进行了迭代，我们以2.x版本为主进行介绍。主要参考了<a href="https://docs.qubole.com/en/latest/admin-guide/engine-admin/spark-admin/autoscale-spark.html" target="_blank" rel="noopener">文章</a>。</p>
<h3 id="Executor数量"><a href="#Executor数量" class="headerlink" title="Executor数量"></a>Executor数量</h3><p>有下面的一些字段需要考虑：</p>
<ol>
<li><code>spark.dynamicAllocation.maxExecutors/minExecutors</code></li>
<li><code>num-executors</code></li>
<li><code>spark.dynamicAllocation.initialExecutors</code></li>
</ol>
<p>首先可以通过<code>spark.dynamicAllocation.maxExecutors</code>和<code>spark.dynamicAllocation.minExecutors</code>来限定最大和最小的Executor数量。<br>一开始Spark会启动<code>num-executors</code>数量个节点，我们可以设置一个较多的Executor节点执行一个小型任务，并跟踪<code>INFO yarn.YarnAllocator</code>，可以发现，最终会减少Executor需要的数量。此外，还有个<code>spark.dynamicAllocation.initialExecutors</code>，根据我的实践，如果同时设定<code>num-executors</code>和<code>spark.dynamicAllocation.initialExecutors</code>，那么后者的优先级通常会更高。我的实践是采用了下面的配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--conf spark.dynamicAllocation.minExecutors=2  --conf spark.dynamicAllocation.maxExecutors=20  --conf spark.dynamicAllocation.enabled=true --conf spark.dynamicAllocation.initialExecutors=3 --conf spark.dynamicAllocation.maxExecutors=10  --num-executors 0  --driver-memory 1g  --executor-memory 1g  --executor-cores 2</span><br></pre></td></tr></table></figure>

<p>结果发现，一开始启动了3个Executor，最后变成了2个。<br><img src="/img/sparksql/yarn-exe-1.png"><br><img src="/img/sparksql/yarn-exe-2.png"><br>我猜想这是因为如果打开了<code>dynamicAllocation</code>，那么<code>spark.dynamicAllocation</code>相关配置就会更高优先级，而<code>num-executors</code>实际上是一个较为陈旧的配置。我在<a href="https://stackoverflow.com/questions/62768677/what-is-the-relationship-between-maxexecutors-num-executors-and-initialexecutor" target="_blank" rel="noopener">爆栈网上提了个问题</a>，希望有人能证实我的思想。</p>
<p>与此同时，当设置了<code>spark.dynamicAllocation.minExecutors</code>后，就不能设置<code>spark.dynamicAllocation.initialExecutors</code>或者<code>spark.executor.instances/num-executors</code>。【Q】如果它的值小于<code>spark.dynamicAllocation.minExecutors</code>，对于这种情况会尝试请求<code>spark.dynamicAllocation.minExecutors</code>这么多个Executor。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">20/07/09 15:53:29 WARN Utils: spark.dynamicAllocation.initialExecutors less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.</span><br><span class="line">20/07/09 15:53:29 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.</span><br></pre></td></tr></table></figure>

<p><code>initialExecutors</code>的设置数量，我司设置是最大数量<code>maxExecutors</code>(然而对外叫<code>num-executors</code>)的平方根。</p>
<p><a href="https://docs.qubole.com/en/latest/admin-guide/engine-admin/spark-admin/autoscale-spark.html" target="_blank" rel="noopener">文章</a>指出，一旦<code>num-executors</code>或者<code>spark.dynamicAllocation.minExecutors</code>配置了，并且实际被分配了，那么就永远不会少于这个数量了，但在上面的实践中感觉并不是这样。根据<a href="https://stackoverflow.com/questions/62768677/what-is-the-relationship-between-maxexecutors-num-executors-and-initialexecutor" target="_blank" rel="noopener">知乎</a>，应该<code>spark.dynamicAllocation.minExecutors</code>是下限。</p>
<p>当Executor空闲超过<a href="https://www.jianshu.com/p/826991ba9375" target="_blank" rel="noopener"><code>spark.dynamicAllocation.executorIdleTimeout</code></a>，那么就会被移出，可以设置<code>spark.dynamicAllocation.cachedExecutorIdleTimeout</code>来避免移除缓存了数据的Executor。当有Task等待时间超过<a href="https://www.jianshu.com/p/826991ba9375" target="_blank" rel="noopener"><code>spark.dynamicAllocation.schedulerBacklogTimeout</code></a>后，会加入新的Executor。一般来说，如果一个Executor空闲60s后将被移出，而如果有Task在backlog中等待1s将会新增Executor。</p>
<p>需要注意的是，并不是Spark集群得不到<code>spark.dynamicAllocation.minExecutors</code>个节点他就不能运行了。事实上Spark任务在Accepted到Running的阶段，Yarn只会先分配给driver这一个container，然后再由Driver来申请它需要的Executor，这个过程也能从上面的log中看出。</p>
<h3 id="Executor内存和CPU"><a href="#Executor内存和CPU" class="headerlink" title="Executor内存和CPU"></a>Executor内存和CPU</h3><p>对于yarn来说，每个Node有NodeManager负责管理。NodeManager主要有两个配置：</p>
<ol>
<li><code>yarn.nodemanager.resource.memory-mb</code>表示每个node上，每个Container能够最多跑的内存。<br> 需要注意的是<code>--executor-memory/spark.executor.memory</code>不能和YARN中的<code>yarn.nodemanager.resource.memory-mb</code>直接对应，原因是Spark可以请求一些堆外内存，因此实际上要请求<code>(1+spark.yarn.executor.memoryOverhead) * spark.executor.memory</code>这么多的内存，这个Overhead的比例大概在7%左右。然后YARN实际分配的内存也会多一点，具体有<code>yarn.scheduler.minimum-allocation-mb</code> 和 <code>yarn.scheduler.increment-allocation-mb</code>控制。</li>
<li><code>yarn.nodemanager.resource.cpu-vcores</code>表示每个node上，每个Container能够最多跑的核。这里的vcore应该是YARN的调度概念，申请5个<code>executor-cores</code>，等于要YARN调度5个vcore。<br> 这里需要注意的是，每个NodeManager自己最好也要保留一个核，比如说我们给每个Executor分配3个核，那么在一台16核的机器上，我们正好可以分配5台机器，剩下来的一个核心给NodeManager。</li>
</ol>
<p><img src="/img/sparksql/spark-yarn-mem.png"></p>
<h3 id="Driver内存"><a href="#Driver内存" class="headerlink" title="Driver内存"></a>Driver内存</h3><p>主要考虑collect的大小</p>
<h1 id="Spark常见问题的解决方案"><a href="#Spark常见问题的解决方案" class="headerlink" title="Spark常见问题的解决方案"></a>Spark常见问题的解决方案</h1><p>不得不说，Spark的相关问题很多还是比较难调试的，这是因为Spark它的错误日志在打印堆栈时往往喜欢打印它的内部状态，我们很难根据它的内部状态去trace到底是我们的什么操作导致它产生这个问题。并且Spark上处理的数据量规模一般都很大，并且都跑在诸如YARN托管的集群上，这个给Spark调试带了了更大的麻烦。</p>
<h2 id="变量在节点之间共享"><a href="#变量在节点之间共享" class="headerlink" title="变量在节点之间共享"></a>变量在节点之间共享</h2><p>当我们需要在节点间共享变量，例如将某个字符串从Driver发送到Executor上时，需要这个变量能够被序列化。特别地，有一个经典的Bug就是<code>Map#mapValues</code>不能被序列化，这个解决方案是在<code>mapValues</code>之后再<code>map(identity)</code>一下。<br>特别需要注意的是因为RDD是分布式存储的，所以不能够直接当做变量处理，例如下面的代码是不能够使用的。对于这种情况，要么是将其中一个小RDD广播，要不就是将两个RDD去做个JOIN。在SparkSQL中，JOIN操作会被视情况优化为广播。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd1.map&#123;</span><br><span class="line">    rdd2.filter(...)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer"><a href="#scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer" class="headerlink" title="scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer"></a>scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer</h2><p>根据<a href="https://stackoverflow.com/questions/40199507/scala-collection-mutable-wrappedarrayofref-cannot-be-cast-to-integer" target="_blank" rel="noopener">SoF</a>，这个错误就是把<code>Array</code>改成<code>Seq</code>就好了。</p>
<h2 id="Extracting-Seq-String-String-String-from-spark-DataFrame"><a href="#Extracting-Seq-String-String-String-from-spark-DataFrame" class="headerlink" title="Extracting Seq[(String,String,String)] from spark DataFrame"></a>Extracting <code>Seq[(String,String,String)]</code> from spark DataFrame</h2><p>这个错误发生在我们想往一个Row里面放一个类型为Seq的字段的时候。根据<a href="https://stackoverflow.com/questions/37553059/extracting-seqstring-string-string-from-spark-dataframe" target="_blank" rel="noopener">SoF</a>，我们可以通过以下的方式来复现这个问题。我们创建了一个以<code>Record</code>为元素的Row，里面有一个<code>content_processed</code>，它的类型是<code>Seq[Feature])</code>，现在我们希望将<code>Record</code>里面的<code>id</code>字段搞掉，我们可以写出这样的代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">Row</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Feature</span>(<span class="params">lemma: <span class="type">String</span>, pos_tag: <span class="type">String</span>, ne_tag: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Record</span>(<span class="params">id: <span class="type">Long</span>, content_processed: <span class="type">Seq</span>[<span class="type">Feature</span>]</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">df</span> </span>= <span class="type">Seq</span>(</span><br><span class="line">  <span class="type">Record</span>(<span class="number">1</span>L, <span class="type">Seq</span>(</span><br><span class="line">    <span class="type">Feature</span>(<span class="string">"ancient"</span>, <span class="string">"jj"</span>, <span class="string">"o"</span>),</span><br><span class="line">    <span class="type">Feature</span>(<span class="string">"olympia_greece"</span>, <span class="string">"nn"</span>, <span class="string">"location"</span>)</span><br><span class="line">  ))</span><br><span class="line">).toDF</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> seems_right = df.map(row =&gt; row.getAs[<span class="type">Seq</span>[<span class="type">Feature</span>]](<span class="string">"content_processed"</span>))</span><br><span class="line"><span class="comment">// res10: org.apache.spark.sql.Dataset[Seq[Feature]] = [value: array&lt;struct&lt;lemma:string,pos_tag:string,ne_tag:string&gt;&gt;]</span></span><br><span class="line"><span class="keyword">val</span> err = seems_right.first</span><br></pre></td></tr></table></figure>

<p>当我们对得到的<code>seems_right</code>执行Action触发计算时，就会得到错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be cast to $line67.$read$$iw$$iw$Feature</span><br><span class="line">        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.serializefromobject_doConsume_0$(Unknown Source)</span><br></pre></td></tr></table></figure>

<p>这个原因还是上面提到的Row的缺陷。其解决方案是借助于Dataset将Row转换为其他的数据结构，例如本命的Record，或者一个能够pattern match这个Record类的数据结构，例如<code>(Long, Seq[(String, String, String)])</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.as[<span class="type">Record</span>].map(_.content_processed).first</span><br><span class="line">df.as[(<span class="type">Long</span>, <span class="type">Seq</span>[(<span class="type">String</span>, <span class="type">String</span>, <span class="type">String</span>)])].map(_._2).first</span><br></pre></td></tr></table></figure>

<h2 id="考虑集群机器的问题"><a href="#考虑集群机器的问题" class="headerlink" title="考虑集群机器的问题"></a>考虑集群机器的问题</h2><p>集群中有些机器是Power PC(PPC)，这些机器可能会去报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(Possible cause: can&apos;t load Power PC 64 LE-bit .so on a AARCH64-bit platform)</span><br><span class="line">        at java.lang.ClassLoader$NativeLibrary.load(Native Method)</span><br></pre></td></tr></table></figure>

<p>这时候需要设置<code>spark.blacklist.enabled=true</code>把它们blacklist掉。</p>
<h2 id="Spark-SQL编写技巧"><a href="#Spark-SQL编写技巧" class="headerlink" title="Spark SQL编写技巧"></a>Spark SQL编写技巧</h2><p>详见Spark相关机制详解中JOIN相关的章节。</p>
<h1 id="Spark的其他组件的简介"><a href="#Spark的其他组件的简介" class="headerlink" title="Spark的其他组件的简介"></a>Spark的其他组件的简介</h1><h2 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h2><p>GraphX是基于Spark实现的一个图计算框架，能够对图进行建模。GraphX内置了一些实用的方法，如PageRank、SCC等，同时也提供了Pregel算法的API，我们可以利用Pregel来实现自己的一些图算法。目前GraphX似乎还没有实用的Python API，比较方便的是借助Scala。</p>
<h2 id="ML和MLLib"><a href="#ML和MLLib" class="headerlink" title="ML和MLLib"></a>ML和MLLib</h2><p>ML和MLLib是Spark机器学习库的两个不同实现。其中MLLib是较老的基于RDD的实现，而ML是较新的基于Dataset的实现</p>
<h2 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h2><h1 id="Spark相关机制详解"><a href="#Spark相关机制详解" class="headerlink" title="Spark相关机制详解"></a>Spark相关机制详解</h1><h2 id="JOIN相关"><a href="#JOIN相关" class="headerlink" title="JOIN相关"></a>JOIN相关</h2><p>Spark有三种Join方式，ShuffledHashJoin、BroadcastHashJoin、SortMergeJoin等。这三种方式都会涉及到数据的传输，所以JOIN的代价是比较大的。<br>前两种属于HashJoin的范畴，HashJoin一般就是将小表做为BuildTable，将大表作为ProbeTable。BuildTable采用Hash进行索引，在JOIN时，对大表进行遍历，并在BuildTable中进行查找JOIN。<br>后一种SortMergeJoin一般是对于两个大表而言的，将两个表都进行排序，然后采用类似归并排序的办法进行JOIN。问题是，这个过程是怎么并行的呢？一个简单的想法是如果我们Sort时候保证两个表的相同的Key都出现在一个Partition里面，那么对这个Partition做merge，就可以得到完整的结果。Spark是这样做的么？是的，<a href="https://cloud.tencent.com/developer/article/1005502" target="_blank" rel="noopener">Spark会先做一次Shuffle，把可能被JOIN的Key先划分到一个分区里面</a>。</p>
<h2 id="Repartition相关"><a href="#Repartition相关" class="headerlink" title="Repartition相关"></a>Repartition相关</h2><p>我们知道，<code>rdd.repartition</code>只是<code>rdd.coalesce</code>的别名，所以我们讨论后者。按照惯例，先上代码</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD.scala</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD that is reduced into `numPartitions` partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This results in a narrow dependency, e.g. if you go from 1000 partitions</span></span><br><span class="line"><span class="comment"> * to 100 partitions, there will not be a shuffle, instead each of the 100</span></span><br><span class="line"><span class="comment"> * new partitions will claim 10 of the current partitions. If a larger number</span></span><br><span class="line"><span class="comment"> * of partitions is requested, it will stay at the current number of partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,</span></span><br><span class="line"><span class="comment"> * this may result in your computation taking place on fewer nodes than</span></span><br><span class="line"><span class="comment"> * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span></span><br><span class="line"><span class="comment"> * you can pass shuffle = true. This will add a shuffle step, but means the</span></span><br><span class="line"><span class="comment"> * current upstream partitions will be executed in parallel (per whatever</span></span><br><span class="line"><span class="comment"> * the current partitioning is).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note With shuffle = true, you can actually coalesce to a larger number</span></span><br><span class="line"><span class="comment"> * of partitions. This is useful if you have a small number of partitions,</span></span><br><span class="line"><span class="comment"> * say 100, potentially with a few partitions being abnormally large. Calling</span></span><br><span class="line"><span class="comment"> * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span></span><br><span class="line"><span class="comment"> * data distributed using a hash partitioner. The optional partition coalescer</span></span><br><span class="line"><span class="comment"> * passed in must be serializable.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">             partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">            (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br><span class="line">    : <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  require(numPartitions &gt; <span class="number">0</span>, <span class="string">s"Number of partitions (<span class="subst">$numPartitions</span>) must be positive."</span>)</span><br><span class="line">  <span class="keyword">if</span> (shuffle) &#123;</span><br><span class="line">    <span class="comment">/** Distributes elements evenly across output partitions, starting from a random partition. */</span></span><br><span class="line">    <span class="keyword">val</span> distributePartition = (index: <span class="type">Int</span>, items: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> position = <span class="keyword">new</span> <span class="type">Random</span>(hashing.byteswap32(index)).nextInt(numPartitions)</span><br><span class="line">      items.map &#123; t =&gt;</span><br><span class="line">        <span class="comment">// Note that the hash code of the key will just be the key itself. The HashPartitioner</span></span><br><span class="line">        <span class="comment">// will mod it with the number of total partitions.</span></span><br><span class="line">        position = position + <span class="number">1</span></span><br><span class="line">        (position, t)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; : <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">T</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// include a shuffle step so that our upstream tasks are still distributed</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">Int</span>, <span class="type">T</span>, <span class="type">T</span>](</span><br><span class="line">        mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = <span class="literal">true</span>),</span><br><span class="line">        <span class="keyword">new</span> <span class="type">HashPartitioner</span>(numPartitions)),</span><br><span class="line">      numPartitions,</span><br><span class="line">      partitionCoalescer).values</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">CoalescedRDD</span>(<span class="keyword">this</span>, numPartitions, partitionCoalescer)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>首先，对于有Shuffle的情况，是<code>CoalescedRDD</code>里面套了一个<code>ShuffledRDD</code>。<br>首先来看这个<code>ShuffledRDD</code>，是用的<code>HashPartitioner</code>，这个是用<code>key.hashCode</code>,去模<code>numPartitions</code>来进行分区的，很简单。<br><code>ShuffledRDD</code>的主要逻辑在<code>mapPartitionsWithIndexInternal</code>函数，它会去<code>mapPartitions</code>，然后加上一个表示分区索引的index。这个index是怎么指定的呢？实际上是<code>distributePartition</code>来做的，这个函数接受一个<code>Int</code>，和一个<code>Iterator[T]</code>，表示一个分区里面所有的元素。这个函数是随机的，也就是对每个原有分区里面的项目，将它们随机分到某个分区里面，因此它并不保证原来相邻的条目最后还是会落到相邻的机器上。这里Internal的意思就是不会去调用<code>sc.clean(f)</code>。<br>下面看看<code>distributePartition</code>的具体实现，首先<code>scala.util.hashing.byteswap32</code>是一个积性Hash函数，积性函数是满足<code>f(ab)=f(a)f(b)</code>的函数。不过我算了一下，也没看出来哪里是积性函数了。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala.util.hashing.byteswap32(<span class="number">100</span>) * scala.util.hashing.byteswap32(<span class="number">2</span>) </span><br><span class="line">scala.util.hashing.byteswap32(<span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<p>后来看了下wikipedia才知道，这里说的应该是乘法哈希，只是一种哈希算法，类似的还有除法哈希（就是mod）和Fibonacci哈希。这个产生[0..M-1]区间内的哈希值公式是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash(key) = floor((k <span class="type">A</span> mod <span class="type">W</span>)/(<span class="type">W</span>/<span class="type">M</span>) )</span><br></pre></td></tr></table></figure>

<p>其中：通常设置<code>M</code>为 2 的幂次方，<code>W</code>为计算机字长大小（也为2的幂次方），<code>a</code>为一个非常接近于<code>W</code>的数。它的关键思想是提取关键字<code>k</code>中间的几位数字。</p>
<p>不过无论如何，这里只是做一个随机数种子，<code>.nextInt(numPartitions)</code>返回一个0到<code>n</code>之间的随机数。<br>下面来看这个<code>CoalescedRDD</code>，它有和<code>ShuffledRDD</code>同样的<code>numPartitions</code>。这个<code>partitionCoalescer</code>实际上是<code>Empty</code>，最后用的是<code>DefaultPartitionCoalescer</code>。主要用在<code>CoalescedRDD.getPartitions</code>里面。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning.</span></span><br><span class="line"><span class="comment"> * It is a performance API to be used carefully only if we are sure that the RDD elements are</span></span><br><span class="line"><span class="comment"> * serializable and don't require closure cleaning.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param preservesPartitioning indicates whether the input function preserves the partitioner,</span></span><br><span class="line"><span class="comment"> *                              which should be `false` unless this is a pair RDD and the input</span></span><br><span class="line"><span class="comment"> *                              function doesn't modify the keys.</span></span><br><span class="line"><span class="comment"> * @param isOrderSensitive whether or not the function is order-sensitive. If it's order</span></span><br><span class="line"><span class="comment"> *                         sensitive, it may return totally different result when the input order</span></span><br><span class="line"><span class="comment"> *                         is changed. Mostly stateful functions are order-sensitive.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">mapPartitionsWithIndexInternal</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    f: (<span class="type">Int</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">    preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">    isOrderSensitive: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">    <span class="keyword">this</span>,</span><br><span class="line">    (_: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; f(index, iter),</span><br><span class="line">    preservesPartitioning = preservesPartitioning,</span><br><span class="line">    isOrderSensitive = isOrderSensitive)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中<code>withScope</code>的实现是</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Execute a block of code in a scope such that all new RDDs created in this body will</span></span><br><span class="line"><span class="comment"> * be part of the same scope. For more detail, see &#123;&#123;org.apache.spark.rdd.RDDOperationScope&#125;&#125;.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Note: Return statements are NOT allowed in the given body.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">withScope</span></span>[<span class="type">U</span>](body: =&gt; <span class="type">U</span>): <span class="type">U</span> = <span class="type">RDDOperationScope</span>.withScope[<span class="type">U</span>](sc)(body)</span><br></pre></td></tr></table></figure>

<p><a href="https://stackoverflow.com/questions/37691391/spark-source-code-how-to-understand-withscope-method" target="_blank" rel="noopener">它的作用是在给定代码块中创建的RDD具有相同的RDDOperationScope</a>。<a href="https://zhuanlan.zhihu.com/p/24616237" target="_blank" rel="noopener">withScope就像是一个 AOP（面向切面编程），嵌入到所有RDD 的转换和操作的函数中，RDDOperationScope会把调用栈记录下来，用于绘制Spark UI的 DAG（有向无环图，可以理解为 Spark 的执行计划）</a>。<br>实际上<code>withScope</code>就类似一个代理，为什么要做代理，是因为<code>RDDOpertionScope</code>需要输出一些调试信息。这有点类似于Haskell的<code>Debug.trace</code>一样。</p>
<h2 id="Persist相关"><a href="#Persist相关" class="headerlink" title="Persist相关"></a>Persist相关</h2><h3 id="unpersist流程"><a href="#unpersist流程" class="headerlink" title="unpersist流程"></a>unpersist流程</h3><p>可以看到，同样是向BlockManagerMaster要求<code>removeRdd</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext.scala</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">unpersistRDD</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  env.blockManager.master.removeRdd(rddId, blocking)</span><br><span class="line">  persistentRdds.remove(rddId) <span class="comment">// 这是一个ConcurrentMap</span></span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerUnpersistRDD</span>(rddId)) <span class="comment">// 前面提到过的，做Event log</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>下面就是从<code>BlockManagerMaster</code>里面去掉所有属于这个RDD的块。可以看到，它往所有的<code>BlockManagerSlave</code>发送<code>RemoveRdd</code>消息。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockManagerMaster.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">removeRdd</span></span>(rddId: <span class="type">Int</span>, blocking: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> future = driverEndpoint.askSync[<span class="type">Future</span>[<span class="type">Seq</span>[<span class="type">Int</span>]]](<span class="type">RemoveRdd</span>(rddId))</span><br><span class="line">  future.failed.foreach(e =&gt;</span><br><span class="line">    logWarning(<span class="string">s"Failed to remove RDD <span class="subst">$rddId</span> - <span class="subst">$&#123;e.getMessage&#125;</span>"</span>, e)</span><br><span class="line">  )(<span class="type">ThreadUtils</span>.sameThread)</span><br><span class="line">  <span class="keyword">if</span> (blocking) &#123;</span><br><span class="line">    timeout.awaitResult(future)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// BlockManagerMessages.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">RemoveRdd</span>(<span class="params">rddId: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">ToBlockManagerSlave</span></span></span><br></pre></td></tr></table></figure>

<p>有一次，我们遇到了这样的错误</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">20</span>/<span class="number">08</span>/<span class="number">01</span> <span class="number">13</span>:<span class="number">00</span>:<span class="number">17</span> <span class="type">ERROR</span> <span class="type">YarnClusterScheduler</span>: <span class="type">Lost</span> executor <span class="number">144</span> on xxx: <span class="type">Executor</span> heartbeat timed out after <span class="number">305856</span> ms</span><br><span class="line"><span class="number">20</span>/<span class="number">08</span>/<span class="number">01</span> <span class="number">13</span>:<span class="number">00</span>:<span class="number">21</span> <span class="type">ERROR</span> <span class="type">YarnClusterScheduler</span>: <span class="type">Lost</span> executor <span class="number">144</span> on xxx: <span class="type">Container</span> container_xxx_01_000145 exited from explicit termination request.</span><br><span class="line"><span class="number">20</span>/<span class="number">08</span>/<span class="number">01</span> <span class="number">14</span>:<span class="number">03</span>:<span class="number">04</span> <span class="type">ERROR</span> <span class="type">ApplicationMaster</span>: <span class="type">User</span> <span class="class"><span class="keyword">class</span> <span class="title">threw</span> <span class="title">exception</span></span>: org.apache.spark.rpc.<span class="type">RpcTimeoutException</span>: <span class="type">Futures</span> timed out after [<span class="number">300</span> seconds]. <span class="type">This</span> timeout is controlled by spark.network.timeout</span><br><span class="line">org.apache.spark.rpc.<span class="type">RpcTimeoutException</span>: <span class="type">Futures</span> timed out after [<span class="number">300</span> seconds]. <span class="type">This</span> timeout is controlled by spark.network.timeout</span><br><span class="line">	at org.apache.spark.rpc.<span class="type">RpcTimeout</span>.org$apache$spark$rpc$<span class="type">RpcTimeout</span>$$createRpcTimeoutException(<span class="type">RpcTimeout</span>.scala:<span class="number">47</span>)</span><br><span class="line">	at org.apache.spark.rpc.<span class="type">RpcTimeout</span>$$anonfun$addMessageIfTimeout$<span class="number">1.</span>applyOrElse(<span class="type">RpcTimeout</span>.scala:<span class="number">62</span>)</span><br><span class="line">	at org.apache.spark.rpc.<span class="type">RpcTimeout</span>$$anonfun$addMessageIfTimeout$<span class="number">1.</span>applyOrElse(<span class="type">RpcTimeout</span>.scala:<span class="number">58</span>)</span><br><span class="line">	at scala.runtime.<span class="type">AbstractPartialFunction</span>.apply(<span class="type">AbstractPartialFunction</span>.scala:<span class="number">36</span>)</span><br><span class="line">	at org.apache.spark.rpc.<span class="type">RpcTimeout</span>.awaitResult(<span class="type">RpcTimeout</span>.scala:<span class="number">76</span>)</span><br><span class="line">	at org.apache.spark.storage.<span class="type">BlockManagerMaster</span>.removeRdd(<span class="type">BlockManagerMaster</span>.scala:<span class="number">131</span>)</span><br><span class="line">	at org.apache.spark.<span class="type">SparkContext</span>.unpersistRDD(<span class="type">SparkContext</span>.scala:<span class="number">1844</span>)</span><br><span class="line">	at org.apache.spark.rdd.<span class="type">RDD</span>.unpersist(<span class="type">RDD</span>.scala:<span class="number">217</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Demo"><a href="#Demo" class="headerlink" title="Demo"></a>Demo</h3><p>可以通过<code>df.explain</code>看物理查询计划<br><img src="/img/sparksql/dfexplainjoin.png"></p>
<h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/67068559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67068559</a></li>
<li><a href="http://www.jasongj.com/spark/rbo/" target="_blank" rel="noopener">http://www.jasongj.com/spark/rbo/</a></li>
<li><a href="https://www.kancloud.cn/kancloud/spark-internals/45243" target="_blank" rel="noopener">https://www.kancloud.cn/kancloud/spark-internals/45243</a></li>
<li><a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">https://www.jianshu.com/p/4c5c2e535da5</a></li>
<li><a href="http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/" target="_blank" rel="noopener">http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/</a></li>
<li><a href="https://github.com/hustnn/TungstenSecret" target="_blank" rel="noopener">https://github.com/hustnn/TungstenSecret</a></li>
<li><a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-shuffle-UnsafeShuffleWriter.html" target="_blank" rel="noopener">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-shuffle-UnsafeShuffleWriter.html</a></li>
<li><a href="https://blog.k2datascience.com/batch-processing-apache-spark-a67016008167" target="_blank" rel="noopener">https://blog.k2datascience.com/batch-processing-apache-spark-a67016008167</a></li>
<li><a href="https://stackoverflow.com/questions/45553492/spark-task-memory-allocation/45570944" target="_blank" rel="noopener">https://stackoverflow.com/questions/45553492/spark-task-memory-allocation/45570944</a></li>
<li><a href="https://0x0fff.com/spark-architecture-shuffle/" target="_blank" rel="noopener">https://0x0fff.com/spark-architecture-shuffle/</a></li>
<li><a href="https://0x0fff.com/spark-memory-management/" target="_blank" rel="noopener">https://0x0fff.com/spark-memory-management/</a></li>
<li><a href="https://www.slideshare.net/databricks/memory-management-in-apache-spark" target="_blank" rel="noopener">https://www.slideshare.net/databricks/memory-management-in-apache-spark</a></li>
<li><a href="https://www.linuxprobe.com/wp-content/uploads/2017/04/unified-memory-management-spark-10000.pdf" target="_blank" rel="noopener">https://www.linuxprobe.com/wp-content/uploads/2017/04/unified-memory-management-spark-10000.pdf</a></li>
<li><a href="https://www.xiaoheidiannao.com/215670.html" target="_blank" rel="noopener">https://www.xiaoheidiannao.com/215670.html</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/101797149" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/101797149</a></li>
<li><a href="https://mp.weixin.qq.com/s/M29AdSNy90ZoWFO6yP967Q?st=4E4EC5032168C875055B8539D8DF21E00C9505E1A359D84092C936E0CCA66518CF3DD79D1F951211AD4E74EE77C357659C0CF2B38EB5D901EEFFBBB7D1D22FF17B8290AF97D9EA29EF49B69C161D5B249ADA7B55585031E1A95FD955BBDF5FD4FFC52F892F43219C7C42DE53661D9EE72F5049491A75C067E71791364C162E767ECF5B3EE162E7D58566458BB0B55F100D4463EFD264C0E118CF40622573B62E87F319989CFEF3656FB8325659A3E1C2&amp;vid=1688850523686960&amp;cst=A4500263DF343A71C6A77F0747F6E08A9AAD04329A7D1B94AC827AA2C31F1654BE4E436938CB546B8839565025E35997&amp;deviceid=24b21599-d5ef-4ce4-9465-be2e176a6aaf&amp;version=3.1.7.3005&amp;platform=win" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/M29AdSNy90ZoWFO6yP967Q?st=4E4EC5032168C875055B8539D8DF21E00C9505E1A359D84092C936E0CCA66518CF3DD79D1F951211AD4E74EE77C357659C0CF2B38EB5D901EEFFBBB7D1D22FF17B8290AF97D9EA29EF49B69C161D5B249ADA7B55585031E1A95FD955BBDF5FD4FFC52F892F43219C7C42DE53661D9EE72F5049491A75C067E71791364C162E767ECF5B3EE162E7D58566458BB0B55F100D4463EFD264C0E118CF40622573B62E87F319989CFEF3656FB8325659A3E1C2&amp;vid=1688850523686960&amp;cst=A4500263DF343A71C6A77F0747F6E08A9AAD04329A7D1B94AC827AA2C31F1654BE4E436938CB546B8839565025E35997&amp;deviceid=24b21599-d5ef-4ce4-9465-be2e176a6aaf&amp;version=3.1.7.3005&amp;platform=win</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div></div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/img/fkm/wxfk.jpg" alt="Calvin Neo WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/img/fkm/zfbfk.jpg" alt="Calvin Neo Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/并行计算/" rel="tag"># 并行计算</a>
          
            <a href="/tags/数据挖掘/" rel="tag"># 数据挖掘</a>
          
            <a href="/tags/数据库/" rel="tag"># 数据库</a>
          
            <a href="/tags/Scala/" rel="tag"># Scala</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/06/scala-lang/" rel="next" title="使用Scala语言进行编程">
                <i class="fa fa-chevron-left"></i> 使用Scala语言进行编程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/10/python-matplotlib-animation/" rel="prev" title="使用matplotlib制作动态图表">
                使用matplotlib制作动态图表 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/favicon.jpg"
               alt="Calvin Neo" />
          <p class="site-author-name" itemprop="name">Calvin Neo</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">239</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">151</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/CalvinNeo" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/CalvinNeo0" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1568200035" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://xqq.im/" title="xqq" target="_blank">xqq</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.lovelywen.com/" title="wenwen" target="_blank">wenwen</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://smlight.github.io/blog/" title="zyyyyy" target="_blank">zyyyyy</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Core"><span class="nav-number">1.</span> <span class="nav-text">Spark Core</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-number">1.1.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD的常见成员"><span class="nav-number">1.1.1.</span> <span class="nav-text">RDD的常见成员</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见RDD"><span class="nav-number">1.1.2.</span> <span class="nav-text">常见RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见RDD外部函数"><span class="nav-number">1.1.3.</span> <span class="nav-text">常见RDD外部函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的架构概览"><span class="nav-number">1.2.</span> <span class="nav-text">Spark的架构概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark上下文"><span class="nav-number">1.3.</span> <span class="nav-text">Spark上下文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkEnv"><span class="nav-number">1.3.1.</span> <span class="nav-text">SparkEnv</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的任务调度"><span class="nav-number">1.4.</span> <span class="nav-text">Spark的任务调度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#启动一个任务"><span class="nav-number">1.4.1.</span> <span class="nav-text">启动一个任务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#失败重试"><span class="nav-number">1.4.2.</span> <span class="nav-text">失败重试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的存储管理"><span class="nav-number">1.5.</span> <span class="nav-text">Spark的存储管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#BlockId和BlockInfo"><span class="nav-number">1.5.1.</span> <span class="nav-text">BlockId和BlockInfo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#持久化"><span class="nav-number">1.5.2.</span> <span class="nav-text">持久化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Checkpoint"><span class="nav-number">1.5.3.</span> <span class="nav-text">Checkpoint</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Save"><span class="nav-number">1.5.4.</span> <span class="nav-text">Save</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD的overwrite问题"><span class="nav-number">1.5.4.1.</span> <span class="nav-text">RDD的overwrite问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD的save问题"><span class="nav-number">1.5.4.2.</span> <span class="nav-text">RDD的save问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BlockInfoManager"><span class="nav-number">1.5.5.</span> <span class="nav-text">BlockInfoManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#本地读Block"><span class="nav-number">1.5.6.</span> <span class="nav-text">本地读Block</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的内存管理"><span class="nav-number">1.6.</span> <span class="nav-text">Spark的内存管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark内存布局"><span class="nav-number">1.6.1.</span> <span class="nav-text">Spark内存布局</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MemoryManager"><span class="nav-number">1.6.2.</span> <span class="nav-text">MemoryManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#请求内存的流程"><span class="nav-number">1.6.3.</span> <span class="nav-text">请求内存的流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tungsten内存管理机制"><span class="nav-number">1.6.4.</span> <span class="nav-text">Tungsten内存管理机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TaskMemoryManager"><span class="nav-number">1.6.5.</span> <span class="nav-text">TaskMemoryManager</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#普通分配acquireExecutionMemory"><span class="nav-number">1.6.5.1.</span> <span class="nav-text">普通分配acquireExecutionMemory</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tungsten分配allocatePage"><span class="nav-number">1.6.5.2.</span> <span class="nav-text">Tungsten分配allocatePage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Job执行流程分析"><span class="nav-number">1.7.</span> <span class="nav-text">Spark Job执行流程分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Job阶段"><span class="nav-number">1.7.1.</span> <span class="nav-text">Job阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stage阶段"><span class="nav-number">1.7.2.</span> <span class="nav-text">Stage阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Task阶段"><span class="nav-number">1.7.3.</span> <span class="nav-text">Task阶段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shuffle"><span class="nav-number">1.8.</span> <span class="nav-text">Shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Map-side-combine"><span class="nav-number">1.8.1.</span> <span class="nav-text">Map side combine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle考古"><span class="nav-number">1.8.2.</span> <span class="nav-text">Shuffle考古</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见对象关系简介"><span class="nav-number">1.8.3.</span> <span class="nav-text">常见对象关系简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ShuffleManager-SortShuffleManager"><span class="nav-number">1.8.4.</span> <span class="nav-text">ShuffleManager/SortShuffleManager</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle-Read端源码分析"><span class="nav-number">1.8.5.</span> <span class="nav-text">Shuffle Read端源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spillable"><span class="nav-number">1.8.6.</span> <span class="nav-text">Spillable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SizeTracker"><span class="nav-number">1.8.7.</span> <span class="nav-text">SizeTracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AppendOnlyMap"><span class="nav-number">1.8.8.</span> <span class="nav-text">AppendOnlyMap</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#赋值"><span class="nav-number">1.8.8.1.</span> <span class="nav-text">赋值</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#迭代器"><span class="nav-number">1.8.8.2.</span> <span class="nav-text">迭代器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExternalAppendOnlyMap"><span class="nav-number">1.8.9.</span> <span class="nav-text">ExternalAppendOnlyMap</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#插入"><span class="nav-number">1.8.9.1.</span> <span class="nav-text">插入</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#读出"><span class="nav-number">1.8.9.2.</span> <span class="nav-text">读出</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spill细节和SpillableIterator的实现"><span class="nav-number">1.8.9.3.</span> <span class="nav-text">Spill细节和SpillableIterator的实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ExternalIterator"><span class="nav-number">1.8.9.4.</span> <span class="nav-text">ExternalIterator</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExternalSorter"><span class="nav-number">1.8.10.</span> <span class="nav-text">ExternalSorter</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#插入-1"><span class="nav-number">1.8.10.1.</span> <span class="nav-text">插入</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle-Write端源码分析"><span class="nav-number">1.8.11.</span> <span class="nav-text">Shuffle Write端源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SortShuffleWriter"><span class="nav-number">1.8.12.</span> <span class="nav-text">SortShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BypassMergeSortShuffleWriter"><span class="nav-number">1.8.13.</span> <span class="nav-text">BypassMergeSortShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UnsafeShuffleWriter"><span class="nav-number">1.8.14.</span> <span class="nav-text">UnsafeShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fetchLocalBlocks和fetchUpToMaxBytes的实现"><span class="nav-number">1.8.15.</span> <span class="nav-text">fetchLocalBlocks和fetchUpToMaxBytes的实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark分布式部署方式"><span class="nav-number">1.9.</span> <span class="nav-text">Spark分布式部署方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark自有部署方式"><span class="nav-number">1.9.1.</span> <span class="nav-text">Spark自有部署方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn"><span class="nav-number">1.9.2.</span> <span class="nav-text">Yarn</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL"><span class="nav-number">2.</span> <span class="nav-text">SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame和Dataset"><span class="nav-number">2.1.</span> <span class="nav-text">DataFrame和Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建DataFrame"><span class="nav-number">2.1.1.</span> <span class="nav-text">创建DataFrame</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从RDD到DF-DS"><span class="nav-number">2.1.2.</span> <span class="nav-text">从RDD到DF/DS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从DF到DS"><span class="nav-number">2.1.3.</span> <span class="nav-text">从DF到DS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从DS到DF"><span class="nav-number">2.1.4.</span> <span class="nav-text">从DS到DF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Row"><span class="nav-number">2.2.</span> <span class="nav-text">Row</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么不能在map函数中返回Row"><span class="nav-number">2.2.1.</span> <span class="nav-text">为什么不能在map函数中返回Row</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Column"><span class="nav-number">2.3.</span> <span class="nav-text">Column</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#借助于withColumn"><span class="nav-number">2.3.1.</span> <span class="nav-text">借助于withColumn</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KeyValueGroupedDataset和RelationalGroupedDataset"><span class="nav-number">2.4.</span> <span class="nav-text">KeyValueGroupedDataset和RelationalGroupedDataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL语法和用法"><span class="nav-number">2.5.</span> <span class="nav-text">SparkSQL语法和用法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkSQL和DataFrame的交互"><span class="nav-number">2.5.1.</span> <span class="nav-text">SparkSQL和DataFrame的交互</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL的上下文"><span class="nav-number">2.6.</span> <span class="nav-text">SparkSQL的上下文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SharedState"><span class="nav-number">2.6.1.</span> <span class="nav-text">SharedState</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SessionState"><span class="nav-number">2.6.2.</span> <span class="nav-text">SessionState</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL的架构总览"><span class="nav-number">2.7.</span> <span class="nav-text">SparkSQL的架构总览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#LogicPlan类"><span class="nav-number">2.7.1.</span> <span class="nav-text">LogicPlan类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkPlan类"><span class="nav-number">2.7.2.</span> <span class="nav-text">SparkPlan类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理流程"><span class="nav-number">2.7.3.</span> <span class="nav-text">处理流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL的解析流程"><span class="nav-number">2.8.</span> <span class="nav-text">SparkSQL的解析流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL-API的执行流程"><span class="nav-number">2.9.</span> <span class="nav-text">SparkSQL API的执行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#map操作分析"><span class="nav-number">2.10.</span> <span class="nav-text">map操作分析</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark性能调优"><span class="nav-number">3.</span> <span class="nav-text">Spark性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark常见性能问题和选项"><span class="nav-number">3.1.</span> <span class="nav-text">Spark常见性能问题和选项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#总体列表"><span class="nav-number">3.1.1.</span> <span class="nav-text">总体列表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#替代性算子"><span class="nav-number">3.1.2.</span> <span class="nav-text">替代性算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用DataFrame-API"><span class="nav-number">3.1.3.</span> <span class="nav-text">使用DataFrame API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关Persist的优化方案"><span class="nav-number">3.1.4.</span> <span class="nav-text">有关Persist的优化方案</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#一些Case"><span class="nav-number">3.1.5.</span> <span class="nav-text">一些Case</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Task-Retry"><span class="nav-number">3.1.5.1.</span> <span class="nav-text">Task Retry</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark日志"><span class="nav-number">3.2.</span> <span class="nav-text">Spark日志</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HistoryServer"><span class="nav-number">3.2.1.</span> <span class="nav-text">HistoryServer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#spark-log"><span class="nav-number">3.2.2.</span> <span class="nav-text">spark log</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#gc-log"><span class="nav-number">3.2.3.</span> <span class="nav-text">gc log</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用调试方法"><span class="nav-number">3.3.</span> <span class="nav-text">常用调试方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-on-YARN的配置参数"><span class="nav-number">3.4.</span> <span class="nav-text">Spark on YARN的配置参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Executor数量"><span class="nav-number">3.4.1.</span> <span class="nav-text">Executor数量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Executor内存和CPU"><span class="nav-number">3.4.2.</span> <span class="nav-text">Executor内存和CPU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Driver内存"><span class="nav-number">3.4.3.</span> <span class="nav-text">Driver内存</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark常见问题的解决方案"><span class="nav-number">4.</span> <span class="nav-text">Spark常见问题的解决方案</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#变量在节点之间共享"><span class="nav-number">4.1.</span> <span class="nav-text">变量在节点之间共享</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer"><span class="nav-number">4.2.</span> <span class="nav-text">scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Extracting-Seq-String-String-String-from-spark-DataFrame"><span class="nav-number">4.3.</span> <span class="nav-text">Extracting Seq[(String,String,String)] from spark DataFrame</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#考虑集群机器的问题"><span class="nav-number">4.4.</span> <span class="nav-text">考虑集群机器的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL编写技巧"><span class="nav-number">4.5.</span> <span class="nav-text">Spark SQL编写技巧</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark的其他组件的简介"><span class="nav-number">5.</span> <span class="nav-text">Spark的其他组件的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GraphX"><span class="nav-number">5.1.</span> <span class="nav-text">GraphX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML和MLLib"><span class="nav-number">5.2.</span> <span class="nav-text">ML和MLLib</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Streaming"><span class="nav-number">5.3.</span> <span class="nav-text">Streaming</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark相关机制详解"><span class="nav-number">6.</span> <span class="nav-text">Spark相关机制详解</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#JOIN相关"><span class="nav-number">6.1.</span> <span class="nav-text">JOIN相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Repartition相关"><span class="nav-number">6.2.</span> <span class="nav-text">Repartition相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Persist相关"><span class="nav-number">6.3.</span> <span class="nav-text">Persist相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#unpersist流程"><span class="nav-number">6.3.1.</span> <span class="nav-text">unpersist流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Demo"><span class="nav-number">6.3.2.</span> <span class="nav-text">Demo</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">7.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Calvin Neo</span>
  <span> &nbsp; Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
</div>
<div>
  <span><a href="/about/yytl/">版权声明</a></span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse 
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://www.calvinneo.com/2019/08/06/spark-sql/';
          this.page.identifier = '2019/08/06/spark-sql/';
          this.page.title = 'Spark和SparkSQL';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://calvinneo.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      $('#local-search-input').focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

</body>
</html>
