<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Scala,Spark,SparkSQL," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Spark是Mapreduce的下一代的分布式计算框架。相比更早期的Mapreduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。 【未完待续】">
<meta name="keywords" content="Scala,Spark,SparkSQL">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark和SparkSQL">
<meta property="og:url" content="http://www.calvinneo.com/2019/08/06/spark-sql/index.html">
<meta property="og:site_name" content="Calvin&#39;s Marbles">
<meta property="og:description" content="Spark是Mapreduce的下一代的分布式计算框架。相比更早期的Mapreduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。 【未完待续】">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/sparkexe.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/rela.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/shuffle.png">
<meta property="og:image" content="http://www.calvinneo.com/img/sparksql/yarn.png">
<meta property="og:updated_time" content="2019-12-10T16:00:02.029Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark和SparkSQL">
<meta name="twitter:description" content="Spark是Mapreduce的下一代的分布式计算框架。相比更早期的Mapreduce的Job和Task的两层，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。 【未完待续】">
<meta name="twitter:image" content="http://www.calvinneo.com/img/sparksql/sparkexe.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.calvinneo.com/2019/08/06/spark-sql/"/>





  <title>Spark和SparkSQL | Calvin's Marbles</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Calvin's Marbles</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.calvinneo.com/2019/08/06/spark-sql/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Calvin Neo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Calvin's Marbles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark和SparkSQL
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-06T16:42:32+08:00">
                2019-08-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Spark是Mapreduce的下一代的分布式计算框架。相比更早期的Mapreduce的<a href="http://hadoop.apache.org/docs/r1.0.4/cn/mapred_tutorial.html" target="_blank" rel="noopener">Job和Task的两层</a>，Spark更为灵活，其执行粒度分为Application、Job、Stage和Task四个层次。</p>
<p>【未完待续】</p>
<a id="more"></a>
<h1 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a>Spark Core</h1><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD(Resilient Distributed Dataset)，即弹性数据集是Spark中的基础结构。RDD是distributive的、immutable的，可以存在在内存中，也可以被缓存。<br>对RDD具有转换操作和行动操作两种截然不同的操作。转换(Transform)操作始终在RDD的Context里面，但行动(Action)操作会去掉RDD的壳。例如<code>take</code>是行动操作，返回的是一个数组而不是RDD了，在Scala中可以看到。</p>
<figure class="highlight zephir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> rdd1 = sc.makeRDD(Seq(<span class="number">10</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">12</span>, <span class="number">3</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[<span class="keyword">Int</span>] = ParallelCollectionRDD[<span class="number">40</span>] at makeRDD at :<span class="number">21</span></span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(<span class="number">1</span>)</span><br><span class="line">res0: <span class="keyword">Array</span>[<span class="keyword">Int</span>] = <span class="keyword">Array</span>(<span class="number">10</span>)                                                    </span><br><span class="line"> </span><br><span class="line">scala&gt; rdd1.take(<span class="number">2</span>)</span><br><span class="line">res1: <span class="keyword">Array</span>[<span class="keyword">Int</span>] = <span class="keyword">Array</span>(<span class="number">10</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>转换操作是Lazy的，直到遇到一个Action操作，Spark才会生成关于整条链的执行计划并执行。这些Action操作将一个Spark Application分为了多个Job。</p>
<h3 id="常见RDD"><a href="#常见RDD" class="headerlink" title="常见RDD"></a>常见RDD</h3><p>RDD是一个抽象类<code>abstract class
RDD[T] extends Serializable with Logging</code>，在Spark中有诸如<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.package" target="_blank" rel="noopener">ShuffledRDD、HadoopRDD</a>等实现。每个RDD都有对应的<code>compute</code>方法，用来描述这个RDD的计算方法。需要注意的是，这些RDD可能被作为某些RDD计算的中间结果，例如<code>CoGroupedRDD</code>，对应的，例如<code>MapPartitionsRDD</code>也可能是经过多个RDD变换得到的，其决定权在于所使用的算子。<br>我们来具体查看一些RDD。</p>
<ol>
<li><p><code>ParallelCollectionRDD</code><br> 这个RDD由<code>parallelize</code>得到</p>
 <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val arr = <span class="keyword">sc.parallelize(0 </span>to <span class="number">1000</span>)</span><br><span class="line"><span class="symbol">arr:</span> <span class="keyword">org.apache.spark.rdd.RDD[Int] </span>= ParallelCollectionRDD[<span class="number">0</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>HadoopRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HadoopRDD</span>[<span class="type">K</span>, <span class="type">V</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] <span class="keyword">with</span> <span class="title">Logging</span></span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>FileScanRDD</code><br> 这个RDD一般从<code>spark.read.text(...)</code>语句中产生，所以实现在<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/FileScanRDD.scala" target="_blank" rel="noopener">sql模块中</a>。</p>
 <figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">class</span> <span class="selector-tag">FileScanRDD</span>(</span><br><span class="line">    <span class="variable">@transient</span> private val <span class="attribute">sparkSession</span>: SparkSession,</span><br><span class="line">    <span class="attribute">readFunction</span>: (PartitionedFile) =&gt; Iterator[InternalRow],</span><br><span class="line">    <span class="variable">@transient</span> val <span class="attribute">filePartitions</span>: Seq[FilePartition])</span><br><span class="line">  <span class="selector-tag">extends</span> <span class="selector-tag">RDD</span><span class="selector-attr">[InternalRow]</span>(sparkSession.sparkContext, Nil) &#123;</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>MapPartitionsRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">U</span>]</span></span><br></pre></td></tr></table></figure>
<p> 这个RDD是<code>map</code>、<code>mapPartitions</code>、<code>mapPartitionsWithIndex</code>操作的结果。<br> 注意，在较早期的版本中，<code>map</code>会得到一个<code>MappedRDD</code>，<code>filter</code>会得到一个<code>FilteredRDD</code>、<code>flatMap</code>会得到一个<code>FlatMappedRDD</code>，不过目前已经找不到了，统一变成<code>MapPartitionsRDD</code></p>
 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a3 = arr.map(<span class="selector-tag">i</span> =&gt; (i+<span class="number">1</span>, i))</span><br><span class="line">a3: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[(Int, Int)] = MapPartitionsRDD[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; val a3 = arr.<span class="attribute">filter</span>(i =&gt; i &gt; <span class="number">3</span>)</span><br><span class="line">a3: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[Int] = MapPartitionsRDD[<span class="number">4</span>] at <span class="attribute">filter</span> at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">scala&gt; val a3 = arr.flatMap(<span class="selector-tag">i</span> =&gt; Array(i))</span><br><span class="line">a3: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[Int] = MapPartitionsRDD[<span class="number">5</span>] at flatMap at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>
<p> <code>join</code>操作的结果也是<code>MapPartitionsRDD</code>，这是因为其执行过程的最后一步<code>flatMapValues</code>会创建一个<code>MapPartitionsRDD</code></p>
 <figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">scala&gt; </span>val rdd1 = <span class="keyword">sc.parallelize(Array((1,1),(1,2),(1,3),(2,1),(2,2),(2,3)))</span></span><br><span class="line"><span class="keyword">rdd1: </span><span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Int)] = ParallelCollectionRDD[<span class="number">8</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rdd2 = <span class="keyword">sc.parallelize(Array((1,1),(1,2),(1,3),(2,1),(2,2),(2,3)))</span></span><br><span class="line"><span class="keyword">rdd2: </span><span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>Int)] = ParallelCollectionRDD[<span class="number">9</span>] <span class="built_in">at</span> parallelize <span class="built_in">at</span> &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">scala&gt; </span>val rddj = rdd1.<span class="keyword">join(rdd2)</span></span><br><span class="line"><span class="keyword">rddj: </span><span class="keyword">org.apache.spark.rdd.RDD[(Int, </span>(Int, Int))] = MapPartitionsRDD[<span class="number">12</span>] <span class="built_in">at</span> <span class="keyword">join </span><span class="built_in">at</span> &lt;console&gt;:<span class="number">27</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>ShuffledRDD</code><br> <code>ShuffledRDD</code>用来存储所有Shuffle操作的结果，其中<code>K</code>、<code>V</code>很好理解，<code>C</code>是Combiner Class。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span></span><br></pre></td></tr></table></figure>
<p> 以<code>groupByKey</code>为例</p>
 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a2 = arr.map(<span class="selector-tag">i</span> =&gt; (i+<span class="number">1</span>, i))</span><br><span class="line">a2: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[(Int, Int)] = MapPartitionsRDD[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; a2.groupByKey</span><br><span class="line">res1: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[(Int, Iterable[Int])] = ShuffledRDD[<span class="number">3</span>] at groupByKey at &lt;console&gt;:<span class="number">26</span></span><br></pre></td></tr></table></figure>
<p> 注意，<code>groupByKey</code>需要K是Hashable的，否则会报错。</p>
 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a2 = arr.map(<span class="selector-tag">i</span> =&gt; (Array.fill(<span class="number">10</span>)(i), i))</span><br><span class="line">a2: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.RDD</span>[(Array[Int], Int)] = MapPartitionsRDD[<span class="number">2</span>] at map at &lt;console&gt;:<span class="number">25</span></span><br><span class="line"></span><br><span class="line">scala&gt; a2.groupByKey</span><br><span class="line">org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.SparkException</span>: HashPartitioner cannot partition array keys.</span><br><span class="line">  at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.PairRDDFunctions</span>$<span class="variable">$anonfun</span><span class="variable">$combineByKeyWithClassTag</span>$<span class="number">1</span>.apply(PairRDDFunctions<span class="selector-class">.scala</span>:<span class="number">84</span>)</span><br><span class="line">  at org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.rdd</span><span class="selector-class">.PairRDDFunctions</span>$<span class="variable">$anonfun</span><span class="variable">$combineByKeyWithClassTag</span>$<span class="number">1</span>.apply(PairRDDFunctions<span class="selector-class">.scala</span>:<span class="number">77</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>CoGroupedRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CoGroupedRDD</span>[<span class="type">K</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[(<span class="type">K</span>, <span class="type">Array</span>[<span class="type">Iterable</span>[_]])]</span></span><br></pre></td></tr></table></figure>
<p> 首先，我们需要了解一下什么是<code>cogroup</code>操作，这个方法有多个重载版本。如下所示的版本，对<code>this</code>或<code>other1</code>或<code>other2</code>的所有的key，生成一个<code>RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))</code>，表示对于这个key，这三个RDD中所有值的集合。容易看到，这个算子能够被用来实现Join和Union（不过后者有点大材小用了）</p>
 <figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def cogroup[<span class="string">W1, W2</span>](<span class="link">other1: RDD[(K, W1</span>)], other2: RDD[(K, W2)], partitioner: Partitioner)</span><br><span class="line">  : RDD[(K, (Iterable[V], Iterable[W1], Iterable[W2]))]</span><br></pre></td></tr></table></figure>
<p> 这里的<code>Partitioner</code>是一个<code>abstract class</code>，具有<code>numPartitions: Int</code>和<code>getPartition(key: Any): Int</code>两个方法。通过继承<code>Partitioner</code>可以自定义分区的实现方式，目前官方提供的有<code>RangePartitioner</code>和<code>HashPartitioner</code>等。</p>
</li>
<li><p><code>UnionRDD</code></p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UnionRDD</span>[<span class="type">T</span>] <span class="keyword">extends</span> <span class="title">RDD</span>[<span class="type">T</span>]</span></span><br></pre></td></tr></table></figure>
<p> <code>UnionRDD</code>一般通过<code>union</code>算子得到</p>
 <figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val a5 = arr.<span class="keyword">union</span>(arr2)</span><br><span class="line"><span class="symbol">a5:</span> org.apache.spark.rdd.RDD[Int] = UnionRDD[<span class="number">7</span>] at <span class="class"><span class="keyword">union</span> <span class="title">at</span> &lt;<span class="title">console</span>&gt;:27</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>CoalescedRDD</code></p>
</li>
</ol>
<h3 id="常见RDD-Function"><a href="#常见RDD-Function" class="headerlink" title="常见RDD Function"></a>常见RDD Function</h3><ol>
<li><code>PairRDDFunctions</code><br> 这个RDD被用来处理KV对，相比<code>RDD</code>，它提供了<code>groupByKey</code>、<code>join</code>等方法。以<code>combineByKey</code>为例，他有三个模板参数，从RDD过来的<code>K</code>和<code>V</code>以及自己的<code>C</code>。相比reduce和fold系列的<code>(V, V) =&gt; V</code>，这多出来的<code>C</code>使<code>combineByKey</code>更灵活，通过<code>combineByKey</code>能够将<code>V</code>变换为<code>C</code>。 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="comment">//实现略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Spark的架构概览"><a href="#Spark的架构概览" class="headerlink" title="Spark的架构概览"></a>Spark的架构概览</h2><p>Spark在设计上的一个特点是它和下层的集群管理是分开的，一个Spark Application可以看做是由集群上的若干进程组成的。因此，我们需要区分Spark中的概念和下层集群中的概念，例如我们常见的Master和Worker是集群中的概念，表示节点；而Driver和Executor是Spark中的概念，表示进程。根据<a href="https://stackoverflow.com/questions/34722415/understand-spark-cluster-manager-master-and-driver-nodes" target="_blank" rel="noopener">爆栈网</a>，Driver可能位于某个Worker节点中，或者位于Master节点上，这取决于部署的方式</p>
<p>在<a href="http://spark.apache.org/docs/latest/cluster-overview.html" target="_blank" rel="noopener">官网</a>上给了这样一幅图，详细阐明了Spark集群下的基础架构。<code>SparkContext</code>是整个Application的管理核心，由Driver来负责管理。<code>SparkContext</code>负责管理所有的Executor，并且和下层的集群管理进行交互，以请求资源。</p>
<p><img src="/img/sparksql/sparkexe.png" alt=""></p>
<p>在Stage层次及以上接受<code>DAGScheduler</code>的调度，而<code>TaskScheduler</code>在Task层面进行调度。</p>
<p>在Spark on Yarn模式下，<a href="https://blog.csdn.net/chic_data/article/details/77317730" target="_blank" rel="noopener">CoarseGrainedExecutorBackend和Executor一一对应</a>，它是一个独立于Worker主进程之外的一个进程，我们可以jps查看到。而Task是作为一个Executor启动的一个线程来跑的，一个Executor中可以跑多个Task。在实现上，<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-CoarseGrainedExecutorBackend.html" target="_blank" rel="noopener"><code>CoarseGrainedExecutorBackend</code></a>继承了<code>ExecutorBackend</code>这个trait，作为一个<code>IsolatedRpcEndpoint</code>，维护<code>Executor</code>对象实例，并通过创建的<code>DriverEndpoint</code>实例的与Driver进行交互。在进程启动时，<code>CoarseGrainedExecutorBackend</code>调用<code>onStart()</code>方法向Driver注册自己，并产生一条<code>&quot;Connecting to driver</code>的INFO。<code>CoarseGrainedExecutorBackend</code>通过<code>DriverEndpoint.receive</code>方法来处理来自Driver的命令，包括<code>LaunchTask</code>、<code>KillTask</code>等。这里注意一下，在scheduler中有一个<code>CoarseGrainedSchedulerBackend</code>，里面实现相似，在看代码时要注意区分开。</p>
<p>有关Executor和Driver的关系，下面这张图更加直观，需要说明的是，<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">一个Worker上面也可能跑有多个Executor</a>，<a href="https://blog.51cto.com/10120275/2364992" target="_blank" rel="noopener">每个Task也可以在多个CPU核心上面运行</a></p>
<p><img src="/img/sparksql/rela.png" alt=""></p>
<h2 id="Spark上下文"><a href="#Spark上下文" class="headerlink" title="Spark上下文"></a>Spark上下文</h2><p>在代码里我们操作一个Spark任务有两种方式，通过SparkContext，或者通过SparkSession</p>
<ol>
<li><p>SparkContext方式<br> SparkContext是Spark自创建来一直存在的类。我们通过SparkConf直接创建SparkContext</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"AppName"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf).set(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>SparkSession方式<br> SparkSession是在Spark2.0之后提供的API，相比SparkContext，他提供了对SparkSQL的支持（持有<code>SQLContext</code>），例如<code>createDataFrame</code>等方法就可以通过SparkSession来访问。<br> 在<code>builder.getOrCreate()</code>的过程中，虽然最终得到的是一个SparkSession，但实际上内部已经创建了一个SparkContext，并由这个SparkSession持有。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder() <span class="comment">// 得到一个Builder</span></span><br><span class="line">.master(<span class="string">"local"</span>).appName(<span class="string">"AppName"</span>).config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">.getOrCreate() <span class="comment">// 得到一个SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SparkSession.scala</span></span><br><span class="line"><span class="keyword">val</span> sparkContext = userSuppliedContext.getOrElse &#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  options.foreach &#123; <span class="keyword">case</span> (k, v) =&gt; sparkConf.set(k, v) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// set a random app name if not given.</span></span><br><span class="line">  <span class="keyword">if</span> (!sparkConf.contains(<span class="string">"spark.app.name"</span>)) &#123;</span><br><span class="line">    sparkConf.setAppName(java.util.<span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="type">SparkContext</span>.getOrCreate(sparkConf)</span><br><span class="line">  <span class="comment">// Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">applyExtensions(</span><br><span class="line">  sparkContext.getConf.get(<span class="type">StaticSQLConf</span>.<span class="type">SPARK_SESSION_EXTENSIONS</span>).getOrElse(<span class="type">Seq</span>.empty),</span><br><span class="line">  extensions)</span><br><span class="line"></span><br><span class="line">session = <span class="keyword">new</span> <span class="type">SparkSession</span>(sparkContext, <span class="type">None</span>, <span class="type">None</span>, extensions)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h3><p><a href="https://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/SparkEnv.html" target="_blank" rel="noopener"><code>SparkEnv</code>持有</a>一个Spark实例在运行时所需要的所有对象，包括Serializer、RpcEndpoint（在早期用的是Akka actor）、BlockManager、MemoryManager、BroadcastManager、SecurityManager、MapOutputTrackerMaster/Worker等等。SparkEnv由SparkContext创建，并在之后通过伴生对象<code>SparkEnv</code>的<code>get</code>方法来访问。在创建时，Driver端的SparkEnv是SparkContext创建的时候调用<code>SparkEnv.createDriverEnv</code>创建的。Executor端的是其守护进程CoarseGrainedExecutorBackend创建的时候调用<code>SparkEnv.createExecutorEnv</code>方法创建的。这两个方法最后都会调用<code>create</code>方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Driver端</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">createSparkEnv</span></span>(</span><br><span class="line">    conf: <span class="type">SparkConf</span>,</span><br><span class="line">    isLocal: <span class="type">Boolean</span>,</span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line">  <span class="type">SparkEnv</span>.createDriverEnv(conf, isLocal, listenerBus, <span class="type">SparkContext</span>.numDriverCores(master, conf))</span><br><span class="line">&#125;</span><br><span class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)</span><br><span class="line"><span class="type">SparkEnv</span>.set(_env)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Executor端</span></span><br><span class="line"><span class="comment">// CoarseGrainedExecutorBackend.scala</span></span><br><span class="line"><span class="keyword">val</span> env = <span class="type">SparkEnv</span>.createExecutorEnv(driverConf, arguments.executorId, arguments.bindAddress,</span><br><span class="line">  arguments.hostname, arguments.cores, cfg.ioEncryptionKey, isLocal = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">env.rpcEnv.setupEndpoint(<span class="string">"Executor"</span>, backendCreateFn(env.rpcEnv, arguments, env))</span><br><span class="line">arguments.workerUrl.foreach &#123; url =&gt;</span><br><span class="line">  env.rpcEnv.setupEndpoint(<span class="string">"WorkerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(env.rpcEnv, url))</span><br><span class="line">&#125;</span><br><span class="line">env.rpcEnv.awaitTermination()</span><br><span class="line"></span><br><span class="line"><span class="comment">// SparkEnv.scala</span></span><br><span class="line"><span class="comment">// create函数</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  blockManagerMaster,</span><br><span class="line">  serializerManager,</span><br><span class="line">  conf, <span class="comment">// Spark Conf对象</span></span><br><span class="line">  memoryManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  securityManager,</span><br><span class="line">  externalShuffleClient)</span><br></pre></td></tr></table></figure>
<h2 id="Spark的任务调度"><a href="#Spark的任务调度" class="headerlink" title="Spark的任务调度"></a>Spark的任务调度</h2><p>Spark的操作可以分为两种，Transform操作是lazy的，而Action操作是Eager的。每一个Action会产生一个Job。<br>Spark的Transform操作可以分为宽依赖(<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/Dependency.scala" target="_blank" rel="noopener"><code>ShuffleDependency</code></a>)和窄依赖(<code>NarrowDependency</code>)操作两种，其中窄依赖还有两个子类<code>OneToOneDependency</code>和<code>RangeDependency</code>。窄依赖操作表示父RDD的每个分区只被子RDD的一个分区所使用，例如<code>union</code>、<code>map</code>、<code>filter</code>等的操作；而宽依赖恰恰相反。宽依赖需要shuffle操作，因为需要将父RDD的结果需要复制给不同节点用来生成子RDD，有关<code>ShuffleDependency</code>将在下面的Shuffle源码分析中详细说明。当DAG的执行中出现宽依赖操作时，Spark会将其前后划分为不同的Stage，在下一章节中将具体分析相关代码。</p>
<p>在Stage之下，就是若干个Task了。这些Task也就是Spark的并行单元，通常来说，按照当前Stage的最后一个RDD的分区数来计算，每一个分区都会启动一个Task来进行计算。我们可以通过<code>rdd.partitions.size</code>来获取一个RDD有多少个分区。</p>
<p>Task具有两种类型，<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala" target="_blank" rel="noopener"><code>ShuffleMapTask</code></a>和<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ResultTask.scala" target="_blank" rel="noopener"><code>ResultTask</code></a>。其中<code>ResultTask</code>是<a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-scheduler-ResultTask.html" target="_blank" rel="noopener"><code>ResultStage</code></a>的Task，也就是最后一个Stage的Task。</p>
<h2 id="Spark的存储管理"><a href="#Spark的存储管理" class="headerlink" title="Spark的存储管理"></a>Spark的存储管理</h2><p>为了实现与底层细节的解耦，Spark的存储基于BlockManager给计算部分提供服务。类似于Driver和Executor，BlockManager机制也分为BlockManagerMaster和BlockManager。Driver上的BlockManagerMaster对于存在与Executor上的BlockManager统一管理。BlockManager只是负责管理所在Executor上的Block。<br>BlockManagerMaster和BlockManager都是在SparkEnv中创建的，</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Mapping from block manager id to the block manager's information.</span></span><br><span class="line"><span class="keyword">val</span> blockManagerInfo = <span class="keyword">new</span> concurrent.<span class="type">TrieMap</span>[<span class="type">BlockManagerId</span>, <span class="type">BlockManagerInfo</span>]()</span><br><span class="line"><span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(</span><br><span class="line">      rpcEnv,</span><br><span class="line">      isLocal,</span><br><span class="line">      conf,</span><br><span class="line">      listenerBus,</span><br><span class="line">      <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_SERVICE_FETCH_RDD_ENABLED</span>)) &#123;</span><br><span class="line">        externalShuffleClient</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;, blockManagerInfo)),</span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_HEARTBEAT_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterHeartbeatEndpoint</span>(rpcEnv, isLocal, blockManagerInfo)),</span><br><span class="line">  conf,</span><br><span class="line">  isDriver)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> blockTransferService =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">NettyBlockTransferService</span>(conf, securityManager, bindAddress, advertiseAddress,</span><br><span class="line">    blockManagerPort, numUsableCores, blockManagerMaster.driverEndpoint)</span><br><span class="line"></span><br><span class="line"><span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(</span><br><span class="line">  executorId,</span><br><span class="line">  rpcEnv,</span><br><span class="line">  blockManagerMaster,</span><br><span class="line">  serializerManager,</span><br><span class="line">  conf,</span><br><span class="line">  memoryManager,</span><br><span class="line">  mapOutputTracker,</span><br><span class="line">  shuffleManager,</span><br><span class="line">  blockTransferService,</span><br><span class="line">  securityManager,</span><br><span class="line">  externalShuffleClient)</span><br></pre></td></tr></table></figure>
<p><code>BlockInfoManager</code>用来管理Block的元信息，主要维护一个<code>infos</code>。其中<code>level</code>项表示这个block的存储级别。<br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockInfoManager.scala</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> infos = new mutable.HashMap[BlockId, BlockInfo]</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[storage] <span class="class"><span class="keyword">class</span> <span class="title">BlockInfo</span></span>(</span><br><span class="line">    <span class="keyword">val</span> level: StorageLevel,</span><br><span class="line">    <span class="keyword">val</span> classTag: ClassTag[_],</span><br><span class="line">    <span class="keyword">val</span> tellMaster: <span class="built_in">Boolean</span>) &#123;</span><br></pre></td></tr></table></figure></p>
<p>Spark提供了如下的持久化级别，其中选项为<code>useDisk</code>、<code>useMemory</code>、<code>useOffHeap</code>、<code>deserialized</code>、<code>replication</code>，分别表示是否采用磁盘、内存、堆外内存、反序列化以及持久化维护的副本数。其中反序列化为false时（好绕啊），会对对象进行序列化存储，能够节省一定空间，但同时会消耗计算资源。需要注意的是，<code>cache</code>操作是<code>persist</code>的一个特例。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> <span class="keyword">extends</span> <span class="title">scala</span>.<span class="title">AnyRef</span> <span class="keyword">with</span> <span class="title">scala</span>.<span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>) <span class="comment">// 默认存储类别</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Spark的内存管理"><a href="#Spark的内存管理" class="headerlink" title="Spark的内存管理"></a>Spark的内存管理</h2><h2 id="Spark-Job执行流程分析"><a href="#Spark-Job执行流程分析" class="headerlink" title="Spark Job执行流程分析"></a>Spark Job执行流程分析</h2><h3 id="Job阶段"><a href="#Job阶段" class="headerlink" title="Job阶段"></a>Job阶段</h3><p>下面我们通过一个RDD上的Action操作count，查看Spark的Job是如何运行和调度的。特别注意的是，在SparkSQL中，Action操作有不同的执行流程，所以宜对比着看。<code>count</code>通过全局的<code>SparkContext.runJob</code>启动一个Job，这个函数转而调用<code>DAGScheduler.runJob</code>。<code>Utils.getIteratorSize</code>实际上就是遍历一遍迭代器，以便统计count。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// RDD.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span> = sc.runJob(<span class="keyword">this</span>, <span class="type">Utils</span>.getIteratorSize _).sum</span><br><span class="line"><span class="comment">// Utils.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getIteratorSize</span></span>(iterator: <span class="type">Iterator</span>[_]): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> count = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">while</span> (iterator.hasNext) &#123;</span><br><span class="line">    count += <span class="number">1</span>L</span><br><span class="line">    iterator.next()</span><br><span class="line">  &#125;</span><br><span class="line">  count</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在参数列表里面的下划线<code>_</code>的作用是将方法转为函数，而方法和函数的定义和区别可参考<a href="http://www.calvinneo.com/2019/08/06/scala-lang/">我的另一篇文章</a>。<br>下面查看<code>runJob</code>函数。比较有趣的是<code>clean</code>函数，它调用<code>ClosureCleaner.clean</code>方法，这个方法用来清理<code>$outer</code>域中未被引用的变量。因为我们要将闭包<code>func</code>序列化，并从Driver发送到Executor上面。序列化闭包的过程就是为每一个闭包生成一个可序列化类，在生成时，会将这个闭包所引用的外部对象也序列化。容易发现，如果我们为了使用外部对象的某些字段，而序列化整个对象，那么开销是很大的，因此通过<code>clean</code>来清除不需要的部分以减少序列化开销。此外，<code>getCallSite</code>用来生成诸如<code>s&quot;$lastSparkMethod at $firstUserFile:$firstUserLine&quot;</code>这样的字符串，它实际上会回溯调用栈，找到第一个不是在Spark包中的函数，即<code>$lastSparkMethod</code>，它是导致一个RDD创建的函数，比如各种Transform操作、<code>sc.parallelize</code>等。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"SparkContext has been shutdown"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)</span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">    logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)</span><br><span class="line">  &#125;</span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br><span class="line">  <span class="comment">// CheckPoint机制</span></span><br><span class="line">  rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">clean</span></span>[<span class="type">F</span> &lt;: <span class="type">AnyRef</span>](f: <span class="type">F</span>, checkSerializable: <span class="type">Boolean</span> = <span class="literal">true</span>): <span class="type">F</span> = &#123;</span><br><span class="line">  <span class="type">ClosureCleaner</span>.clean(f, checkSerializable)</span><br><span class="line">  f</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们发现，传入的func只接受一个<code>Iterator[_]</code>参数，但是其形参声明却是接受<code>TaskContext</code>和<code>Iterator[T]</code>两个参数。这是为什么呢？这是因为<code>runJob</code>有不少重载函数，例如下面的这个<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面我们查看<code>DAGScheduler.runJob</code>函数，它实际上就是调用<code>submitJob</code>，然后等待Job执行的结果。由于Spark的<code>DAGScheduler</code>是基于事件循环的，它拥有一个<code>DAGSchedulerEventProcessLoop</code>类型的变量<code>eventProcessLoop</code>，不同的对象向它<code>post</code>事件，然后在它的<code>onReceive</code>循环中会依次对这些事件调用处理函数。<br>我们需要注意的是<code>partitions</code>不同于我们传入的<code>rdd.partitions</code>，前者是一个<code>Array[Int]</code>，后者是一个<code>Array[Partition]</code>。并且在逻辑意义上，前者表示需要计算的partition，对于如first之类的Action操作来说，它只是rdd的所有partition的一个子集，我们将在稍后的<code>submitMissingTasks</code>函数中继续看到这一点。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](...): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 下面就是在等了</span></span><br><span class="line">  <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">      <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">      exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">      <span class="keyword">throw</span> exception</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>], <span class="comment">// target RDD to run tasks on，就是被执行count的RDD</span></span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>, <span class="comment">// 在RDD每一个partition上需要跑的函数</span></span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>, <span class="comment">// 被调用的位置</span></span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="comment">// 检查是否在一个不存在的分区上创建一个Task</span></span><br><span class="line">  <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">  partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>( <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> + <span class="string">"Total number of partitions: "</span> + maxPartitions)&#125;</span><br><span class="line">    </span><br><span class="line">  <span class="comment">// jobId是从后往前递增的</span></span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">  <span class="keyword">if</span> (partitions.isEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> time = clock.getTimeMillis()</span><br><span class="line">    <span class="comment">// listenerBus是一个LiveListenerBus对象，从DAGScheduler构造时得到，用来做event log</span></span><br><span class="line">    <span class="comment">// SparkListenerJobStart定义在SparkListener.scala文件中</span></span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerJobStart</span>(jobId, time, <span class="type">Seq</span>[<span class="type">StageInfo</span>](), <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">    listenerBus.post(<span class="type">SparkListenerJobEnd</span>(jobId, time, <span class="type">JobSucceeded</span>))</span><br><span class="line">    <span class="comment">// 如果partitions是空的，那么就直接返回</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  assert(partitions.nonEmpty)</span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">  <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">  <span class="comment">// 我们向eventProcessLoop提交一个JobSubmitted事件</span></span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">  waiter</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// DAGSchedulerEvent.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">JobSubmitted</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    jobId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    finalRDD: <span class="type">RDD</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]</span>) <span class="title">=&gt;</span> <span class="title">_</span>,</span></span><br><span class="line"><span class="class">    <span class="title">partitions</span></span>: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span> = <span class="literal">null</span>)</span><br><span class="line">  <span class="keyword">extends</span> <span class="type">DAGSchedulerEvent</span></span><br></pre></td></tr></table></figure></p>
<p>下面我们具体看看对<code>JobSubmitted</code>的响应<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(...) &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="comment">// 首先我们尝试创建一个`finalStage: ResultStage`，这是整个Job的最后一个Stage。</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// func: (TaskContext, Iterator[_]) =&gt; _</span></span><br><span class="line">    <span class="comment">// 下面的语句是可能抛BarrierJobSlotsNumberCheckFailed或者其他异常的，</span></span><br><span class="line">    <span class="comment">// 例如一个HadoopRDD所依赖的HDFS文件被删除了</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(...): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(rdd, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId, callSite)</span><br><span class="line">  stageIdToStage(id) = stage</span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这里<code>createResultStage</code>所返回的<code>ResultStage</code>继承了<code>Stage</code>类。<code>Stage</code>类有个<code>rdd</code>参数，对<code>ResultStage</code>而言就是<code>finalRDD</code>，对<code>ShuffleMapStage</code>而言就是<code>ShuffleDependency.rdd</code><br><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createShuffleMapStage</span></span>[K, V, C](</span><br><span class="line">    shuffleDep: ShuffleDependency[K, V, C], jobId: Int): ShuffleMapStage = &#123;</span><br><span class="line">  val rdd = shuffleDep.rdd</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p>下面我们来看看<code>checkBarrierStageWithNumSlots</code>这个函数，因为它会抛出<code>BarrierJobSlotsNumberCheckFailed</code>这个异常，被<code>handleJobSubmitted</code>捕获。这个函数主要是为了检测是否有足够的slots去运行所有的barrier task。<a href="https://zhuanlan.zhihu.com/p/49628311" target="_blank" rel="noopener">屏障调度器</a>是Spark为了支持深度学习在2.4.0版本所引入的一个特性。它要求在barrier stage中同时启动所有的Task，当任意的task执行失败的时候，总是重启整个barrier stage。这么麻烦是因为Spark希望能够在Task中提供一个barrier以供显式同步。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">checkBarrierStageWithNumSlots</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> numPartitions = rdd.getNumPartitions</span><br><span class="line">  <span class="keyword">val</span> maxNumConcurrentTasks = sc.maxNumConcurrentTasks</span><br><span class="line">  <span class="keyword">if</span> (rdd.isBarrier() &amp;&amp; numPartitions &gt; maxNumConcurrentTasks) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">BarrierJobSlotsNumberCheckFailed</span>(numPartitions, maxNumConcurrentTasks)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">BarrierJobSlotsNumberCheckFailed</span> =&gt;</span><br><span class="line">      <span class="comment">// If jobId doesn't exist in the map, Scala coverts its value null to 0: Int automatically.</span></span><br><span class="line">      <span class="comment">// barrierJobIdToNumTasksCheckFailures是一个ConcurrentHashMap，表示对每个BarrierJob上失败的Task数量</span></span><br><span class="line">      <span class="keyword">val</span> numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">        (_: <span class="type">Int</span>, value: <span class="type">Int</span>) =&gt; value + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      ...</span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">        messageScheduler.schedule(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, finalRDD, func,</span><br><span class="line">              partitions, callSite, listener, properties))</span><br><span class="line">          &#125;,</span><br><span class="line">          timeIntervalNumTasksCheck,</span><br><span class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Job failed, clear internal data.</span></span><br><span class="line">        barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</span><br><span class="line">      listener.jobFailed(e)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Job submitted, clear internal data.</span></span><br><span class="line">  barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p>下面开始创建Job。<code>ActiveJob</code>表示在<code>DAGScheduler</code>里面运行的一个Job。<br>Job只负责向“叶子”Stage要结果，而之前Stage的运行是由<code>DAGScheduler</code>来调度的。这是因为若干Job可能共用同一个Stage的计算结果，所以将某个Stage强行归属到某个Job是不符合Spark设计逻辑的。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">  clearCacheLocs()</span><br><span class="line">  <span class="comment">// 在这里会打印四条日志，这个可以被用来在Spark.log里面定位事件</span></span><br><span class="line">  logInfo(<span class="string">"Got job %s (%s) with %d output partitions"</span>.format(</span><br><span class="line">    job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">  logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">" ("</span> + finalStage.name + <span class="string">")"</span>)</span><br><span class="line">  logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)</span><br><span class="line">  logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">  <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">  <span class="comment">// 从最后一个stage开始调用submitStage</span></span><br><span class="line">  submitStage(finalStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Stage阶段"><a href="#Stage阶段" class="headerlink" title="Stage阶段"></a>Stage阶段</h3><p>Stage是如何划分的呢？又是如何计算Stage之间的依赖的？我们继续查看<code>submitStage</code>这个函数，对于一个Stage，首先调用<code>getMissingParentStages</code>来看看它的父Stage能不能直接用，也就是说这个Stage的rdd所依赖的<strong>所有</strong>父RDD能不能直接用，如果不行的话，就要先算父Stage的。在前面的论述里，我们知道，若干Job可能共用同一个Stage的计算结果，而不同的Stage也可能依赖同一个RDD。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">	<span class="comment">// 找到这个stage所属的job</span></span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="comment">// 如果依赖之前的Stage，先列出来，并且按照id排序</span></span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">"missing: "</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">      	<span class="comment">// 运行这个Stage</span></span><br><span class="line">        logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      	<span class="comment">// 先提交所有的parent stage</span></span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面具体查看<code>getMissingParentStages</code>这个函数，可以看到，Stage的计算链是以最后一个RDD为树根逆着向上遍历得到的，而这个链条的终点要么是一个<code>ShuffleDependency</code>，要么是一个所有分区都被缓存了的RDD。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">  <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="comment">// 这里是个**DFS**，栈是手动维护的，主要是为了防止爆栈</span></span><br><span class="line">  waitingForVisit += stage.rdd</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">      visited += rdd</span><br><span class="line">      <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">      <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">        <span class="comment">// 如果这个RDD有没有被缓存的Partition，那么它就需要被计算</span></span><br><span class="line">        <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">          <span class="comment">// 我们检查这个RDD的所有依赖</span></span><br><span class="line">          dep <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</span><br><span class="line">              <span class="comment">// 我们发现一个宽依赖，因此我们创建一个新的Shuffle Stage，并加入到missing中（如果不存在）</span></span><br><span class="line">              <span class="comment">// 由于是宽依赖，所以我们不需要向上找了</span></span><br><span class="line">              <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">              <span class="keyword">if</span> (!mapStage.isAvailable) &#123;</span><br><span class="line">                missing += mapStage</span><br><span class="line">              &#125;</span><br><span class="line">            <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt;</span><br><span class="line">              <span class="comment">// 如果是一个窄依赖，就加入到waitingForVisit中</span></span><br><span class="line">              <span class="comment">// prepend是在头部加，+=是在尾部加</span></span><br><span class="line">              waitingForVisit.prepend(narrowDep.rdd)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    visit(waitingForVisit.remove(<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line">  missing.toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Task阶段"><a href="#Task阶段" class="headerlink" title="Task阶段"></a>Task阶段</h3><p>下面是重头戏<code>submitMissingTasks</code>，这个方法负责生成TaskSet，并且将它提交给TaskScheduler低层调度器。<br><code>partitionsToCompute</code>计算有哪些分区是待计算的。根据Stage类型的不同，<code>findMissingPartitions</code>的计算方法也不同。对于<code>ResultStage</code>来说，<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">  logDebug(<span class="string">"submitMissingTasks("</span> + stage + <span class="string">")"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// ResultStage.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> job = activeJob.get</span><br><span class="line">  (<span class="number">0</span> until job.numPartitions).filter(id =&gt; !job.finished(id))</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ActiveJob.scala</span></span><br><span class="line"><span class="keyword">val</span> numPartitions = finalStage <span class="keyword">match</span> &#123;</span><br><span class="line">  <span class="comment">// 对于ResultStage，不一定得到当前rdd的所有分区，例如first()和lookup()的Action，</span></span><br><span class="line">  <span class="comment">// 因此这里是r.partitions而不是r.rdd.partitions</span></span><br><span class="line">  <span class="keyword">case</span> r: <span class="type">ResultStage</span> =&gt; r.partitions.length</span><br><span class="line">  <span class="keyword">case</span> m: <span class="type">ShuffleMapStage</span> =&gt; m.rdd.partitions.length</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ShuffleMapStage.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(): <span class="type">Seq</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  mapOutputTrackerMaster</span><br><span class="line">    .findMissingPartitions(shuffleDep.shuffleId)</span><br><span class="line">    .getOrElse(<span class="number">0</span> until numPartitions)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MapOutputTrackerMaster.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findMissingPartitions</span></span>(shuffleId: <span class="type">Int</span>): <span class="type">Option</span>[<span class="type">Seq</span>[<span class="type">Int</span>]] = &#123;</span><br><span class="line">  shuffleStatuses.get(shuffleId).map(_.findMissingPartitions())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这个<code>outputCommitCoordinator</code>是由<code>SparkEnv</code>维护的<code>OutputCommitCoordinator</code>对象，它决定到底谁有权利向HDFS写数据。在Executor上的请求会通过他持有的Driver的<code>OutputCommitCoordinatorEndpoint</code>的引用发送给Driver处理<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">  <span class="comment">// with this Stage</span></span><br><span class="line">  val properties = jobIdToActiveJob(jobId).properties</span><br><span class="line"></span><br><span class="line">  runningStages += stage</span><br><span class="line">  <span class="comment">// 在检测Tasks是否serializable之前，就要SparkListenerStageSubmitted，</span></span><br><span class="line">  <span class="comment">// 如果不能serializable，那就在这**之后**给一个SparkListenerStageCompleted</span></span><br><span class="line"></span><br><span class="line">  stage match &#123;</span><br><span class="line">    case s: ShuffleMapStage =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(stage = s<span class="selector-class">.id</span>, maxPartitionId = s<span class="selector-class">.numPartitions</span> - <span class="number">1</span>)</span><br><span class="line">    case s: ResultStage =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(</span><br><span class="line">        stage = s<span class="selector-class">.id</span>, maxPartitionId = s<span class="selector-class">.rdd</span><span class="selector-class">.partitions</span><span class="selector-class">.length</span> - <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p>用<code>getPreferredLocs</code>计算每个分区的最佳计算位置，它实际上是调用<code>getPreferredLocsInternal</code>这个函数。这个函数是一个关于<code>visit: HashSet[(RDD[_], Int)]</code>的递归函数，visit用<code>(rdd, partition)</code>元组唯一描述一个分区。<code>getPreferredLocs</code>的计算逻辑是这样的：</p>
<ol>
<li>如果已经visit过了，就返回Nil</li>
<li>如果是被cached的，通过<code>getCacheLocs</code>返回cache的位置</li>
<li>如果RDD有自己的偏好位置，例如输入RDD，那么使用<code>rdd.preferredLocations</code>返回它的偏好位置</li>
<li>如果还没返回，但RDD有窄依赖，那么遍历它的所有依赖项，返回第一个具有位置偏好的依赖项的值</li>
</ol>
<p>理论上，一个最优的位置选取应该尽可能靠近数据源以减少网络传输，但目前版本的Spark还没有实现<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try &#123;</span><br><span class="line">    stage match &#123;</span><br><span class="line">      case s: ShuffleMapStage =&gt;</span><br><span class="line">        partitionsToCompute<span class="selector-class">.map</span> &#123; id =&gt; (id, getPreferredLocs(stage<span class="selector-class">.rdd</span>, id))&#125;.toMap</span><br><span class="line">      case s: ResultStage =&gt;</span><br><span class="line">        partitionsToCompute<span class="selector-class">.map</span> &#123; id =&gt;</span><br><span class="line">          val <span class="selector-tag">p</span> = s.partitions(id)</span><br><span class="line">          (id, getPreferredLocs(stage<span class="selector-class">.rdd</span>, p))</span><br><span class="line">        &#125;.toMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch &#123;</span><br><span class="line">    case NonFatal(e) =&gt;</span><br><span class="line">      <span class="comment">// 如果有非致命异常就创建一个新的Attempt，并且abortStage（这还不致命么）</span></span><br><span class="line">      stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">      listenerBus.post(SparkListenerStageSubmitted(stage<span class="selector-class">.latestInfo</span>, properties))</span><br><span class="line">      abortStage(stage, s<span class="string">"Task creation failed: $e\n$&#123;Utils.exceptionString(e)&#125;"</span>, Some(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      return</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p>下面，我们开始attempt这个Stage，我们需要将<code>taskBinaryBytes</code>通过<code>closureSerializer</code>打包，然后广播。<br><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果没有Task要执行，实际上就是skip了，那么就没有Submission Time这个字段</span></span><br><span class="line">  <span class="keyword">if</span> (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">    stage.latestInfo.submissionTime = Some(clock.getTimeMillis())</span><br><span class="line">  &#125;</span><br><span class="line">  listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.</span></span><br><span class="line">  <span class="comment">// Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast</span></span><br><span class="line">  <span class="comment">// the serialized copy of the RDD and for each task we will deserialize it, which means each</span></span><br><span class="line">  <span class="comment">// task gets a different copy of the RDD. This provides stronger isolation between tasks that</span></span><br><span class="line">  <span class="comment">// might modify state of objects referenced in their closures. This is necessary in Hadoop</span></span><br><span class="line">  <span class="comment">// where the JobConf/Configuration object is not thread-safe.</span></span><br><span class="line">  <span class="keyword">var</span> taskBinary: Broadcast[<span class="built_in">Array</span>[Byte]] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> partitions: <span class="built_in">Array</span>[Partition] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).</span></span><br><span class="line">    <span class="comment">// For ResultTask, serialize and broadcast (rdd, func).</span></span><br><span class="line">    <span class="keyword">var</span> taskBinaryBytes: <span class="built_in">Array</span>[Byte] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// taskBinaryBytes and partitions are both effected by the checkpoint status. We need</span></span><br><span class="line">    <span class="comment">// this synchronization in case another concurrent job is checkpointing this RDD, so we get a</span></span><br><span class="line">    <span class="comment">// consistent view of both variables.</span></span><br><span class="line">    RDDCheckpointData.synchronized &#123;</span><br><span class="line">      taskBinaryBytes = stage match &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="function"><span class="params">ShuffleMapStage</span> =&gt;</span></span><br><span class="line">          JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="function"><span class="params">ResultStage</span> =&gt;</span></span><br><span class="line">          <span class="comment">// 注意这里的stage.func已经被ClosureCleaner清理过了</span></span><br><span class="line">          JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      partitions = stage.rdd.partitions</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (taskBinaryBytes.length &gt; TaskSetManager.TASK_SIZE_TO_WARN_KIB * <span class="number">1024</span>) &#123;</span><br><span class="line">      logWarning(s<span class="string">"Broadcasting large task binary with size "</span> +</span><br><span class="line">        s<span class="string">"$&#123;Utils.bytesToString(taskBinaryBytes.length)&#125;"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 广播</span></span><br><span class="line">    taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="comment">// In the case of a failure during serialization, abort the stage.</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="function"><span class="params">NotSerializableException</span> =&gt;</span></span><br><span class="line">      abortStage(stage, <span class="string">"Task not serializable: "</span> + e.toString, Some(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Abort execution</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="function"><span class="params">Throwable</span> =&gt;</span></span><br><span class="line">      abortStage(stage, s<span class="string">"Task serialization failed: $e\n$&#123;Utils.exceptionString(e)&#125;"</span>, Some(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Abort execution</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>下面，我们根据Stage的类型生成Task。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  val tasks: Seq[Task[_]] = try &#123;</span><br><span class="line">    val serializedTaskMetrics = closureSerializer.serialize(stage<span class="selector-class">.latestInfo</span><span class="selector-class">.taskMetrics</span>).array()</span><br><span class="line">    stage match &#123;</span><br><span class="line">      case stage: ShuffleMapStage =&gt;</span><br><span class="line">        stage<span class="selector-class">.pendingPartitions</span><span class="selector-class">.clear</span>()</span><br><span class="line">        partitionsToCompute<span class="selector-class">.map</span> &#123; id =&gt;</span><br><span class="line">          val locs = taskIdToLocations(id)</span><br><span class="line">          val part = partitions(id)</span><br><span class="line">          stage<span class="selector-class">.pendingPartitions</span> += id</span><br><span class="line">          new ShuffleMapTask(stage<span class="selector-class">.id</span>, stage<span class="selector-class">.latestInfo</span><span class="selector-class">.attemptNumber</span>,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),</span><br><span class="line">            Option(sc.applicationId), sc<span class="selector-class">.applicationAttemptId</span>, stage<span class="selector-class">.rdd</span><span class="selector-class">.isBarrier</span>())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      case stage: ResultStage =&gt;</span><br><span class="line">        partitionsToCompute<span class="selector-class">.map</span> &#123; id =&gt;</span><br><span class="line">          val <span class="selector-tag">p</span>: Int = stage.partitions(id)</span><br><span class="line">          val part = partitions(p)</span><br><span class="line">          val locs = taskIdToLocations(id)</span><br><span class="line">          new ResultTask(stage<span class="selector-class">.id</span>, stage<span class="selector-class">.latestInfo</span><span class="selector-class">.attemptNumber</span>,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            Option(jobId), Option(sc.applicationId), sc<span class="selector-class">.applicationAttemptId</span>,</span><br><span class="line">            stage<span class="selector-class">.rdd</span><span class="selector-class">.isBarrier</span>())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>我们将生成的<code>tasks</code>包装成一个<code>TaskSet</code>，并且提交给<code>taskScheduler</code>。<br><figure class="highlight dart"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// DAGScheduler.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> (tasks.nonEmpty) &#123;</span><br><span class="line">    logInfo(s<span class="string">"Submitting <span class="subst">$&#123;tasks.size&#125;</span> missing tasks from $stage (<span class="subst">$&#123;stage.rdd&#125;</span>) (first 15 "</span> +</span><br><span class="line">      s<span class="string">"tasks are for partitions <span class="subst">$&#123;tasks.take(<span class="number">15</span>).map(_.partitionId)&#125;</span>)"</span>)</span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> TaskSet(</span><br><span class="line">      tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Because we posted SparkListenerStageSubmitted earlier, we should mark</span></span><br><span class="line">    <span class="comment">// the stage as completed here in case there are no tasks to run</span></span><br><span class="line">    markStageAsFinished(stage, None)</span><br><span class="line"></span><br><span class="line">    stage match &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: ShuffleMapStage =&gt;</span><br><span class="line">        logDebug(s<span class="string">"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; "</span> +</span><br><span class="line">            s<span class="string">"(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,"</span> +</span><br><span class="line">            s<span class="string">"available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,"</span> +</span><br><span class="line">            s<span class="string">"partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span>)</span><br><span class="line">        markMapStageJobsAsFinished(stage)</span><br><span class="line">      <span class="keyword">case</span> stage : ResultStage =&gt;</span><br><span class="line">        logDebug(s<span class="string">"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    submitWaitingChildStages(stage)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h2><p>在Stage和Stage之间，Spark需要Shuffle数据。这个流程包含上一个Stage上的Shuffle Write，中间的数据传输，以及下一个Stage的Shuffle Read。如下图所示<br><img src="/img/sparksql/shuffle.png" alt=""></p>
<p>Shuffle类操作常常发生在宽依赖的RDD之间，这类算子需要将多个节点上的数据拉取到同一节点上进行计算，其中存在大量磁盘IO、序列化和网络传输开销，它们可以分为以下几点来讨论。<br>当Spark中的某个节点故障之后，常常需要重算RDD中的某几个分区。对于窄依赖而言，父RDD的一个分区只对应一个子RDD分区，因此丢失子RDD的分区，重算整个父RDD分区是必要的。而对于宽依赖而言，父RDD会被多个子RDD使用，而可能当前丢失的子RDD只使用了父RDD中的某几个分区的数据，而我们仍然要重新计算整个父RDD，这造成了计算资源的浪费。<br>当使用Aggregate类（如<code>groupByKey</code>）或者Join类这种Shuffle算子时，如果选择的<code>key</code>上的数据是倾斜(skew)的，会导致部分节点上的负载增大。对于这种情况除了可以增加Executor的内存，还可以重新选择分区函数（例如在之前的key上加盐）来平衡分区。<br>Shuffle Read操作容易产生OOM，其原因是尽管在<code>BlockStoreShuffleReader</code>中会产生外部排序的<code>resultIter</code>，但在这之前，<code>ExternalAppendOnlyMap</code>先要从BlockManager拉取数据<code>(k, v)</code>到自己的<code>currentMap</code>中，如果这里的<code>v</code>很大，那么就会导致Executor的OOM问题。可以从<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">PairRDDFunctions</a>的文档中佐证这一点。在<code>Dataset</code>中并没有<code>reduceByKey</code>，原因可能<a href="https://stackoverflow.com/questions/38383207/rolling-your-own-reducebykey-in-spark-dataset" target="_blank" rel="noopener">与Catalyst Optimizer的优化</a>有关，但考虑到<code>groupByKey</code>还是比较坑的，感觉这个举措并不明智。</p>
<h3 id="Shuffle考古"><a href="#Shuffle考古" class="headerlink" title="Shuffle考古"></a>Shuffle考古</h3><p>在Spark0.8版本前，Spark只有Hash Based Shuffle的机制。在这种方式下，假定Shuffle Write阶段（有的也叫Map阶段）有<code>W</code>个Task，在Shuffle Read阶段（有的也叫Reduce阶段）有<code>R</code>个Task，那么就会产生<code>W*R</code>个文件。这样的坏处是对文件系统产生很大压力，并且IO也差（随机读写）。由于这些文件是先全量在内存里面构造，再dump到磁盘上，所以Shuffle在Write阶段就很可能OOM。<br>为了解决这个问题，在Spark 0.8.1版本加入了File Consolidation，以求将<code>W</code>个Task的输出尽可能合并。现在，Executor上的每一个<a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">执行单位</a>都生成自己独一份的文件。假定总共有<code>C</code>个核心，每个Task占用<code>T</code>个核心，那么总共有<code>C/T</code>个执行单位。考虑极端情况，如果<code>C==T</code>，那么任务实际上是串行的，所以写一个文件就行了。因此，最终会生成<code>C/T*R</code>个文件。<br>但这个版本仍然没有解决OOM的问题。虽然对于reduce这类操作，比如count，因为是来一个combine一个，所以只要你的V不是数组，也不想强行把结果concat成一个数组，一般都没有较大的内存问题。但是考虑如果我们执行<code>groupByKey</code>这样的操作，在Read阶段每个Task需要得到得到自己负责的key对应的所有value，而我们现在每个Task得到的是若干很大的文件，这个文件里面的key是杂乱无章的。如果我们需要得到一个key对应的所有value，那么我们就需要遍历这个文件，将key和对应的value全部存放在一个结构比如HashMap中，并进行合并。因此，我们必须保证这个HashMap足够大。既然如此，我们很容易想到一个基于外部排序的方案，我们为什么不能对key进行外排呢？确实在Hadoop MapReduce中会做归并排序，因此Reducer侧的数据按照key组织好的了。但Spark在这个版本没有这么做，并且Spark在下一个版本就这么做了。<br>在Spark 0.9版本之后，引入了<code>ExternalAppendOnlyMap</code>，通过这个结构，SparkShuffle在combine的时候如果内存不够，就能Spill到磁盘，并在Spill的时候进行排序。当然，<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions" target="_blank" rel="noopener">内存还是要能承载一个KV的</a>，我们将在稍后的源码分析中深入研究这个问题。</p>
<p>终于在Spark1.1版本之后引入了Sorted Based Shuffle。此时，Shuffle Write阶段会按照Partition id以及key对记录进行排序。同时将全部结果写到一个数据文件中，同时生成一个索引文件，Shuffle Read的Task可以通过该索引文件获取相关的数据。</p>
<h3 id="Shuffle-Read端源码分析"><a href="#Shuffle-Read端源码分析" class="headerlink" title="Shuffle Read端源码分析"></a>Shuffle Read端源码分析</h3><p>Shuffle Read一般位于一个Stage的开始，这时候上一个Stage会给我们留下一个ShuffledRDD。在它的<code>compute</code>方法中会首先取出<code>shuffleManager: ShuffleManager</code>。<code>ShuffleManager</code>是一个Trait，它的两个实现就是<code>org.apache.spark.shuffle.hash.HashShuffleManager</code>和<br><code>org.apache.spark.shuffle.sort.SortShuffleManager</code><br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">override def compute(<span class="built_in">split</span>: Partition, <span class="built_in">context</span>: TaskContext): Iterator[(K, C)] = &#123;</span><br><span class="line">  val dep = <span class="built_in">dependencies</span>.head.asInstanceOf[ShuffleDependency[K, V, C]]</span><br><span class="line">  val metrics = <span class="built_in">context</span>.taskMetrics().createTempShuffleReadMetrics()</span><br><span class="line">  SparkEnv.<span class="built_in">get</span>.shuffleManager // 由SparkEnv维护的ShuffleManager</span><br><span class="line">    .getReader(dep.shuffleHandle, <span class="built_in">split</span>.index, <span class="built_in">split</span>.index + <span class="number">1</span>, <span class="built_in">context</span>, metrics) // 返回一个BlockStoreShuffleReader</span><br><span class="line">    .<span class="built_in">read</span>().asInstanceOf[Iterator[(K, C)]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>ShuffleManager</code>通过调用<code>BlockStoreShuffleReader.read</code>返回一个迭代器<code>Iterator[(K, C)]</code>。在<code>BlockStoreShuffleReader.read</code>方法中，首先得到一个<code>ShuffleBlockFetcherIterator</code><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">  <span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">    ...</span><br><span class="line">    ) <span class="comment">// 返回一个ShuffleBlockFetcherIterator</span></span><br><span class="line">    .toCompletionIterator <span class="comment">// 返回一个Iterator[(BlockId, InputStream)]</span></span><br></pre></td></tr></table></figure></p>
<p><code>ShuffleBlockFetcherIterator</code>用<code>fetchUpToMaxBytes()</code>和 <code>fetchLocalBlocks()</code>分别读取remote和local的Block。在拉取远程数据的时候，会统计<code>bytesInFlight</code>、<code>reqsInFlight</code>等信息，并使用<code>maxBytesInFlight</code>和<code>maxReqsInFlight</code>节制。同时，为了允许5个并发同时拉取数据，还会设置<code>targetRemoteRequestSize = math.max(maxBytesInFlight / 5, 1L)</code>去请求每次拉取数据的最大大小。通过<code>ShuffleBlockFetcherIterator.splitLocalRemoteBytes</code>，现在改名叫<code>partitionBlocksByFetchMode</code>函数，可以将blocks分为Local和Remote的。关于两个函数的具体实现，将单独讨论。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">val serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line">// <span class="keyword">Create</span> a <span class="keyword">key</span>/<span class="keyword">value</span> iterator <span class="keyword">for</span> <span class="keyword">each</span> stream</span><br><span class="line">val recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">  serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// <span class="keyword">Update</span> the <span class="keyword">context</span> task metrics <span class="keyword">for</span> <span class="keyword">each</span> <span class="built_in">record</span> read.</span><br><span class="line">// CompletionIterator相比普通的Iterator的区别就是在结束之后会调用一个completion函数</span><br><span class="line">// CompletionIterator通过它伴生对象的<span class="keyword">apply</span>方法创建，传入第二个参数即completionFunction</span><br><span class="line">val metricIter = CompletionIterator[(<span class="keyword">Any</span>, <span class="keyword">Any</span>), Iterator[(<span class="keyword">Any</span>, <span class="keyword">Any</span>)]](</span><br><span class="line">  recordIter.map &#123; <span class="built_in">record</span> =&gt;</span><br><span class="line">    readMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">record</span></span><br><span class="line">  &#125;,</span><br><span class="line">  context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line">// An interruptible iterator must be used here <span class="keyword">in</span> <span class="keyword">order</span> <span class="keyword">to</span> support task cancellation</span><br><span class="line">val interruptibleIter = <span class="keyword">new</span> InterruptibleIterator[(<span class="keyword">Any</span>, <span class="keyword">Any</span>)](<span class="keyword">context</span>, metricIter)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p>
<p>经过一系列转换，我们得到一个<code>interruptibleIter</code>。接下来，根据是否有mapSideCombine对它进行聚合。这里的<code>dep</code>来自于<code>BaseShuffleHandle</code>对象，它是一个<code>ShuffleDependency</code>。在前面Spark的任务调度中已经提到，<code>ShuffleDependency</code>就是宽依赖。<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line">  ...</span><br><span class="line">  val aggregatedIter: Iterator[Product2[K, C]] = <span class="keyword">if</span> (dep<span class="selector-class">.aggregator</span><span class="selector-class">.isDefined</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="comment">// We are reading values that are already combined</span></span><br><span class="line">      val combinedKeyValuesIterator = interruptibleIter<span class="selector-class">.asInstanceOf</span>[Iterator[(K, C)]]</span><br><span class="line">      dep<span class="selector-class">.aggregator</span><span class="selector-class">.get</span><span class="selector-class">.combineCombinersByKey</span>(combinedKeyValuesIterator, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We don't know the value type, but also don't care -- the dependency *should*</span></span><br><span class="line">      <span class="comment">// have made sure its compatible w/ this aggregator, which will convert the value</span></span><br><span class="line">      <span class="comment">// type to the combined type C</span></span><br><span class="line">      val keyValuesIterator = interruptibleIter<span class="selector-class">.asInstanceOf</span>[Iterator[(K, Nothing)]]</span><br><span class="line">      dep<span class="selector-class">.aggregator</span><span class="selector-class">.get</span><span class="selector-class">.combineValuesByKey</span>(keyValuesIterator, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    interruptibleIter<span class="selector-class">.asInstanceOf</span>[Iterator[Product2[K, C]]]</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<p>这里的<code>aggregator</code>是<code>Aggregator[K, V, C]</code>，这里的<code>K</code>、<code>V</code>和<code>C</code>与熟悉<code>combineByKey</code>的是一样的。需要注意的是，在combine的过程中借助了<code>ExternalAppendOnlyMap</code>，这是之前提到的在Spark 0.9中引入的重要特性。通过调用<code>insertAll</code>方法能够将<code>interruptibleIter</code>内部的数据添加到<code>ExternalAppendOnlyMap</code>中，并在之后更新MemoryBytesSpilled、DiskBytesSpilled、PeakExecutionMemory三个统计维度，这也是我们在Event Log中所看到的统计维度。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Aggregator.scala</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>] (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    mergeValue: (<span class="type">C</span>, <span class="type">V</span></span>) <span class="title">=&gt;</span> <span class="title">C</span>,</span></span><br><span class="line"><span class="class">    <span class="title">mergeCombiners</span></span>: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">combineValuesByKey</span></span>(</span><br><span class="line">      iter: <span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ExternalAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](createCombiner, mergeValue, mergeCombiners)</span><br><span class="line">    combiners.insertAll(iter)</span><br><span class="line">    updateMetrics(context, combiners)</span><br><span class="line">    combiners.iterator</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">combineCombinersByKey</span></span>(</span><br><span class="line">      iter: <span class="type">Iterator</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]],</span><br><span class="line">      context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">    <span class="keyword">val</span> combiners = <span class="keyword">new</span> <span class="type">ExternalAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](identity, mergeCombiners, mergeCombiners)</span><br><span class="line">    combiners.insertAll(iter)</span><br><span class="line">    updateMetrics(context, combiners)</span><br><span class="line">    combiners.iterator</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Update task metrics after populating the external map. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">updateMetrics</span></span>(context: <span class="type">TaskContext</span>, map: <span class="type">ExternalAppendOnlyMap</span>[_, _, _]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Option</span>(context).foreach &#123; c =&gt;</span><br><span class="line">      c.taskMetrics().incMemoryBytesSpilled(map.memoryBytesSpilled)</span><br><span class="line">      c.taskMetrics().incDiskBytesSpilled(map.diskBytesSpilled)</span><br><span class="line">      c.taskMetrics().incPeakExecutionMemory(map.peakMemoryUsedBytes)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在获得Aggregate迭代器之后，最后一步，我们要进行排序，这时候就需要用到<code>ExternalSorter</code>这个对象。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BlockStoreShuffleReader.scala</span></span><br><span class="line">...</span><br><span class="line">  <span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">      <span class="keyword">val</span> sorter = <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">      sorter.insertAll(aggregatedIter)</span><br><span class="line">      context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">      context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">      context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">      <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">      context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">        sorter.stop()</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      aggregatedIter</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="ExternalAppendOnlyMap和AppendOnlyMap"><a href="#ExternalAppendOnlyMap和AppendOnlyMap" class="headerlink" title="ExternalAppendOnlyMap和AppendOnlyMap"></a>ExternalAppendOnlyMap和AppendOnlyMap</h3><p>我们查看<code>ExternalAppendOnlyMap</code>的实现。<code>ExternalAppendOnlyMap</code>拥有一个<code>currentMap</code>管理在内存中存储的键值对们。和一个<code>DiskMapIterator</code>的数组<code>spilledMaps</code>，表示Spill到磁盘上的键值对们。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span>[collection] <span class="keyword">var</span> currentMap = <span class="keyword">new</span> <span class="type">SizeTrackingAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> spilledMaps = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">DiskMapIterator</span>]</span><br></pre></td></tr></table></figure></p>
<p>先来看<code>currentMap</code>，它是一个<code>SizeTrackingAppendOnlyMap</code>。这个东西实际上就是一个<code>AppendOnlyMap</code>，不过给它加上了统计数据大小的功能，主要是借助于<code>SizeTracker</code>中<code>afterUpdate</code>和<code>resetSamples</code>两个方法。我们知道<strong>非序列化对象</strong>在内存存储上是<a href="https://www.ibm.com/developerworks/cn/analytics/library/ba-cn-apache-spark-memory-management/index.html" target="_blank" rel="noopener">不连续的</a>，我们需要通过遍历迭代器才能知道对象的具体大小，而这个开销是比较大的。因此，通过<code>SizeTracker</code>我们可以得到一个内存空间占用的估计值，从来用来判定是否需要Spill。<br>下面，我们来看<code>currentMap.insertAll</code>这个方法<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(entries: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (currentMap == <span class="literal">null</span>) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(</span><br><span class="line">      <span class="string">"Cannot insert new elements into a map after calling iterator"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 我们复用update函数，从而避免每一次都创建一个新的闭包（编程环境这么恶劣的么。。。）</span></span><br><span class="line">  <span class="keyword">var</span> curEntry: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">val</span> update: (<span class="type">Boolean</span>, <span class="type">C</span>) =&gt; <span class="type">C</span> = (hadVal, oldVal) =&gt; &#123;</span><br><span class="line">    <span class="keyword">if</span> (hadVal) </span><br><span class="line">      <span class="comment">// 如果不是第一个V，就merge</span></span><br><span class="line">      <span class="comment">// mergeValue: (C, V) =&gt; C,</span></span><br><span class="line">      mergeValue(oldVal, curEntry._2) </span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      <span class="comment">// 如果是第一个V，就新建一个C</span></span><br><span class="line">      <span class="comment">// createCombiner: V =&gt; C,</span></span><br><span class="line">      createCombiner(curEntry._2)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (entries.hasNext) &#123;</span><br><span class="line">    curEntry = entries.next()</span><br><span class="line">    <span class="keyword">val</span> estimatedSize = currentMap.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">      _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(currentMap, estimatedSize)) &#123;</span><br><span class="line">      <span class="comment">// 如果发生了Spill，就重新创建一个currentMap</span></span><br><span class="line">      currentMap = <span class="keyword">new</span> <span class="type">SizeTrackingAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// key: K, updateFunc: (Boolean, C) =&gt; C</span></span><br><span class="line">    currentMap.changeValue(curEntry._1, update)</span><br><span class="line">    addElementsRead()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看出，在<code>insertAll</code>中主要做了两件事情：</p>
<ol>
<li><p>遍历<code>curEntry &lt;- entries</code>，并通过传入的<code>update</code>函数进行Combine<br> 在内部存储上，<code>AppendOnlyMap</code>，包括后面将看到的一些其他KV容器，都倾向于将<code>(K, V)</code>对放到哈希表的相邻两个位置，这样的好处应该是避免访问时再进行一次跳转。</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 这里的nullValue和haveNullValue是用来单独处理k为null的情况的，下面会详细说明</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> haveNullValue = <span class="literal">false</span></span><br><span class="line"><span class="comment">// 有关null.asInstanceOf[V]的花里胡哨的语法，详见 https://stackoverflow.com/questions/10749010/if-an-int-cant-be-null-what-does-null-asinstanceofint-mean</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> nullValue: <span class="type">V</span> = <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeValue</span></span>(key: <span class="type">K</span>, updateFunc: (<span class="type">Boolean</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">V</span> = &#123;</span><br><span class="line">  <span class="comment">// updateFunc就是从insertAll传入的update</span></span><br><span class="line">  assert(!destroyed, destructionMessage)</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">  <span class="keyword">if</span> (k.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!haveNullValue) &#123;</span><br><span class="line">      <span class="comment">// 如果这时候还没有null的这个key，就新创建一个</span></span><br><span class="line">      incrementSize()</span><br><span class="line">    &#125;</span><br><span class="line">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class="line">    haveNullValue = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">return</span> nullValue</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">1</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 乘以2的原因是他按照K1 V1 K2 V2这样放的</span></span><br><span class="line">    <span class="keyword">val</span> curKey = data(<span class="number">2</span> * pos)</span><br><span class="line">    <span class="keyword">if</span> (curKey.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">      <span class="comment">// 如果对应的key不存在，就新创建一个</span></span><br><span class="line">      <span class="comment">// 这也是为什么前面要单独处理null的原因，这里的null被用来做placeholder了</span></span><br><span class="line">      <span class="comment">// 可以看到，第一个参数传的false，第二个是花里胡哨的null</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">false</span>, <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>]) </span><br><span class="line">      data(<span class="number">2</span> * pos) = k</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      incrementSize()</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (k.eq(curKey) || k.equals(curKey)) &#123; <span class="comment">// 又是从Java继承下来的花里胡哨的特性</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">true</span>, data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 再散列</span></span><br><span class="line">      <span class="keyword">val</span> delta = i</span><br><span class="line">      pos = (pos + delta) &amp; mask</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>] <span class="comment">// Never reached but needed to keep compiler happy</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>估计<code>currentMap</code>的当前大小，并调用<code>currentMap.maybeSpill</code>向磁盘Spill<br> 我们将在单独的章节论述<code>SizeTracker</code>如何估计集合大小，先看具体的Spill过程，可以梳理出<code>shouldSpill==true</code>的情况<br> 1、 <code>elementsRead % 32 == 0</code><br> 2、 <code>currentMemory &gt;= myMemoryThreshold</code><br> 3、 通过<code>acquireMemory</code>请求的内存不足以扩展到<code>2 * currentMemory</code>的大小</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spillable.scala</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  ...</span><br><span class="line">  </span><br><span class="line"><span class="comment">// MemoryConsumer.scala</span></span><br><span class="line">public long acquireMemory(long size) &#123;</span><br><span class="line">  long granted = taskMemoryManager.acquireExecutionMemory(size, <span class="keyword">this</span>);</span><br><span class="line">  used += granted;</span><br><span class="line">  <span class="keyword">return</span> granted;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p> 下面就是真正Spill的过程了，其实就是调用spill函数</p>
 <figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Spillable.scala</span></span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span> <span class="comment">// 统计Spill的次数</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span> <span class="comment">// 重置强制Spill计数器_elementsRead</span></span><br><span class="line">    <span class="comment">// 这里就是我们在Event Log里面看到的Memory Spill的统计量</span></span><br><span class="line">    <span class="comment">// 他表示在Spill之后我们能够释放多少内存</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>在<code>insertAll</code>之后，会返回一个迭代器，我们查看相关方法。可以发现如果<code>spilledMaps</code>都是空的，也就是没有Spill的话，就返回内存里面<code>currentMap</code>的<code>iterator</code>，否则就返回一个<code>ExternalIterator</code>。<br>对于第一种情况，会用<code>SpillableIterator</code>包裹一下。这个类在很多地方有定义，包括<code>ExternalAppendOnlyMap.scala</code>，<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala" target="_blank" rel="noopener">ExternalSorter.scala</a>里面。在当前使用的实现中，它实际上就是封装了一下<code>Iterator</code>，使得能够spill，转换成<code>CompletionIterator</code>等。<br>对于第二种情况，<code>ExternalIterator</code>比较有趣，将在稍后进行讨论。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="keyword">if</span> (spilledMaps.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// 如果没有发生Spill</span></span><br><span class="line">    destructiveIterator(currentMap.iterator)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果发生了Spill</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalIterator</span>()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">destructiveIterator</span></span>(inMemoryIterator: <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)] = &#123;</span><br><span class="line">  readingIterator = <span class="keyword">new</span> <span class="type">SpillableIterator</span>(inMemoryIterator)</span><br><span class="line">  readingIterator.toCompletionIterator</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>而<code>currentMap.iterator</code>实际上就是一个朴素无华的迭代器的实现。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nextValue</span></span>(): (<span class="type">K</span>, <span class="type">V</span>) = &#123;</span><br><span class="line">  <span class="keyword">if</span> (pos == <span class="number">-1</span>) &#123;    <span class="comment">// Treat position -1 as looking at the null value</span></span><br><span class="line">    <span class="keyword">if</span> (haveNullValue) &#123;</span><br><span class="line">      <span class="keyword">return</span> (<span class="literal">null</span>.asInstanceOf[<span class="type">K</span>], nullValue)</span><br><span class="line">    &#125;</span><br><span class="line">    pos += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span> (pos &lt; capacity) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!data(<span class="number">2</span> * pos).eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">      <span class="keyword">return</span> (data(<span class="number">2</span> * pos).asInstanceOf[<span class="type">K</span>], data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">    &#125;</span><br><span class="line">    pos += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="ExternalSorter"><a href="#ExternalSorter" class="headerlink" title="ExternalSorter"></a>ExternalSorter</h3><p><code>ExternalSorter</code>的作用是对输入的<code>(K, V)</code>进行排序，以产生新的<code>(K, C)</code>对，排序过程中可选择进行combine，否则输出的<code>C == V</code>。需要注意的是<code>ExternalSorter</code>不仅被用在Shuffle Read端，也被用在了Shuffle Write端，所以在后面会提到Map-side combine的概念。<code>ExternalSorter</code>具有如下的参数，在给定<code>ordering</code>之后，<code>ExternalSorter</code>就会按照它来排序。在Spark源码中建议如果希望进行Map-side combining的话，就指定<code>ordering</code>，否则就可以设置<code>ordering</code>为<code>null</code><br><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private[spark] class ExternalSorter[<span class="keyword">K</span>, V, <span class="keyword">C</span>](</span><br><span class="line">    context: TaskContext,</span><br><span class="line">    aggregator: Option[Aggregator[<span class="keyword">K</span>, V, <span class="keyword">C</span>]] = <span class="keyword">None</span>,</span><br><span class="line">    partitioner: Option[Partitioner] = <span class="keyword">None</span>,</span><br><span class="line">    ordering: Option[<span class="keyword">Ordering</span>[<span class="keyword">K</span>]] = <span class="keyword">None</span>,</span><br><span class="line">    serializer: Serializer = SparkEnv.get.serializer)</span><br><span class="line">  extends Spillable[WritablePartitionedPairCollection[<span class="keyword">K</span>, <span class="keyword">C</span>]](context.taskMemoryManager())</span><br></pre></td></tr></table></figure></p>
<p>由于<code>ExternalSorter</code>支持有combine和没有combine的两种模式，因此对应设置了两个对象。<code>map = new PartitionedAppendOnlyMap[K, C]</code>，以及<code>buffer = new PartitionedPairBuffer[K, C]</code>。其中，<code>PartitionedAppendOnlyMap</code>就是一个<code>SizeTrackingAppendOnlyMap</code>。<code>PartitionedPairBuffer</code>则继承了<code>WritablePartitionedPairCollection</code>，由于不需要按照key进行combine，所以它的实现接近于一个Array。<br><code>ExternalSorter.insertAll</code>方法和之前看到的<code>ExternalAppendOnlyMap</code>方法是大差不差的，他也会对可以聚合的特征进行聚合，并且TODO上还说如果聚合之后的reduction factor不够明显，就停止聚合。</p>
<p>相比之前的aggregator，<code>ExternalSorter</code>不仅能aggregate，还能sort。所以为啥不直接搞一个<code>ExternalSorter</code>而是还要在前面垫一个<code>ExternalAppendOnlyMap</code>呢？</p>
<h3 id="ExternalIterator"><a href="#ExternalIterator" class="headerlink" title="ExternalIterator"></a>ExternalIterator</h3><p>下面我们来看<code>ExternalAppendOnlyMap</code>中<code>ExternalIterator</code>的实现。它是一个典型的外部排序的实现，有一个PQ用来merge。不过这次的迭代器换成了<code>destructiveSortedIterator</code>，也就是我们都是排序的了。这个道理也是显而易见的，不sort一下，我们怎么和硬盘上的数据做聚合呢？<br><figure class="highlight pony"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ExternalAppendOnlyMap.scala</span></span><br><span class="line"><span class="meta">val</span> mergeHeap = <span class="function"><span class="keyword">new</span> <span class="title">mutable</span>.<span class="title">PriorityQueue</span>[<span class="title">StreamBuffer</span>]</span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">sortedMap</span> = <span class="title">destructiveIterator</span>(currentMap.destructiveSortedIterator(keyComparator))</span></span><br><span class="line"><span class="function"><span class="comment">// 我们得到一个Array的迭代器</span></span></span><br><span class="line"><span class="function"><span class="title">val</span> <span class="title">inputStreams</span> = (<span class="type">Seq</span>(sortedMap) ++ <span class="title">spilledMaps</span>).<span class="title">map</span>(it =&gt; it.buffered)</span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="title">inputStreams</span>.<span class="title">foreach</span> &#123; <span class="title">it</span> =&gt;</span></span><br><span class="line">  <span class="meta">val</span> kcPairs = <span class="function"><span class="keyword">new</span> <span class="title">ArrayBuffer</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span></span><br><span class="line"><span class="function">  <span class="comment">// 读完所有具有所有相同hash(key)的序列，并创建一个StreamBuffer</span></span></span><br><span class="line"><span class="function">  <span class="comment">// 需要注意的是，由于哈希碰撞的原因，里面可能有多个key</span></span></span><br><span class="line"><span class="function">  <span class="title">readNextHashCode</span>(it, kcPairs)</span></span><br><span class="line"><span class="function">  <span class="title">if</span> (kcPairs.length &gt; <span class="number">0</span>) &#123;</span></span><br><span class="line"><span class="function">    <span class="title">mergeHeap</span>.<span class="title">enqueue</span>(new <span class="type">StreamBuffer</span>(it, kcPairs))</span></span><br><span class="line"><span class="function">  &#125;</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>我们先来看看<code>destructiveSortedIterator</code>的实现<br><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// AppendOnlyMap.scala</span></span><br><span class="line">def destructiveSortedIterator(keyComparator: <span class="type">Comparator</span>[K]): <span class="type">Iterator</span>[(K, V)] = &#123;</span><br><span class="line">  destroyed = <span class="literal">true</span></span><br><span class="line">  <span class="keyword">var</span> keyIndex, <span class="keyword">new</span><span class="type">Index</span> = <span class="number">0</span></span><br><span class="line">  <span class="comment">// 下面这个循环将哈希表里面散乱的KV对压缩到最前面</span></span><br><span class="line">  <span class="keyword">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class="line">    <span class="keyword">if</span> (data(<span class="number">2</span> * keyIndex) != <span class="literal">null</span>) &#123;</span><br><span class="line">      data(<span class="number">2</span> * <span class="keyword">new</span><span class="type">Index</span>) = data(<span class="number">2</span> * keyIndex)</span><br><span class="line">      data(<span class="number">2</span> * <span class="keyword">new</span><span class="type">Index</span> + <span class="number">1</span>) = data(<span class="number">2</span> * keyIndex + <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">new</span><span class="type">Index</span> += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    keyIndex += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  assert(curSize == <span class="keyword">new</span><span class="type">Index</span> + (<span class="keyword">if</span> (haveNullValue) <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[K, AnyRef]).sort(data, <span class="number">0</span>, <span class="keyword">new</span><span class="type">Index</span>, keyComparator)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 这下面和前面实现大差不差，就省略了</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Iterator</span>[(K, V)] &#123;</span><br><span class="line">    ...</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面我们来看看实现的<code>next()</code>接口函数，它是外部排序中的一个典型的归并过程。我们需要注意的是<code>minBuffer</code>是一个<code>StreamBuffer</code>，维护一个<code>hash(K), V</code>的<code>ArrayBuffer</code>，类似<code>H1 V1 H1 V2 H2 V3</code>这样的序列，而不是我们想的<code>(K, V)</code>流。因此其中是可能有哈希碰撞的。我们从<code>mergeHeap</code>中<code>dequeue</code>出来的<code>StreamBuffer</code>是当前<code>hash(K)</code>最小的所有<code>K</code>的集合。<br><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">override def next(): (K, C) = &#123;</span><br><span class="line">  if (mergeHeap.isEmpty) &#123;</span><br><span class="line">    // 如果堆是空的，就再见了</span><br><span class="line">    throw new NoSuchElementException</span><br><span class="line">  &#125;</span><br><span class="line">  // Select a key <span class="keyword">from</span> the StreamBuffer that holds the lowest key hash</span><br><span class="line">  // mergeHeap选择所有StreamBuffer中最小hash的，作为<span class="keyword">min</span>Buffer</span><br><span class="line">  val <span class="keyword">min</span>Buffer = mergeHeap.dequeue()</span><br><span class="line">  // <span class="keyword">min</span>Pairs是一个ArrayBuffer[T]，表示这个StreamBuffer维护的所有KV对</span><br><span class="line">  val <span class="keyword">min</span>Pairs = <span class="keyword">min</span>Buffer.pairs</span><br><span class="line">  val <span class="keyword">min</span>Hash = <span class="keyword">min</span>Buffer.<span class="keyword">min</span>KeyHash</span><br><span class="line">  // 从一个ArrayBuffer[T]中移出Index为<span class="number">0</span>的项目</span><br><span class="line">  val <span class="keyword">min</span>Pair = removeFromBuffer(<span class="keyword">min</span>Pairs, <span class="number">0</span>)</span><br><span class="line">  // 得到非哈希的 (<span class="keyword">min</span>Key, <span class="keyword">min</span>Combiner)</span><br><span class="line">  val <span class="keyword">min</span>Key = <span class="keyword">min</span>Pair._1</span><br><span class="line">  var <span class="keyword">min</span>Combiner = <span class="keyword">min</span>Pair._2</span><br><span class="line">  assert(hashKey(<span class="keyword">min</span>Pair) == <span class="keyword">min</span>Hash)</span><br><span class="line"></span><br><span class="line">  // For <span class="literal">all</span> other streams that may have this key (i.e. have the same minimum key hash),</span><br><span class="line">  // merge <span class="keyword">in</span> the corresponding value (if <span class="literal">any</span>) <span class="keyword">from</span> that stream</span><br><span class="line">  val mergedBuffers = ArrayBuffer[StreamBuffer](<span class="keyword">min</span>Buffer)</span><br><span class="line">  while (mergeHeap.nonEmpty &amp;&amp; mergeHeap.head.<span class="keyword">min</span>KeyHash == <span class="keyword">min</span>Hash) &#123;</span><br><span class="line">    val newBuffer = mergeHeap.dequeue()</span><br><span class="line">    // 如果newBuffer的key和<span class="keyword">min</span>Key相等的话（考虑哈希碰撞），就合并</span><br><span class="line">    <span class="keyword">min</span>Combiner = mergeIfKeyExists(<span class="keyword">min</span>Key, <span class="keyword">min</span>Combiner, newBuffer)</span><br><span class="line">    mergedBuffers += newBuffer</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  // Repopulate each visited stream buffer and add it back <span class="keyword">to</span> the queue if it is non-empty</span><br><span class="line">  mergedBuffers.foreach &#123; buffer =&gt;</span><br><span class="line">    if (buffer.isEmpty) &#123;</span><br><span class="line">      readNextHashCode(buffer.iterator, buffer.pairs)</span><br><span class="line">    &#125;</span><br><span class="line">    if (!buffer.isEmpty) &#123;</span><br><span class="line">      mergeHeap.enqueue(buffer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  (<span class="keyword">min</span>Key, <span class="keyword">min</span>Combiner)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="SizeTracker"><a href="#SizeTracker" class="headerlink" title="SizeTracker"></a>SizeTracker</h3><p>首先在每次集合更新之后，会调用<code>afterUpdate</code>，当到达采样的interval之后，会<code>takeSample</code>。<br><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line">protected <span class="function"><span class="keyword">def</span> <span class="title">afterUpdate</span></span>(): Unit = &#123;</span><br><span class="line">  numUpdates += <span class="number">1</span></span><br><span class="line">  <span class="keyword">if</span> (nextSampleNum == numUpdates) &#123;</span><br><span class="line">    takeSample()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>takeSample</code>函数中第一句话就涉及多个对象，一个一个来看。<br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line"><span class="keyword">private</span> def takeSample(): <span class="built_in">Unit</span> = &#123;</span><br><span class="line">  samples.enqueue(Sample(SizeEstimator.estimate(<span class="keyword">this</span>), numUpdates))</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p><code>SizeEstimator.estimate</code>的实现类似去做一个state队列上的BFS。<br><figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">private def estimate(obj: AnyRef, visited: IdentityHashMap[AnyRef, AnyRef]): Long = &#123;</span><br><span class="line">  val <span class="keyword">state</span> = new SearchState(visited)</span><br><span class="line">  <span class="keyword">state</span>.enqueue(obj)</span><br><span class="line">  while (!<span class="keyword">state</span>.isFinished) &#123;</span><br><span class="line">    visitSingleObject(<span class="keyword">state</span>.dequeue(), <span class="keyword">state</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">state</span>.size</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private def visitSingleObject(obj: AnyRef, <span class="keyword">state</span>: SearchState): Unit = &#123;</span><br><span class="line">  val cls = obj.getClass</span><br><span class="line">  if (cls.isArray) &#123;</span><br><span class="line">    visitArray(obj, cls, <span class="keyword">state</span>)</span><br><span class="line">  &#125; else if (cls.getName.startsWith(<span class="string">"scala.reflect"</span>)) &#123;</span><br><span class="line">    // M<span class="literal">any</span> objects <span class="keyword">in</span> the scala.reflect package reference <span class="keyword">global</span> reflection objects which, <span class="keyword">in</span></span><br><span class="line">    // turn, reference many other large <span class="keyword">global</span> objects. Do nothing <span class="keyword">in</span> this case.</span><br><span class="line">  &#125; else if (obj.isInstanceOf[ClassLoader] || obj.isInstanceOf[Class[_]]) &#123;</span><br><span class="line">    // Hadoop JobConfs created <span class="keyword">in</span> the interpreter have a ClassLoader, which greatly confuses</span><br><span class="line">    // the size estimator since it references the whole REPL. Do nothing <span class="keyword">in</span> this case. In</span><br><span class="line">    // general <span class="literal">all</span> ClassLoaders and Classes will be shared between objects anyway.</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    obj <span class="built_in">match</span> &#123;</span><br><span class="line">      case s: KnownSizeEstimation =&gt;</span><br><span class="line">        <span class="keyword">state</span>.size += s.estimatedSize</span><br><span class="line">      case _ =&gt;</span><br><span class="line">        val classInfo = getClassInfo(cls)</span><br><span class="line">        <span class="keyword">state</span>.size += alignSize(classInfo.shellSize)</span><br><span class="line">        <span class="keyword">for</span> (field <span class="variable">&lt;- classInfo.pointerFields) &#123;</span></span><br><span class="line"><span class="variable">          state.enqueue(field.get(obj))</span></span><br><span class="line"><span class="variable">        &#125;</span></span><br><span class="line"><span class="variable">    &#125;</span></span><br><span class="line"><span class="variable">  &#125;</span></span><br><span class="line"><span class="variable">&#125;</span></span><br></pre></td></tr></table></figure></p>
<p>然后我们创建一个<code>Sample</code>，并且放到队列<code>samples</code>中<br><figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> object SizeTracker &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="keyword">class</span> Sample(<span class="keyword">size</span>: <span class="keyword">Long</span>, numUpdates: <span class="keyword">Long</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>下面的主要工作就是计算一个<code>bytesPerUpdate</code><br><figure class="highlight processing"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  ...</span><br><span class="line">  <span class="comment">// Only use the last two samples to extrapolate</span></span><br><span class="line">  <span class="comment">// 如果sample太多了，就删除掉一些</span></span><br><span class="line">  <span class="keyword">if</span> (samples.<span class="built_in">size</span> &gt; <span class="number">2</span>) &#123;</span><br><span class="line">    samples.dequeue()</span><br><span class="line">  &#125;</span><br><span class="line">  val bytesDelta = samples.toList.<span class="built_in">reverse</span> <span class="built_in">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> latest :: previous :: tail =&gt;</span><br><span class="line">      (latest.<span class="built_in">size</span> - previous.<span class="built_in">size</span>).toDouble / (latest.numUpdates - previous.numUpdates)</span><br><span class="line">    <span class="comment">// If fewer than 2 samples, assume no change</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="number">0</span></span><br><span class="line">  &#125;</span><br><span class="line">  bytesPerUpdate = math.<span class="built_in">max</span>(<span class="number">0</span>, bytesDelta)</span><br><span class="line">  nextSampleNum = math.<span class="built_in">ceil</span>(numUpdates * SAMPLE_GROWTH_RATE).toLong</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们统计到上次估算之后经历的update数量，并乘以<code>bytesPerUpdate</code>，即可得到总大小<br><figure class="highlight flix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SizeTracker.scala</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">estimateSize</span></span>(): Long = &#123;</span><br><span class="line">  assert(samples.nonEmpty)</span><br><span class="line">  val extrapolatedDelta = bytesPerUpdate * (numUpdates - samples.last.numUpdates)</span><br><span class="line">  (samples.last.size + extrapolatedDelta).toLong</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="Shuffle-Write端源码分析"><a href="#Shuffle-Write端源码分析" class="headerlink" title="Shuffle Write端源码分析"></a>Shuffle Write端源码分析</h3><p>Shuffle Write端的实现主要依赖<code>ShuffleManager</code>中的<code>ShuffleWriter</code>对象，目前使用的<code>ShuffleManager</code>是<code>SortShuffleManager</code>，因此只讨论它。它是一个抽象类，主要有<code>SortShuffleWriter</code>、<code>UnsafeShuffleWriter</code>、<code>BypassMergeSortShuffleWriter</code>等实现。</p>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] </span>&#123;</span><br><span class="line">  <span class="comment">/** Write a sequence of records to this task's output */</span></span><br><span class="line">  <span class="meta">@throws</span>[<span class="type">IOException</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Close this writer, passing along whether the map completed */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(success: <span class="type">Boolean</span>): <span class="type">Option</span>[<span class="type">MapStatus</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>SortShuffleWriter</code>的实现可以说很简单了，就是将<code>records</code>放到一个<code>ExternalSorter</code>里面，然后创建一个<code>ShuffleMapOutputWriter</code>。<code>shuffleExecutorComponents</code>实际上是一个<code>LocalDiskShuffleExecutorComponents</code>。<code>ShuffleMapOutputWriter</code>是一个Java接口，实际上被创建的是<code>LocalDiskShuffleMapOutputWriter</code><br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 如果不需要进行mapSideCombine，那么我们传入空的aggregator和ordering，</span></span><br><span class="line">    <span class="comment">// 我们在map端不负责对key进行排序，统统留给reduce端吧</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don't bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> mapOutputWriter = shuffleExecutorComponents.createMapOutputWriter(</span><br><span class="line">    dep.shuffleId, mapId, dep.partitioner.numPartitions)</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p>
<p>紧接着，调用<code>ExternalSorter.writePartitionedMapOutput</code>将自己维护的<code>map</code>或者<code>buffer</code>（根据是否有Map Side Aggregation）写到<code>mapOutputWriter</code>提供的<code>partitionWriter</code>里面。其过程用到了一个<a href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/collection/WritablePartitionedPairCollection.scala" target="_blank" rel="noopener">叫<code>destructiveSortedWritablePartitionedIterator</code>的迭代器</a>，相比<code>destructiveSortedIterator</code>，它是多了Writable和Partitioned两个词。前者的意思是我可以写到文件，后者的意思是我先按照partitionId排序，然后在按照给定的Comparator排序。<br>接着就是<code>commitAllPartitions</code>，这个函数负责创建一个索引文件，并原子地提交。注意到，到当前版本，每一个执行单元只会生成一份数据文件和一份索引。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">  ...</span><br><span class="line">  sorter.writePartitionedMapOutput(dep.shuffleId, mapId, mapOutputWriter)</span><br><span class="line">  <span class="keyword">val</span> partitionLengths = mapOutputWriter.commitAllPartitions()</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths, mapId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LocalDiskShuffleMapOutputWriter.java</span></span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">public long[] commitAllPartitions() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  ...</span><br><span class="line">  cleanUp();</span><br><span class="line">  <span class="type">File</span> resolvedTmp = outputTempFile != <span class="literal">null</span> &amp;&amp; outputTempFile.isFile() ? outputTempFile : <span class="literal">null</span>;</span><br><span class="line">  blockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, resolvedTmp);</span><br><span class="line">  <span class="keyword">return</span> partitionLengths;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// IndexShuffleBlockResolver.java</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeIndexFileAndCommit</span></span>(shuffleId: <span class="type">Int</span>, mapId: <span class="type">Long</span>, lengths: <span class="type">Array</span>[<span class="type">Long</span>], dataTmp: <span class="type">File</span>): <span class="type">Unit</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">BlockId</span>, dirs: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">String</span>]]): <span class="type">ManagedBuffer</span></span><br></pre></td></tr></table></figure></p>
<p>写完的索引文件将会被<code>getBlockData</code>函数查阅，从而知道每个block的开始与结束位置。这个<code>getBlockData</code>同样位于<code>IndexShuffleBlockResolver</code>类中。它们继承了<code>ShuffleBlockResolver</code>这个trait，用定义如何从一个logical shuffle block identifier（例如map、reduce或shuffle）中。这个类维护Block和文件的映射关系，维护index文件，向<code>BlockStore</code>提供抽象。</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>下面我们来看看<code>BypassMergeSortShuffleWriter</code>的实现。它到底Bypass了什么东西呢？其实是sort和aggregate。</p>
<h3 id="fetchLocalBlocks和fetchUpToMaxBytes的实现"><a href="#fetchLocalBlocks和fetchUpToMaxBytes的实现" class="headerlink" title="fetchLocalBlocks和fetchUpToMaxBytes的实现"></a>fetchLocalBlocks和fetchUpToMaxBytes的实现</h3><p>简单说明一下<code>fetchLocalBlocks</code>和<code>fetchUpToMaxBytes</code>的实现<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ShuffleBlockFetcherIterator.scala</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> localBlocks = scala.collection.mutable.<span class="type">LinkedHashSet</span>[<span class="type">BlockId</span>]()</span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">fetchLocalBlocks</span></span>() &#123;</span><br><span class="line">  logDebug(<span class="string">s"Start fetching local blocks: <span class="subst">$&#123;localBlocks.mkString(", ")&#125;</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> iter = localBlocks.iterator</span><br><span class="line">  <span class="keyword">while</span> (iter.hasNext) &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = iter.next()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> buf = blockManager.getBlockData(blockId)</span><br><span class="line">      ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// BlockManager.scala</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getBlockData</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">ManagedBuffer</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (blockId.isShuffle) &#123;</span><br><span class="line">    <span class="comment">// 需要通过ShuffleBlockResolver来获取</span></span><br><span class="line">    shuffleManager.shuffleBlockResolver.getBlockData(blockId.asInstanceOf[<span class="type">ShuffleBlockId</span>])</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    getLocalBytes(blockId) <span class="keyword">match</span> &#123;</span><br></pre></td></tr></table></figure></p>
<h2 id="Spark分布式部署方式"><a href="#Spark分布式部署方式" class="headerlink" title="Spark分布式部署方式"></a>Spark分布式部署方式</h2><h3 id="Spark自有部署方式"><a href="#Spark自有部署方式" class="headerlink" title="Spark自有部署方式"></a>Spark自有部署方式</h3><p>最常用的其实是单机模式也就是<code>spark-submit --master local</code>，这里local是默认选项。在程序执行过程中，只会生成一个SparkSubmit进程，不会产生Master和Worker节点，也不依赖Hadoop。当然，Windows里面可能需要winutils这个工具的，但也是直接下载，而不需要装Hadoop。<br>在集群化上，Spark可以部署在<a href="https://www.jianshu.com/p/65a3476757a5" target="_blank" rel="noopener">On Yarn和On Mesos、K8S和Standalone</a>上面，而又分别对应了Cluster和Client两种deploy mode。</p>
<p>首先是Spark自带Cluster Manager的Standalone Client模式，也是我们最常用的集群测试模式，需要启动Master和Slave节点，但仍然不依赖Hadoop。<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">./bin/spark-submit</span> <span class="params">--master</span> spark:<span class="string">//localhost</span><span class="function">:7077</span>  <span class="params">--class</span> org.apache.spark.examples.SparkPi <span class="string">./examples/jars/spark-examples_2.11-2.4.4.jar</span> 100</span><br></pre></td></tr></table></figure></p>
<p>下面一种是Spark自带Cluster Manager的Standalone Cluster模式，一字之差，还是有不同的，用下面的方式启动<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">./bin/spark-submit</span> <span class="params">--master</span> spark:<span class="string">//wl1</span><span class="function">:6066</span> <span class="params">--deploy-mode</span> cluster <span class="comment"># 默认cluster</span></span><br></pre></td></tr></table></figure></p>
<p>上面两种的配置一般修改Spark的spark-defaults.conf和spark-env.sh也就可以了，不涉及hadoop</p>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>Spark跑在yarn上面，这个还依赖hadoop集群，但Spark不需要自己提供Master和Worker了。Yarn同样提供了Cluster和Client两种模式，如下所示<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --<span class="keyword">master</span> <span class="title">yarn-cluster</span></span><br><span class="line">./bin/spark-submit --<span class="keyword">master</span> <span class="title">yarn-client</span></span><br></pre></td></tr></table></figure></p>
<p>Yarn Cluster就是通常使用的部署方式，此时Spark Driver是运行在Yarn的ApplicationMaster上的，而Client方式的Driver是在任务提交机上面运行，<a href="https://www.zybuluo.com/sasaki/note/252413" target="_blank" rel="noopener">ApplicationMaster只负责向ResourceManager申请Executor需要的资源</a>。</p>
<p>我们在Spark的WebUI中常常看到诸如Container、ApplicationMaster、ResourceMaster、NodeMaster这些东西，其实他们都是Yarn里面的常见概念。具体的联系在下面的图上一目了然。<br><img src="/img/sparksql/yarn.png" alt=""><br>对应到Spark中，<a href="https://www.zybuluo.com/sasaki/note/252413" target="_blank" rel="noopener">每个SparkContext对应一个ApplicationMaster，每个Executor对应一个Container</a>。</p>
<h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><p>打开项目源码根目录，SparkSQL由4个项目组成，分别为Spark Core、Spark Catalyst、Spark Hive和Spark Hive ThriftServer。我们主要和前两者打交道，Core中就是SparkSQL的核心，包括Dataset等类的实现。Catalyst是Spark的水晶优化器。</p>
<h2 id="DataFrame和Dataset"><a href="#DataFrame和Dataset" class="headerlink" title="DataFrame和Dataset"></a>DataFrame和Dataset</h2><p>我们可以将RDD看为一个分布式的容器<code>M[T]</code>，我们对<code>T</code>是未知的。而事实上我们处理数据集往往就是个来自HBase或者其他数据仓库大宽表。如果使用RDD会导致很多的拆箱和装箱的操作。并且由于<code>T</code>是一个黑盒，Spark也很难对RDD的计算进行优化。为此，Spark推出了SparkSQL来解决这个问题。而SparkSQL的一个核心机制就是DataFrame和Dataset。由于DataFrame可以看做一个<code>Dataset[Row]</code>，所以，我们主要以Dataset来研究对象。</p>
<h3 id="从RDD到DF-DS"><a href="#从RDD到DF-DS" class="headerlink" title="从RDD到DF/DS"></a>从RDD到DF/DS</h3><p>RDD可以通过<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SQLImplicits.scala" target="_blank" rel="noopener">下面代码中的一个隐式转换</a> 得到一个<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DatasetHolder.scala" target="_blank" rel="noopener">DatasetHolder</a>，接着借助于<code>DatasetHolder</code>中提供的<code>toDS</code>和<code>toDF</code>来实现到<code>DataFrame</code>和<code>Dataset</code>的转换。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">implicit def rddToDatasetHolder[<span class="string">T : Encoder</span>](<span class="link">rdd: RDD[T]</span>): DatasetHolder[T] = &#123;</span><br><span class="line"><span class="code">    DatasetHolder(_sqlContext.createDataset(rdd))</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其实在上面文件里面还定义了一系列隐式转换所需要的<code>Encoder</code>，例如对于大多数的case class都需要调用<code>newProductArrayEncoder</code>。有关这部分的进一步说明，可以查看<a href="https://github.com/summerDG/spark-code-analysis/blob/master/analysis/sql/spark_sql_parser.md" target="_blank" rel="noopener">文章</a></p>
<p>同样，从Dataset/DataFrame到RDD可以通过调用<code>.rdd</code>方法来轻松得到。不过这个操作是Action么？在爆栈网上有<a href="https://stackoverflow.com/questions/44708629/is-dataset-rdd-an-action-or-transformation" target="_blank" rel="noopener">相关讨论1，认为不是Action但有开销</a>；和<a href="https://stackoverflow.com/questions/55885145/whats-the-overhead-of-converting-an-rdd-to-a-dataframe-and-back-again" target="_blank" rel="noopener">相关讨论2，认为是无开销的</a>。我们查看具体代码<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">lazy val rdd: RDD[T] = &#123;</span><br><span class="line">    val objectType = exprEnc<span class="selector-class">.deserializer</span><span class="selector-class">.dataType</span></span><br><span class="line">    rddQueryExecution<span class="selector-class">.toRdd</span><span class="selector-class">.mapPartitions</span> &#123; rows =&gt;</span><br><span class="line">        rows.map(_.get(<span class="number">0</span>, objectType)<span class="selector-class">.asInstanceOf</span>[T])</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="从DF到DS"><a href="#从DF到DS" class="headerlink" title="从DF到DS"></a>从DF到DS</h3><h3 id="从DS到DF"><a href="#从DS到DF" class="headerlink" title="从DS到DF"></a>从DS到DF</h3><figure class="highlight haxe"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">@scala.annotation.varargs</span><br><span class="line">def toDF(colNames: <span class="type">String</span>*): <span class="type">DataFrame </span>= &#123;</span><br><span class="line">    require(schema.size == colNames.size,</span><br><span class="line">      <span class="string">"The number of columns doesn't match.\n"</span> +</span><br><span class="line">        s<span class="string">"Old column names ($&#123;schema.size&#125;): "</span> + schema.fields.map(<span class="literal">_</span>.name).mkString(<span class="string">", "</span>) + <span class="string">"\n"</span> +</span><br><span class="line">        s<span class="string">"New column names ($&#123;colNames.size&#125;): "</span> + colNames.mkString(<span class="string">", "</span>))</span><br><span class="line"></span><br><span class="line">    val <span class="keyword">new</span><span class="type">Cols</span> = logicalPlan.output.zip(colNames).map &#123; <span class="keyword">case</span> (oldAttribute, <span class="keyword">new</span><span class="type">Name</span>) =&gt;</span><br><span class="line">      Column(oldAttribute).as(<span class="keyword">new</span><span class="type">Name</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    select(<span class="keyword">new</span><span class="type">Cols</span> : <span class="type">_</span>*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Dataset算子"><a href="#Dataset算子" class="headerlink" title="Dataset算子"></a>Dataset算子</h2><p>在Dataset中，同样提供了诸如<code>map</code>之类的算子，不过它们的实现是从Dataset和DataFrame之间的变换了。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span> : <span class="type">Encoder</span>](func: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">Dataset</span>[<span class="type">U</span>] = withTypedPlan &#123;</span><br><span class="line">  <span class="type">MapElements</span>[<span class="type">T</span>, <span class="type">U</span>](func, logicalPlan)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">MapElements</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span> : <span class="type">Encoder</span>, <span class="type">U</span> : <span class="type">Encoder</span>](</span><br><span class="line">      func: <span class="type">AnyRef</span>,</span><br><span class="line">      child: <span class="type">LogicalPlan</span>): <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> deserialized = <span class="type">CatalystSerde</span>.deserialize[<span class="type">T</span>](child)</span><br><span class="line">    <span class="keyword">val</span> mapped = <span class="type">MapElements</span>(</span><br><span class="line">      func,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].clsTag.runtimeClass,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].schema,</span><br><span class="line">      <span class="type">CatalystSerde</span>.generateObjAttr[<span class="type">U</span>],</span><br><span class="line">      deserialized)</span><br><span class="line">    <span class="type">CatalystSerde</span>.serialize[<span class="type">U</span>](mapped)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">MapElements</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    func: <span class="type">AnyRef</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    argumentClass: <span class="type">Class</span>[_],</span></span></span><br><span class="line"><span class="class"><span class="params">    argumentSchema: <span class="type">StructType</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    outputObjAttr: <span class="type">Attribute</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    child: <span class="type">LogicalPlan</span></span>) <span class="keyword">extends</span> <span class="title">ObjectConsumer</span> <span class="keyword">with</span> <span class="title">ObjectProducer</span></span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">TypedFilter</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">T</span> : <span class="type">Encoder</span>](func: <span class="type">AnyRef</span>, child: <span class="type">LogicalPlan</span>): <span class="type">TypedFilter</span> = &#123;</span><br><span class="line">    <span class="type">TypedFilter</span>(</span><br><span class="line">      func,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].clsTag.runtimeClass,</span><br><span class="line">      implicitly[<span class="type">Encoder</span>[<span class="type">T</span>]].schema,</span><br><span class="line">      <span class="type">UnresolvedDeserializer</span>(encoderFor[<span class="type">T</span>].deserializer),</span><br><span class="line">      child)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="SparkSQL的上下文"><a href="#SparkSQL的上下文" class="headerlink" title="SparkSQL的上下文"></a>SparkSQL的上下文</h2><p><code>SparkSQL</code>的上下文通过<code>SQLContext</code>维护，它由一个SparkSession持有，并指向其所有者，以及所有者维护的SparkContext。在Spark 2.0之后，大部分<code>SparkSQL</code>的逻辑工作被迁移到了<code>SparkSession</code>中，所以这个类可以被看做是一个兼容性的封装。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SQLContext</span> <span class="title">private</span>[sql](<span class="params">val sparkSession: <span class="type">SparkSession</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">sessionState</span></span>: <span class="type">SessionState</span> = sparkSession.sessionState</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">sharedState</span></span>: <span class="type">SharedState</span> = sparkSession.sharedState</span><br><span class="line">  <span class="keyword">private</span>[sql] <span class="function"><span class="keyword">def</span> <span class="title">conf</span></span>: <span class="type">SQLConf</span> = sessionState.conf</span><br></pre></td></tr></table></figure></p>
<h2 id="SparkSQL的解析流程"><a href="#SparkSQL的解析流程" class="headerlink" title="SparkSQL的解析流程"></a>SparkSQL的解析流程</h2><p>首先会对SQL进行Parse，得到一个Unresolved LogicalPlan。这里Unresolved的意思是诸如变量名和表名这些东西是不确定的，需要在Analyzer的阶段借助于Catalog来决议得到LogicalPlan。Catalog就是描述了SQLContext里面的诸如表之类的对象。<br>通过<a href="https://github.com/apache/spark/tree/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis" target="_blank" rel="noopener">Analyzer</a>，将Unresolved LogicalPlan决议为Logical Plan。<br><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala" target="_blank" rel="noopener">Logical Plan</a>继承了<code>QueryPlan[LogicalPlan]</code>，是一棵AST。SparkSQL的执行目标就是树根的值，在计算过程中，父节点的计算依赖于子节点的计算结果。<code>LogicalPlan</code>又拥有三个子类<code>BinaryNode</code>/<code>UnaryNode</code>和<code>LeafNode</code>，然后有产生了<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala" target="_blank" rel="noopener">OrderPreservingUnaryNode</a>等子类。这些Node被<a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/basicLogicalOperators.scala" target="_blank" rel="noopener">另一些子类</a>所继承，这些<code>basicLogicalOperators</code><a href="https://blog.csdn.net/oopsoom/article/details/38274621" target="_blank" rel="noopener">描述了包括Project/Filter/Sample/Union/Join/Limit等操作</a>。<br>通过Optimizer，对Logical Plan进行优化。Catalyst主要做的是RBO，但诸如华为等公司和机构也有提出过CBO的方案。<br>优化后的Optimized Logical Plan会生成为Physical Plan。Physical Plan在Spark中的对应代码实现是<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkPlan.scala" target="_blank" rel="noopener">SparkPlan</a>，他同样继承了<code>QueryPlan[SparkPlan]</code>。</p>
<h2 id="Spark-SQL的执行流程"><a href="#Spark-SQL的执行流程" class="headerlink" title="Spark SQL的执行流程"></a>Spark SQL的执行流程</h2><p>和RDD一样，Dataset同样只在Action操作才会计算，我们选取最典型的<code>count()</code>来研究。可以看到，<code>count</code>操作实际上会执行<code>plan.executeCollect()</code>，而这里的<code>plan</code>是一个<code>SparkPlan</code>，<code>qe</code>是一个<code>QueryExecution</code>。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// Dataset.scala</span><br><span class="line">def count(): Long = withAction("count", groupBy().count().queryExecution) &#123; plan =&gt;</span><br><span class="line">  plan.executeCollect().head.getLong(0)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private def withAction[<span class="string">U</span>](<span class="link">name: String, qe: QueryExecution</span>)(action: SparkPlan =&gt; U) = &#123;</span><br><span class="line">  SQLExecution.withNewExecutionId(sparkSession, qe, Some(name)) &#123;</span><br><span class="line"><span class="code">    qe.executedPlan.foreach &#123; plan =&gt;</span></span><br><span class="line"><span class="code">      plan.resetMetrics()</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    action(qe.executedPlan)</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">private def withNewExecutionId[<span class="string">U</span>](<span class="link">body: =&gt; U</span>): U = &#123;</span><br><span class="line">  SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>QueryExecution</code>用来描述整个SQL执行的上下文，从如下示例中可以看出，它维护了从Unsolved Logical Plan到Physical Plan的整个转换流程。<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val ds = Seq(Person("Calvin", 22, 1)).toDS</span><br><span class="line">ds: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint ... 1 more field]</span><br><span class="line"></span><br><span class="line">scala&gt; val fds = ds.filter(p =&gt; p.age&gt;1)</span><br><span class="line">fds: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint ... 1 more field]</span><br><span class="line"></span><br><span class="line">scala&gt; ds.queryExecution</span><br><span class="line">res9: org.apache.spark.sql.execution.QueryExecution =</span><br><span class="line"><span class="section">== Parsed Logical Plan ==</span></span><br><span class="line">LocalRelation [name#3, age#4L, money#5L]</span><br><span class="line"></span><br><span class="line"><span class="section">== Analyzed Logical Plan ==</span></span><br><span class="line">name: string, age: bigint, money: bigint</span><br><span class="line">LocalRelation [name#3, age#4L, money#5L]</span><br><span class="line"></span><br><span class="line"><span class="section">== Optimized Logical Plan ==</span></span><br><span class="line">LocalRelation [name#3, age#4L, money#5L]</span><br><span class="line"></span><br><span class="line"><span class="section">== Physical Plan ==</span></span><br><span class="line">LocalTableScan [name#3, age#4L, money#5L]</span><br><span class="line"><span class="code">```</span>Scala</span><br><span class="line">那么，<span class="code">`count()`</span>做的就是对<span class="code">`qe`</span>做一些手脚，然后调用<span class="code">`qe.executedPlan.executeCollect().head.getLong(0)`</span>。于是我们查看<span class="code">`executeCollect()`</span>这个方法，他实际上就是execute和collect两部分。execute部分实际上是对<span class="code">`getByteArrayRdd`</span>的一个调用。而<span class="code">`collect`</span>部分就是调用<span class="code">`byteArrayRdd.collect()`</span>，从而得到一个<span class="code">`ArrayBuffer[InternalRow]`</span>。</span><br><span class="line"><span class="code">```</span>Scala</span><br><span class="line"><span class="comment">// SparkPlan.scala</span></span><br><span class="line">def executeCollect(): Array[InternalRow] = &#123;</span><br><span class="line"><span class="code">  // byteArrayRdd是一个RDD[(Long, Array[Byte])]</span></span><br><span class="line"><span class="code">  val byteArrayRdd = getByteArrayRdd()</span></span><br><span class="line"></span><br><span class="line"><span class="code">  val results = ArrayBuffer[InternalRow]()</span></span><br><span class="line"><span class="code">  byteArrayRdd.collect().foreach &#123; countAndBytes =&gt;</span></span><br><span class="line"><span class="code">    decodeUnsafeRows(countAndBytes._2).foreach(results.+=)</span></span><br><span class="line"><span class="code">  &#125;</span></span><br><span class="line"><span class="code">  results.toArray</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><code>getByteArrayRdd</code>的作用是将一系列<code>UnsafeRow</code>打包成一个<code>Array[Byte]</code>以方便序列化，这个<code>Array[Byte]</code>的结构是<code>[size] [bytes of UnsafeRow] [size] [bytes of UnsafeRow] ... [-1]</code>。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getByteArrayRdd</span></span>(n: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">RDD</span>[(<span class="type">Long</span>, <span class="type">Array</span>[<span class="type">Byte</span>])] = &#123;</span><br><span class="line">  execute().mapPartitionsInternal &#123; iter =&gt;</span><br><span class="line">    <span class="keyword">var</span> count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">val</span> buffer = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](<span class="number">4</span> &lt;&lt; <span class="number">10</span>)  <span class="comment">// 4K</span></span><br><span class="line">    <span class="keyword">val</span> codec = <span class="type">CompressionCodec</span>.createCodec(<span class="type">SparkEnv</span>.get.conf)</span><br><span class="line">    <span class="keyword">val</span> bos = <span class="keyword">new</span> <span class="type">ByteArrayOutputStream</span>()</span><br><span class="line">    <span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(codec.compressedOutputStream(bos))</span><br><span class="line">    <span class="comment">// `iter.hasNext` may produce one row and buffer it, we should only call it when the limit is</span></span><br><span class="line">    <span class="comment">// not hit.</span></span><br><span class="line">    <span class="keyword">while</span> ((n &lt; <span class="number">0</span> || count &lt; n) &amp;&amp; iter.hasNext) &#123;</span><br><span class="line">      <span class="keyword">val</span> row = iter.next().asInstanceOf[<span class="type">UnsafeRow</span>]</span><br><span class="line">      out.writeInt(row.getSizeInBytes)</span><br><span class="line">      row.writeToStream(out, buffer)</span><br><span class="line">      count += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    out.writeInt(<span class="number">-1</span>)</span><br><span class="line">    out.flush()</span><br><span class="line">    out.close()</span><br><span class="line">    <span class="type">Iterator</span>((count, bos.toByteArray))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>可以看到<code>getByteArrayRdd</code>中调用了<code>execute</code>方法，继而调用<code>doExecute</code>，得到一个<code>RDD[InternalRow]</code>。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">execute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>] = executeQuery &#123;</span><br><span class="line">  <span class="keyword">if</span> (isCanonicalizedPlan) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"A canonicalized plan is not supposed to be executed."</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  doExecute()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doExecute</span></span>(): <span class="type">RDD</span>[<span class="type">InternalRow</span>]</span><br></pre></td></tr></table></figure></p>
<p>由于SparkPlan是一个抽象类，所以这里的<code>doExecute()</code>没有看到实现，具体的实现根据其操作对象的不同分布在<a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/objects.scala" target="_blank" rel="noopener">objects.scala</a>上。</p>
<h2 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h2><p><a href="https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala" target="_blank" rel="noopener">Row</a>是SparkSQL的基石。它实际上是一个<code>trait</code>，我们经常使用是它的子类<code>GenericRow</code>和<code>GenericRowWithSchema</code>，而<code>Row</code>的内部实现则是<code>InternalRow</code>。<code>GenericRow是</code>Row<code>从</code>apply<code>创建时的默认构造。它没有schema。在</code>GenericRowWithSchema<code>中重新实现了</code>filedIndex<code>这个函数，允许我们使用</code>row.getAs<a href="&quot;colName&quot;">String</a><code>这样的方法。如果经常使用SparkSQL的API会发现我们不能从一个</code>DataFrame<code>通过map到一个</code>Row<code>的方式得到另一个</code>DataFrame<code>，反而可以从一个</code>Seq<code>得到，其原因就是因为</code>DataFrame<code>有schema而</code>Row<code>没有。我们通过下面的实验来检查从一个</code>Seq`到DataFrame的转换<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.types</span>.&#123;DoubleType, LongType, StringType, StructField, StructType&#125;</span><br><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.functions</span>.&#123;concat, lit, udf&#125;</span><br><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span>.&#123;Row&#125;</span><br><span class="line">import org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.catalyst</span><span class="selector-class">.expressions</span>.&#123;GenericRow, GenericRowWithSchema&#125;</span><br><span class="line">case class Person(name: String, age: Long, money: Long)</span><br><span class="line">import spark<span class="selector-class">.implicits</span>._</span><br><span class="line"></span><br><span class="line">val ds = Seq(Person(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">1</span>)).toDS</span><br><span class="line">val ds2 = Seq(Person(<span class="string">"Neo"</span>, <span class="number">23</span>, <span class="number">1</span>)).toDS</span><br><span class="line">val dfj = ds.union(ds2)</span><br><span class="line">val dsj_fail = dfj<span class="selector-class">.toDS</span> <span class="comment">// 注意DataFrame没有toDS方法，toDS是由RDD转DS用的</span></span><br><span class="line">val dsj = dfj<span class="selector-class">.as</span>[Person]</span><br><span class="line"></span><br><span class="line">val ds3 = Seq((<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">1</span>)).toDS</span><br><span class="line">val ds4 = Seq((<span class="string">"Neo"</span>, <span class="number">23</span>, <span class="number">1</span>)).toDS</span><br><span class="line">val dfj2 = ds3.union(ds4)</span><br></pre></td></tr></table></figure></p>
<p>有关<code>Row</code>和<code>GenericRowWithSchema</code>之间的转换，我们可以进行下面的实验<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 复用之前的头部</span></span><br><span class="line">val df = Seq(Person(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">100</span>), Person(<span class="string">"Neo"</span>, <span class="number">33</span>, <span class="number">300</span>)).toDF</span><br><span class="line"></span><br><span class="line"><span class="comment">// df的schema</span></span><br><span class="line">scala&gt; df.schema</span><br><span class="line">res2: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.types</span><span class="selector-class">.StructType</span> = StructType(StructField(name,StringType,true), StructField(age,LongType,false), StructField(money,LongType,false))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 第1行的schema</span></span><br><span class="line">scala&gt; df.take(<span class="number">1</span>)(<span class="number">0</span>).schema</span><br><span class="line">res3: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.types</span><span class="selector-class">.StructType</span> = StructType(StructField(name,StringType,true), StructField(age,LongType,false), StructField(money,LongType,false))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 查看type，发现是GenericRowWithSchema而不是Row</span></span><br><span class="line">scala&gt; df.take(<span class="number">1</span>)(<span class="number">0</span>)<span class="selector-class">.getClass</span><span class="selector-class">.getSimpleName</span></span><br><span class="line">res5: String = GenericRowWithSchema</span><br><span class="line"></span><br><span class="line"><span class="comment">// 增加一列</span></span><br><span class="line">scala&gt; val r = Row.unapplySeq(df.take(<span class="number">1</span>)(<span class="number">0</span>))<span class="selector-class">.get</span><span class="selector-class">.toArray</span> ++ Seq(<span class="string">"SZ"</span>)</span><br><span class="line">r: Array[Any] = Array(Calvin, <span class="number">22</span>, <span class="number">100</span>, SZ)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 对应增加一列schema</span></span><br><span class="line">scala&gt; val nsch = df.take(<span class="number">1</span>)(<span class="number">0</span>)<span class="selector-class">.schema</span><span class="selector-class">.add</span>(StructField(<span class="string">"addr"</span>,StringType,true))</span><br><span class="line">nsch: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.types</span><span class="selector-class">.StructType</span> = StructType(StructField(name,StringType,true), StructField(age,LongType,false), StructField(money,LongType,false), StructField(addr,StringType,true))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建一个新SchemaRow</span></span><br><span class="line">scala&gt; val row_sch = new GenericRowWithSchema(r, nsch)</span><br><span class="line">row_sch: org<span class="selector-class">.apache</span><span class="selector-class">.spark</span><span class="selector-class">.sql</span><span class="selector-class">.catalyst</span><span class="selector-class">.expressions</span><span class="selector-class">.GenericRowWithSchema</span> = [Calvin,<span class="number">22</span>,<span class="number">100</span>,SZ]</span><br></pre></td></tr></table></figure></p>
<p>只有<code>GenericRowWithSchema</code>有，因此我们可以创建一个<a href="https://stackoverflow.com/questions/33934615/how-to-introduce-the-schema-in-a-row-in-spark" target="_blank" rel="noopener">GenericRowWithSchema</a>，其实现在<code>org.apache.spark.sql.catalyst.expressions.{GenericRow, GenericRowWithSchema}</code>。</p>
<h2 id="列"><a href="#列" class="headerlink" title="列"></a>列</h2><p>从上文中可以看到，<code>DataFrame</code>中的数据依旧是按照行组织的，通过外挂了一个schema，我们能够有效地识别列。在这种情况下对行的改动是容易的，但是如何对列进行改动呢？一般有两种办法</p>
<h3 id="借助于withColumn"><a href="#借助于withColumn" class="headerlink" title="借助于withColumn"></a>借助于withColumn</h3><figure class="highlight gams"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val df = Seq(Person(<span class="string">"Calvin"</span>, <span class="number">22</span>, <span class="number">100</span>), Person(<span class="string">"Neo"</span>, <span class="number">33</span>, <span class="number">300</span>)).toDF</span><br><span class="line"><span class="comment">// 通过cast函数进行类型转换，concat函数进行字符串连接</span></span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, concat(<span class="symbol">$</span><span class="string">"name"</span>, <span class="symbol">$</span><span class="string">"age"</span>.cast(StringType))).show()</span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, <span class="symbol">$</span><span class="string">"age"</span>+<span class="symbol">$</span><span class="string">"money"</span>).show()</span><br></pre></td></tr></table></figure>
<p>当然，在<code>$</code>表达式之外，我们还可以使用udf，甚至<a href="https://stackoverflow.com/questions/53191271/how-to-do-conditional-withcolumn-in-a-spark-dataframe" target="_blank" rel="noopener">带条件地进行withColumn</a><br><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 除了$表达式，还可以使用udf</span></span><br><span class="line"><span class="keyword">val</span> addMoneyUDF = udf((age: <span class="built_in">Long</span>, money: <span class="built_in">Long</span>) =&gt; age + money)</span><br><span class="line">df.withColumn(<span class="string">"name2"</span>, addMoneyUDF($<span class="string">"age"</span>, $<span class="string">"money"</span>))</span><br></pre></td></tr></table></figure></p>
<p>特别需要注意的是<code>withColumn</code>是存在性能开销的。如果我们在代码里频繁（例如使用一个for循环）withColumn，那么就可能出现一个Job结束，而下一个Job迟迟不开始的情况。如果我们将日志等级设置为TRACE，可以看到代码中也存在了很多batch resolution的情况。这是因为较深层次的依赖会导致SparkSQL不能分清到底需要缓存哪些数据以用来恢复，因此只能全部缓存。另外<a href="https://medium.com/@manuzhang/the-hidden-cost-of-spark-withcolumn-8ffea517c015" target="_blank" rel="noopener">文章</a>中还表示会造成大量的analysis开销。此外，伴随着withColumn的是UDF或者UDAF的使用，在Spark the definitive一书中指出，这类的算子容易导致OOM等问题。</p>
<h2 id="groupBy和groupByKey"><a href="#groupBy和groupByKey" class="headerlink" title="groupBy和groupByKey"></a>groupBy和groupByKey</h2><p>不同于RDD的相关方法，DataFrame系列的<code>groupBy</code>和<code>groupByKey</code>会返回两个<a href="https://www.imooc.com/article/258924?block_id=tuijian_wz" target="_blank" rel="noopener">不同的类型</a>，<code>RelationalGroupedDataset</code>和<code>KeyValueGroupedDataset</code>。一般来说，虽然<code>groupByKey</code>更为灵活，能够生成自定义的key用来group，但<code>KeyValueGroupedDataset</code>只提供相对较少的操作，所以最好还是使用<code>groupby</code>。另外，在group操作之后就没有诸如union的操作，我们需要再显式map回<code>DataFrame</code>。</p>
<h1 id="Spark性能调优"><a href="#Spark性能调优" class="headerlink" title="Spark性能调优"></a>Spark性能调优</h1><p>一般来说，Spark可能出现瓶颈的地方有内存、网络和CPU，对于上面的这些问题，宜分为Driver和Executor两块进行考虑<br>内存方面的向硬盘的溢写、从gc.log中看到的GC的猛增、节点的未响应和OOM。<br>网络问题的主要场景是诸如Shuffle类的操作涉及在多个节点上传输，节点之间Connection reset by peer。</p>
<h2 id="Spark常见性能问题和选项"><a href="#Spark常见性能问题和选项" class="headerlink" title="Spark常见性能问题和选项"></a>Spark常见性能问题和选项</h2><table>
<thead>
<tr>
<th style="text-align:center">诊断</th>
<th style="text-align:center">现象</th>
<th style="text-align:center">解决方案</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Executor内存不足</td>
<td style="text-align:center">Driver端ExecutorLostFailure，Executor端gc.log显示大量GC和FullGC</td>
<td style="text-align:center">需要考虑Shuffle Read数据过大，或者数据倾斜。对于前者，可以考虑增加分区数或者换个Partitioner，增加Executor内存，增加Executor数量，减少Executor上的Task并行度，提前Filter，使用序列化。</td>
</tr>
<tr>
<td style="text-align:center">Executor内存不足</td>
<td style="text-align:center">Local Bytes Read+Remote Bytes Read很大</td>
<td style="text-align:center">考虑是Shuffle Read的问题，同上。需要注意的是当使用groupBy系列算子时，可能一个KV对就很大的，所以增加Executor内存会更保险</td>
</tr>
<tr>
<td style="text-align:center">Driver内存不足</td>
<td style="text-align:center">Driver端gc.log显示大量GC和FullGC，spark.log中DAGScheduler相关log显示collect算子耗时过长</td>
<td style="text-align:center">考虑增大Driver内存，避免collect大量数据</td>
</tr>
<tr>
<td style="text-align:center">Driver内存不足</td>
<td style="text-align:center">Driver端gc.log显示大量GC和FullGC</td>
<td style="text-align:center">减少UDF的使用，减少诸如withColumn的使用</td>
</tr>
<tr>
<td style="text-align:center">Driver内存不足</td>
<td style="text-align:center">Driver端gc.log显示大量GC和FullGC，Driver的spark.log中出现大量<code>BlockManagerInfo: Added broadcast</code>，并且剩余内存较少，Executor的spark.log中出现<code>TorrentBroadcast: Reading broadcast</code>事件且耗时过长</td>
<td style="text-align:center">减少broadcast的数据量</td>
</tr>
<tr>
<td style="text-align:center">数据倾斜</td>
<td style="text-align:center">少数Task耗时显著高于平均值</td>
<td style="text-align:center">考虑换个Partitioner，扩大<code>spark.shuffle.file.buffer</code>、<code>spark.reducer.maxSizeInFlight</code>、<code>spark.shuffle.memoryFraction</code>，打开spark.shuffle.consolidateFiles</td>
</tr>
</tbody>
</table>
<h3 id="替代性算子"><a href="#替代性算子" class="headerlink" title="替代性算子"></a>替代性算子</h3><p>为了避免由于Shuffle操作导致的性能问题，常用的解决方案是使用map-side-combine的算子。这个思路就是先将聚合操作下推到每个节点本地，再将每个节点上的聚合结果拉到同一节点上进行聚合，这样能够显著减少通信量。这种方法的常见实践就是采用如下所示的一些替代性算子:</p>
<table>
<thead>
<tr>
<th style="text-align:center">原算子</th>
<th style="text-align:center">替代算子</th>
<th style="text-align:center">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">groupByKey</td>
<td style="text-align:center">reduceByKey/aggregateByKey</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">reduceByKey</td>
<td style="text-align:center">aggregateByKey</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">foreach</td>
<td style="text-align:center">foreachPartitions</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">filter</td>
<td style="text-align:center">filter+coalesce</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">repartition+sort</td>
<td style="text-align:center">repartitionAndSortWithinPartitions</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">repartition</td>
<td style="text-align:center">coalesce</td>
<td style="text-align:center">如果目标分区数量小于当前分区数量</td>
</tr>
</tbody>
</table>
<h3 id="有关Persist的优化方案"><a href="#有关Persist的优化方案" class="headerlink" title="有关Persist的优化方案"></a>有关Persist的优化方案</h3><p>根据<a href="https://spark.apache.org/docs/latest/programming-guide.html#which-storage-level-to-choose" target="_blank" rel="noopener">RDD Programming Guide</a>，虽然Spark会自动做persist，但是对于肯定需要复用的数据，显式persist并没有坏处。这里需要注意的是我们要尽量提高RDD的复用程度。<br>一般来说，如果内存中能够全部放下对象，选择默认的<code>MEMORY_ONLY</code>级别能够最大程度利用CPU，否则就需要考虑使用序列化的<code>MEMORY_ONLY_SER</code>存储。当内存再不够时，就需要考虑将其持久化到磁盘上，但这会带来较高的时间代价。虽然在Spark的较新版本中，通过Unsafe Shuffle可以直接对序列化之后的对象进行sort shuffle，但这不是通用的。</p>
<h2 id="Event-log"><a href="#Event-log" class="headerlink" title="Event log"></a>Event log</h2><p>Spark会记录Event log，并在History Server或者Spark UI中供访问调试使用。</p>
<h2 id="HistoryServer"><a href="#HistoryServer" class="headerlink" title="HistoryServer"></a>HistoryServer</h2><p>Spark提供了History Server以保存<a href="https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_EventLog.md" target="_blank" rel="noopener">Event Log</a>，以便追踪历史任务的性能。History Server部署在18080，可以使用WebUI，也可以使用18080的<code>/api/vi/application</code>的api来请求json版本。<br>这种方式需要在运行前手动<code>export SPARK_MASTER_HOST=localhost</code>（会被诸如start-master.sh等文件访问修改）或者<code>sh ./sbin/start-master.sh -h localhost &amp;&amp; ./sbin/start-slave.sh spark://localhost:7077</code>可以通过<code>-h</code>指定localhost。不然可能Slave会连不上localhost，因为他会访问你的电脑名字，例如<code>CALVINNEO-MB0:7077</code>而不是localhost。<br>在<code>spark-defaults.conf</code>中，有关Event Log的配置项有两种，一个是在HDFS上，一个是在硬盘上。<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 硬盘</span></span><br><span class="line">spark.eventLog.enabled           <span class="literal">true</span></span><br><span class="line">spark.eventLog.dir               hdfs:<span class="string">//localhost</span><span class="function">:9000</span>/user/spark/appHist</span><br><span class="line"><span class="comment"># HDFS</span></span><br><span class="line">spark.<span class="keyword">history</span>.fs.logDirectory      <span class="string">.../spark-2.4.4-bin-hadoop2.7/conf/history/spark-events</span></span><br></pre></td></tr></table></figure></p>
<p>这个在磁盘上，供给History Server用，但是实际上和HDFS的内容是一样的。需要注意的是，一旦<code>spark.eventLog.enabled</code>被设置为<code>True</code>，就需要保证9000是可以访问的，不然可能会报错。</p>
<h2 id="spark-log"><a href="#spark-log" class="headerlink" title="spark log"></a>spark log</h2><h2 id="gc-log"><a href="#gc-log" class="headerlink" title="gc log"></a>gc log</h2><h2 id="常用调试方法"><a href="#常用调试方法" class="headerlink" title="常用调试方法"></a>常用调试方法</h2><ol>
<li><p>查看RDD的分区数</p>
 <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd<span class="selector-class">.partitions</span><span class="selector-class">.size</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看RDD的logical plan</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">rdd</span><span class="selector-class">.toDebugString</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看queryExecution</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">d</span><span class="selector-class">.queryExecution</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>查看schema</p>
 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">d</span><span class="selector-class">.printSchema</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h1 id="Spark常见错误"><a href="#Spark常见错误" class="headerlink" title="Spark常见错误"></a>Spark常见错误</h1><h2 id="变量在节点之间共享"><a href="#变量在节点之间共享" class="headerlink" title="变量在节点之间共享"></a>变量在节点之间共享</h2><p>当我们需要在节点间共享变量，例如将某个字符串从Driver发送到Executor上时，需要这个变量能够被序列化。特别地，有一个经典的Bug就是<code>Map#mapValues</code>不能被序列化，这个解决方案是在<code>mapValues</code>之后再<code>map(identity)</code>一下。<br>特别需要注意的是因为RDD是分布式存储的，所以不能够直接当做变量处理，例如下面的代码是不能够使用的。对于这种情况，要么是将其中一个小RDD广播，要不就是将两个RDD去做个JOIN。在SparkSQL中，JOIN操作会被视情况优化为广播。<br><figure class="highlight golo"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rdd1.<span class="keyword">map</span>&#123;</span><br><span class="line">    rdd2.<span class="keyword">filter</span>(...)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer"><a href="#scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer" class="headerlink" title="scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer"></a>scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer</h2><p>根据<a href="https://stackoverflow.com/questions/40199507/scala-collection-mutable-wrappedarrayofref-cannot-be-cast-to-integer" target="_blank" rel="noopener">SoF</a>，这个错误就是把<code>Array</code>改成<code>Seq</code>就好了。</p>
<h2 id="Extracting-Seq-String-String-String-from-spark-DataFrame"><a href="#Extracting-Seq-String-String-String-from-spark-DataFrame" class="headerlink" title="Extracting Seq[(String,String,String)] from spark DataFrame"></a>Extracting <code>Seq[(String,String,String)]</code> from spark DataFrame</h2><p>根据<a href="https://stackoverflow.com/questions/37553059/extracting-seqstring-string-string-from-spark-dataframe" target="_blank" rel="noopener">SoF</a></p>
<h1 id="Spark的其他组件的简介"><a href="#Spark的其他组件的简介" class="headerlink" title="Spark的其他组件的简介"></a>Spark的其他组件的简介</h1><h2 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h2><p>GraphX是基于Spark实现的一个图计算框架，能够对图进行建模。GraphX内置了一些实用的方法，如PageRank、SCC等，同时也提供了Pregel算法的API，我们可以利用Pregel来实现自己的一些图算法。目前GraphX似乎还没有实用的Python API，比较方便的是借助Scala。</p>
<h2 id="ML和MlLib"><a href="#ML和MlLib" class="headerlink" title="ML和MlLib"></a>ML和MlLib</h2><h2 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><a href="https://zhuanlan.zhihu.com/p/67068559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67068559</a></li>
<li><a href="http://www.jasongj.com/spark/rbo/" target="_blank" rel="noopener">http://www.jasongj.com/spark/rbo/</a></li>
<li><a href="https://www.kancloud.cn/kancloud/spark-internals/45243" target="_blank" rel="noopener">https://www.kancloud.cn/kancloud/spark-internals/45243</a></li>
<li><a href="https://www.jianshu.com/p/4c5c2e535da5" target="_blank" rel="noopener">https://www.jianshu.com/p/4c5c2e535da5</a></li>
<li><a href="http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/" target="_blank" rel="noopener">http://jerryshao.me/2014/01/04/spark-shuffle-detail-investigation/</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div></div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/img/fkm/wxfk.jpg" alt="Calvin Neo WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/img/fkm/zfbfk.jpg" alt="Calvin Neo Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Scala/" rel="tag"># Scala</a>
          
            <a href="/tags/Spark/" rel="tag"># Spark</a>
          
            <a href="/tags/SparkSQL/" rel="tag"># SparkSQL</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/06/scala-lang/" rel="next" title="使用Scala语言进行编程">
                <i class="fa fa-chevron-left"></i> 使用Scala语言进行编程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/11/23/multiprocessing-implement/" rel="prev" title="multiprocessing模块实现">
                multiprocessing模块实现 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/favicon.jpg"
               alt="Calvin Neo" />
          <p class="site-author-name" itemprop="name">Calvin Neo</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">124</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">139</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/CalvinNeo" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/CalvinNeo0" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1568200035" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://xqq.im/" title="xqq" target="_blank">xqq</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.lovelywen.com/" title="wenwen" target="_blank">wenwen</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://smlight.github.io/blog/" title="zyyyyy" target="_blank">zyyyyy</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark-Core"><span class="nav-number">1.</span> <span class="nav-text">Spark Core</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-number">1.1.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#常见RDD"><span class="nav-number">1.1.1.</span> <span class="nav-text">常见RDD</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#常见RDD-Function"><span class="nav-number">1.1.2.</span> <span class="nav-text">常见RDD Function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的架构概览"><span class="nav-number">1.2.</span> <span class="nav-text">Spark的架构概览</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark上下文"><span class="nav-number">1.3.</span> <span class="nav-text">Spark上下文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkEnv"><span class="nav-number">1.3.1.</span> <span class="nav-text">SparkEnv</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的任务调度"><span class="nav-number">1.4.</span> <span class="nav-text">Spark的任务调度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的存储管理"><span class="nav-number">1.5.</span> <span class="nav-text">Spark的存储管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark的内存管理"><span class="nav-number">1.6.</span> <span class="nav-text">Spark的内存管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-Job执行流程分析"><span class="nav-number">1.7.</span> <span class="nav-text">Spark Job执行流程分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Job阶段"><span class="nav-number">1.7.1.</span> <span class="nav-text">Job阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Stage阶段"><span class="nav-number">1.7.2.</span> <span class="nav-text">Stage阶段</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Task阶段"><span class="nav-number">1.7.3.</span> <span class="nav-text">Task阶段</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shuffle"><span class="nav-number">1.8.</span> <span class="nav-text">Shuffle</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle考古"><span class="nav-number">1.8.1.</span> <span class="nav-text">Shuffle考古</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle-Read端源码分析"><span class="nav-number">1.8.2.</span> <span class="nav-text">Shuffle Read端源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExternalAppendOnlyMap和AppendOnlyMap"><span class="nav-number">1.8.3.</span> <span class="nav-text">ExternalAppendOnlyMap和AppendOnlyMap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExternalSorter"><span class="nav-number">1.8.4.</span> <span class="nav-text">ExternalSorter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ExternalIterator"><span class="nav-number">1.8.5.</span> <span class="nav-text">ExternalIterator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SizeTracker"><span class="nav-number">1.8.6.</span> <span class="nav-text">SizeTracker</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle-Write端源码分析"><span class="nav-number">1.8.7.</span> <span class="nav-text">Shuffle Write端源码分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SortShuffleWriter"><span class="nav-number">1.8.8.</span> <span class="nav-text">SortShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BypassMergeSortShuffleWriter"><span class="nav-number">1.8.9.</span> <span class="nav-text">BypassMergeSortShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fetchLocalBlocks和fetchUpToMaxBytes的实现"><span class="nav-number">1.8.10.</span> <span class="nav-text">fetchLocalBlocks和fetchUpToMaxBytes的实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark分布式部署方式"><span class="nav-number">1.9.</span> <span class="nav-text">Spark分布式部署方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark自有部署方式"><span class="nav-number">1.9.1.</span> <span class="nav-text">Spark自有部署方式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn"><span class="nav-number">1.9.2.</span> <span class="nav-text">Yarn</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL"><span class="nav-number">2.</span> <span class="nav-text">SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame和Dataset"><span class="nav-number">2.1.</span> <span class="nav-text">DataFrame和Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#从RDD到DF-DS"><span class="nav-number">2.1.1.</span> <span class="nav-text">从RDD到DF/DS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从DF到DS"><span class="nav-number">2.1.2.</span> <span class="nav-text">从DF到DS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#从DS到DF"><span class="nav-number">2.1.3.</span> <span class="nav-text">从DS到DF</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dataset算子"><span class="nav-number">2.2.</span> <span class="nav-text">Dataset算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL的上下文"><span class="nav-number">2.3.</span> <span class="nav-text">SparkSQL的上下文</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL的解析流程"><span class="nav-number">2.4.</span> <span class="nav-text">SparkSQL的解析流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL的执行流程"><span class="nav-number">2.5.</span> <span class="nav-text">Spark SQL的执行流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Row"><span class="nav-number">2.6.</span> <span class="nav-text">Row</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#列"><span class="nav-number">2.7.</span> <span class="nav-text">列</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#借助于withColumn"><span class="nav-number">2.7.1.</span> <span class="nav-text">借助于withColumn</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#groupBy和groupByKey"><span class="nav-number">2.8.</span> <span class="nav-text">groupBy和groupByKey</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark性能调优"><span class="nav-number">3.</span> <span class="nav-text">Spark性能调优</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark常见性能问题和选项"><span class="nav-number">3.1.</span> <span class="nav-text">Spark常见性能问题和选项</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#替代性算子"><span class="nav-number">3.1.1.</span> <span class="nav-text">替代性算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关Persist的优化方案"><span class="nav-number">3.1.2.</span> <span class="nav-text">有关Persist的优化方案</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Event-log"><span class="nav-number">3.2.</span> <span class="nav-text">Event log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HistoryServer"><span class="nav-number">3.3.</span> <span class="nav-text">HistoryServer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spark-log"><span class="nav-number">3.4.</span> <span class="nav-text">spark log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gc-log"><span class="nav-number">3.5.</span> <span class="nav-text">gc log</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用调试方法"><span class="nav-number">3.6.</span> <span class="nav-text">常用调试方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark常见错误"><span class="nav-number">4.</span> <span class="nav-text">Spark常见错误</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#变量在节点之间共享"><span class="nav-number">4.1.</span> <span class="nav-text">变量在节点之间共享</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scala-collection-mutable-WrappedArray-ofRef-cannot-be-cast-to-Integer"><span class="nav-number">4.2.</span> <span class="nav-text">scala.collection.mutable.WrappedArray$ofRef cannot be cast to Integer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Extracting-Seq-String-String-String-from-spark-DataFrame"><span class="nav-number">4.3.</span> <span class="nav-text">Extracting Seq[(String,String,String)] from spark DataFrame</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Spark的其他组件的简介"><span class="nav-number">5.</span> <span class="nav-text">Spark的其他组件的简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#GraphX"><span class="nav-number">5.1.</span> <span class="nav-text">GraphX</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ML和MlLib"><span class="nav-number">5.2.</span> <span class="nav-text">ML和MlLib</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Streaming"><span class="nav-number">5.3.</span> <span class="nav-text">Streaming</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">6.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Calvin Neo</span>
  <span> &nbsp; Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
</div>
<div>
  <span><a href="/about/yytl/">版权声明</a></span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse 
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://www.calvinneo.com/2019/08/06/spark-sql/';
          this.page.identifier = '2019/08/06/spark-sql/';
          this.page.title = 'Spark和SparkSQL';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://calvinneo.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      $('#local-search-input').focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  

</body>
</html>
