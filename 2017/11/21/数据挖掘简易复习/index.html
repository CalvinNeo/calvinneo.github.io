<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="数据挖掘," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="《数据挖掘》课程简易复习提纲，主要根据PPT整理。时间仓促，不排除存在部分内容爆炸。">
<meta name="keywords" content="数据挖掘">
<meta property="og:type" content="article">
<meta property="og:title" content="数据挖掘简易复习">
<meta property="og:url" content="http://www.calvinneo.com/2017/11/21/数据挖掘简易复习/index.html">
<meta property="og:site_name" content="Calvin&#39;s Marbles">
<meta property="og:description" content="《数据挖掘》课程简易复习提纲，主要根据PPT整理。时间仓促，不排除存在部分内容爆炸。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-07-23T12:37:15.200Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据挖掘简易复习">
<meta name="twitter:description" content="《数据挖掘》课程简易复习提纲，主要根据PPT整理。时间仓促，不排除存在部分内容爆炸。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.calvinneo.com/2017/11/21/数据挖掘简易复习/"/>





  <title>数据挖掘简易复习 | Calvin's Marbles</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Calvin's Marbles</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.calvinneo.com/2017/11/21/数据挖掘简易复习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Calvin Neo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Calvin's Marbles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                数据挖掘简易复习
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-21T20:35:20+08:00">
                2017-11-21
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/21/数据挖掘简易复习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/21/数据挖掘简易复习/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>《数据挖掘》课程简易复习提纲，主要根据PPT整理。时间仓促，不排除存在部分内容爆炸。<br><a id="more"></a></p>
<h1 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h1><p>Data Mining: process of semi-automaticlly analyzing large databases to find patterns that are:</p>
<ol>
<li>Valid: hold on new data with certainty</li>
<li>Novel: non-obvious to the system</li>
<li>Useful: should be possible to act on the item</li>
<li>Understandable: humans should be able to interpret the pattern</li>
</ol>
<h1 id="Data-Exploration"><a href="#Data-Exploration" class="headerlink" title="Data Exploration"></a>Data Exploration</h1><p>数据具有属性(attribute/feature)，属性可以分为Norminal、Binary、Ordinal或者Numeric的，也可以分为连续的或离散的。</p>
<h2 id="度量数据"><a href="#度量数据" class="headerlink" title="度量数据"></a>度量数据</h2><h3 id="度量中心"><a href="#度量中心" class="headerlink" title="度量中心"></a>度量中心</h3><p>度量中心的方式包括Mean平均数，Medium中位数和众数Mode。根据这三种数的位置关系可以分为symmetric和asymmetric(Positively skew和Negatively skew)两种</p>
<h3 id="度量离散程度"><a href="#度量离散程度" class="headerlink" title="度量离散程度"></a>度量离散程度</h3><p>Quantile，常用的有四分位数，把Q3-Q1定义为IQR(Interquartile range)。<br>在作业中还提到了absolute deviation，表示到中心点（median、mean、mode）的度量，等于<br>$$<br>\frac{1}{n} \sum^{n}_{i = 1}{|x_i - m(X)|}<br>$$</p>
<h2 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h2><p>包括直方图、Box Plots、散点图、康托图、Parallel Coordinates、Star Plots、Chernoff Faces</p>
<h3 id="Box-Plots"><a href="#Box-Plots" class="headerlink" title="Box Plots"></a>Box Plots</h3><p>包含Upper/Lower Extreme、Upper/Lower Quartile、Median、Outlier和Whicker。Outlier离群点在后面会有专门一章讨论</p>
<h3 id="Parallel-Coordinates"><a href="#Parallel-Coordinates" class="headerlink" title="Parallel Coordinates"></a>Parallel Coordinates</h3><p>平行坐标将高维数据映射到一个N个纵坐标轴的折线，用于高维数据。类似的有Star Plots。</p>
<h2 id="Dissimilarity-matrix"><a href="#Dissimilarity-matrix" class="headerlink" title="Dissimilarity matrix"></a>Dissimilarity matrix</h2><p>相异度矩阵</p>
<h3 id="对Nominal而言"><a href="#对Nominal而言" class="headerlink" title="对Nominal而言"></a>对Nominal而言</h3><p>$$<br>d(i, j) = \frac{total - match}{total}<br>$$</p>
<h3 id="对Binary而言"><a href="#对Binary而言" class="headerlink" title="对Binary而言"></a>对Binary而言</h3><p>令$q = (1, 1), r = (1, 0), s = (0, 1), t = (0, 0)$，分别表示对象$A$、$B$中属性值出现的四种对应情况的计数。</p>
<p>$$<br>dissimilarity = \frac{r + s}{total} \\<br>asymmetricBinaryDissimilarity = \frac{r + s}{q + r + s} \\<br>Jaccard = 1 - asymmetricBinaryDissimilarity<br>$$<br>原因是$(1, 1)$和$(0, 0)$有时不能同等看待，例如有时$(1, 1)$非常罕见。</p>
<h3 id="对Numeric而言"><a href="#对Numeric而言" class="headerlink" title="对Numeric而言"></a>对Numeric而言</h3><p>曼哈顿距离（L1范数）和欧几里得距离（L2范数）分别是k为1和2的闵可夫斯基距离。$k \to \infty$是切比雪夫距离。</p>
<h3 id="对Ordinal而言"><a href="#对Ordinal而言" class="headerlink" title="对Ordinal而言"></a>对Ordinal而言</h3><p>首先进行排序，得到$x_{i, feature}$对应的排行$rank_{i, feature}$（从1开始），然后把排行归一化。<br>$$<br>z_{i, feature} = \frac{rank_{i, feature} - 1}{total_{feature} - 1}<br>$$</p>
<h3 id="混合属性"><a href="#混合属性" class="headerlink" title="混合属性"></a>混合属性</h3><p>$$<br>d(i, j) = \frac{\sum_{feature}{exsit_{i, j} d_{i, j}}}{\sum_{feature}{exsit_{i, j}}}<br>$$<br>其中$d_{i, j}$：<br>对Numeric是，相当于做个归一化<br>$$<br>\frac{|x_{i, feature} - x_{j, feature}|}{max_h(h, feature) - min_h(h, feature)}<br>$$<br>对Nominal是根据$x_{i, feature}$和$x_{j, feature}$是否相等取0或者1<br>对于Ordinal是先根据排名进行标准化，再按照Numeric的方法计算</p>
<h1 id="Data-Preprocessing"><a href="#Data-Preprocessing" class="headerlink" title="Data Preprocessing"></a>Data Preprocessing</h1><p>data cleaning、data integration、data reduction和data transformation是数据预处理的基本任务。<br>数据清洗包括缺失值、噪声、离群点和不一致问题的处理。数据集成包括对多种来源数据进行合并（实体识别），并处理其中的非一致和冗余数据。数据归约包含对维数和数量的归约。数据变换包括归一化、离散化和Concept Hierarchy Generation。</p>
<h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><h3 id="处理缺失值"><a href="#处理缺失值" class="headerlink" title="处理缺失值"></a>处理缺失值</h3><p>忽略、人工填充、常数、填入<strong>全体</strong>中位数或均值、填入<strong>同类</strong>的中位数或均值、使用最可能的值填充（回归、贝叶斯算法等）。</p>
<h3 id="处理噪声"><a href="#处理噪声" class="headerlink" title="处理噪声"></a>处理噪声</h3><p>根据分箱(binning)、回归、聚类（检测离群点）<br>分箱要求首先对数据排序，再划分为等频的箱，然后选择使用均值光滑（用均值替换箱中所有点）、边界光滑（用离自己最近的边界值替换）、中位数光滑等。<br>这些光滑噪声的手段常也被用作离散化和数据归约。</p>
<h2 id="数据集成"><a href="#数据集成" class="headerlink" title="数据集成"></a>数据集成</h2><p>可以通过相关性分析来检查数据冗余，例如对Nominal数据有$\chi^2$检验，对于Numeric数据有协方差和相关系数</p>
<h3 id="chi-2-检验"><a href="#chi-2-检验" class="headerlink" title="$\chi^2$检验"></a>$\chi^2$检验</h3><p>卡方检验可以描述两个向量$A[1..m]$和$B[1..n]$的相似程度。<br>卡方检验的第一步是有一个数据集矩阵$M$，$M$中的元素$M_{ij}$，表示同时满足$A_i$和$B_i$性质的样本个数<br>将矩阵填入一张表中，在表的最右端和最下端扩充一列/行，用来记录对应行/列的总和</p>
<p>首先计算$e_{ij}$，也就是对应的列和乘以行和除以总数<br>$$<br>e_{ij} = \frac{sum(B = b_i) * sum(A = a_j)}{sum(all)}<br>$$</p>
<p>将它填在括号里面，再对于矩阵中所有的单元格计算</p>
<p>$$<br>\chi^2 = \sum_{m}{ \sum_{n}{ \frac{(x_{ij} - e_{ij})^2}{e_{ij}} } }<br>$$</p>
<h3 id="皮尔逊相关系数"><a href="#皮尔逊相关系数" class="headerlink" title="皮尔逊相关系数"></a>皮尔逊相关系数</h3><p>协方差可以衡量两个变量的总体误差，方差可以看做两个变量相同时协方差的特例<br>对于实数$X$、$Y$，定义协方差为<br>$$<br>Cov(X, Y) = E((X - \mathbb{E}(X))(Y -\mathbb{E}(Y)))<br>$$<br>对于向量$A$、$B$，定义协方差为</p>
<p>$$<br>Cov(A, B) = E((A - \bar{A})(B - \bar{B})) = \frac{\sum_{i = 1}^{n}{(A_i - \bar{A})(B_i - \bar{B})}}{n} = E(A * B) - \bar{A} \bar{B}<br>$$</p>
<p>皮尔逊相关系数为</p>
<p>$$<br>r_{A, B} = \frac{Cov(A, B)}{\sigma_A \sigma_B}<br>$$</p>
<p>注意这里分母标准差和分子期望中的$n$被约掉了，取值范围为$[-1, 1]$，等于0时变量独立，大于0时正相关，小于0负相关。<br>注意相关性不暗示因果性。</p>
<h2 id="数据归约"><a href="#数据归约" class="headerlink" title="数据归约"></a>数据归约</h2><h3 id="小波分析"><a href="#小波分析" class="headerlink" title="小波分析"></a>小波分析</h3><h3 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h3><p>选取的主成分对应的坐标轴应当追求方差尽可能大</p>
<ol>
<li>规范化数据</li>
<li>计算$k$个<strong>正交</strong>向量，即主成分。该过程可以对协方差矩阵求特征值和特征矩阵。</li>
<li>取出对应特征值最大的<strong>k</strong>个向量</li>
</ol>
<h3 id="标准化方法"><a href="#标准化方法" class="headerlink" title="标准化方法"></a>标准化方法</h3><ol>
<li>min-max：<br> $$<br> v’_i = \frac{v_i - min_A}{max_A - min_A} (newMax_A - newMin_A) + newMin_A<br> $$</li>
<li>Z-score：<br> 这里$\bar{A}$也可以替换成medoid或者mode等中心度量值，$\sigma_A$也可替换成absolute deviation$s$等离散程度度量值<br> $$<br> v’_i = \frac{v_i - \bar{A}}{\sigma_A}<br> $$</li>
<li>demical scaling：<br> $$<br> v’_i = \frac{v_i}{10^j} \quad where \\<br> \underset{j}{\mathrm{argmin}}(max(|v’_i|) &lt; 1)<br> $$</li>
</ol>
<h1 id="Frequent-Itemsets"><a href="#Frequent-Itemsets" class="headerlink" title="Frequent Itemsets"></a>Frequent Itemsets</h1><p>Frequent Pattterns are patterns that appear frequently in a dataset<br>support：$A$和$B$同时出现的概率$p(A \cap B)$，可以是绝对的即出现次数，可以是相对的，再除以事务数<br>confidence：$p(B|A)$ = $c(A \cap B) / c(A)$<br>频繁项集指的是支持度满足最小支持度阈值$min_{sup}$的项集<br>闭频繁项集：$X$的任意超集$Y$的支持度不等于（小于）$X$的支持度<br>极大频繁项集：$X$是频繁的，并且没有任何$X$的超集$Y$是频繁的</p>
<p>下面使用Apriori算法和FP-Growth算法来发现频繁$k$项集，注意我们不关注频繁$k+1$项集，尽管在过程中我们可以作为一个子问题得到它。</p>
<h2 id="Apriori算法"><a href="#Apriori算法" class="headerlink" title="Apriori算法"></a>Apriori算法</h2><p>Apriori算法基于Apriori性质：频繁项集的子集一定是频繁的</p>
<h3 id="使用Apriori生成频繁项集"><a href="#使用Apriori生成频繁项集" class="headerlink" title="使用Apriori生成频繁项集"></a>使用Apriori生成频繁项集</h3><h3 id="使用频繁项集生成关联项集"><a href="#使用频繁项集生成关联项集" class="headerlink" title="使用频繁项集生成关联项集"></a>使用频繁项集生成关联项集</h3><ol>
<li>对于所有的频繁项集$l$，生成$l$所有的非空子集</li>
<li>对于$l$的每个非空子集$s$，输出规则$s \rightarrow (l - s)$，如果满足$p(l|s) \ge min\_conf$</li>
</ol>
<h3 id="Apriori优化方法"><a href="#Apriori优化方法" class="headerlink" title="Apriori优化方法"></a>Apriori优化方法</h3><ol>
<li>Hash-based itemset counting：hash到多个桶里进行初步删选</li>
<li>Transaction reduction：删去不包含任何$k$项集的事务</li>
<li>Partitioning：任何在DB中可能频繁的项集，至少在一个DB分区中是频繁的<br> 这个性质很有意思，假设将数据集$D$分成了$n$个不重叠的部分$D_1, .. D_n$，下面证明任何在$D$中频繁（相对最小支持度为$s$）的项至少在$D$的一个部分$D_i$中频繁。<br> 使用反证法证明。设有一个项集$x$在$D$中频繁。则有<br> $$<br> support\_count(x \in D) / |D| \ge s<br> $$<br> 而$x$在$D_1, .. D_n$都不频繁，则有<br> $$<br> \frac{ support\_count(x \in D_i) }{ |D_i| } \lt s<br> $$<br> 有<br> $$<br> \sum_{i = 1}^{n}{ support\_count(x \in D_i) } \lt s \, \sum_{i = 1}^{n}{ |D_i| }<br> $$<br> 即<br> $$<br> \sum_{ i = 1 }^{ n } { \frac{ support\_count(x \in D_i) }{ |D| }  }  \lt s<br> $$<br> 上面的式子就是<br> $$<br> n * support\_count(x \in D) / |D| &lt; n * s<br> $$<br> 与假设矛盾</li>
<li>Sampling：在给定数据集的子集中挖掘，降低支持度要求，并采用一些方法确定其完整性</li>
<li>Dynamic itemset couting：</li>
</ol>
<h2 id="FP-Growth"><a href="#FP-Growth" class="headerlink" title="FP-Growth"></a>FP-Growth</h2><h3 id="利用FP-Tree生成频繁项集"><a href="#利用FP-Tree生成频繁项集" class="headerlink" title="利用FP-Tree生成频繁项集"></a>利用FP-Tree生成频繁项集</h3><p>在建成FP-Tree后，从后往前生成频繁项集。以《数据挖掘：概念与技术(第3版)》为例，考虑最后的节点$I5$：</p>
<ol>
<li><p>构造$I5$的条件模式基(prefix path sub-tree ending in $e$)<br> 首先从$I5$的所有节点（可以是叶子节点也可以是内部节点，这里$I5$就都是叶子节点，而$I1$就有一个内部节点）往树根找到一条树链，将树链上的所有节点的支持度改为等于叶子节点$I5$的支持度</p>
 <figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;I2, I1, I5 : 1&gt;</span><br><span class="line">&lt;I2, I1, I3, I5 : 1&gt;</span><br></pre></td></tr></table></figure>
<p> 接着提取出前缀路径，即条件模式基</p>
 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;I2, I1: 1&gt;</span></span><br><span class="line"><span class="section">&lt;I2, I1, I3 : 1&gt;</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="2">
<li><p>使用条件模式基构造条件FP树<br> $I5$的条件FP树可以看做是原事务集中含有$I5$的所有项组成的集合，然后把里面的$I5$全部去掉<br> 因此可以使用和构造FP树相同的办法</p>
 <figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">&lt;I2, I1: 1&gt;</span></span><br></pre></td></tr></table></figure>
<p> 注意此时$I3$被去掉了，因为它在条件FP树上的的支持度为1，不满足2的最小支持度。实际上指的是$(I2, I3, I5)$这个项集的支持度不足。</p>
</li>
</ol>
<ol start="3">
<li>查看$I5$是否是频繁项集（可以由简单计数得到）</li>
<li>如果$I5$是频繁的，找到所有以$I5$结尾的频繁项集</li>
</ol>
<h1 id="Basic-Classification"><a href="#Basic-Classification" class="headerlink" title="Basic Classification"></a>Basic Classification</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>训练集、测试集</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树分为树枝，表示对特征测试的结果，由此产生子节点。子节点分为内部节点和叶子节点，叶子节点展示所属类的标签，内部节点展示被测试的特征<br>决策树不需要领域知识和数值参数，能够处理多维数据，学习结果直观<br>在设计决策树算法时需要考虑：如何确定特征测试条件（二类划分还是多类划分等）从而获得最佳切分（考量homogeneous/purity），什么时候可以终止切分（同类、早停）<br>如果决策树的一个节点下只有一个类，这个节点就是pure的</p>
<h3 id="节点的信息熵"><a href="#节点的信息熵" class="headerlink" title="节点的信息熵"></a>节点的信息熵</h3><p>假设数据集$D$可被分为类$[1..n]$，定义节点$t$下的数据属于类$j$的条件概率是$p(j|t)$，即<br>$$<br>P(X = j) = p(j|t)<br>$$<br>由此可以定义信息熵为<br>$$<br>info(t) = entropy(t) = H(t) = -\sum_{j = 1}^{n}{p{(j|t)}\,log\,p(j|t)}<br>$$</p>
<p>信息熵能够描述节点的homogeneity，当熵最大（$=log\,n$）时意味着随机性越大，信息也就越少；当熵最小（=0）则相反。<br>所有数据从根节点可以通过所属的类分为$n$部分，这种关于类别的信息熵称为数据集的经验熵，如果不利用特征建立决策树，我们只能根据经验熵得到一个数据所属类别的概率。</p>
<p>$$<br>H_{j}(D) = - p(j)\,log\,p(j) \\<br>H(D) = \sum_{j = 1}^{n}{ H_{j}(D) }<br>$$</p>
<p>也可以定义关于特征$A$的信息熵，其中特征$A$取值为$[1..n_A]$</p>
<p>$$<br>H_{A}(D) = \sum_{j = 1}^{n_A}{ H_{j}(D) }<br>$$</p>
<h3 id="信息增益、信息增益比和基尼指数"><a href="#信息增益、信息增益比和基尼指数" class="headerlink" title="信息增益、信息增益比和基尼指数"></a>信息增益、信息增益比和基尼指数</h3><p>决策树生成是一个递归的过程，通过测试父节点$t$的特征$A$，将其划分为$k$个子节点，其得到的经验条件熵要小于等于父节点集合的经验熵，产生的差值为信息增益。这是符合直觉的，特征$A$的信息应当能够减少分类的不确定程度。其中$n_i$表示特征$A$取值为第$i$种时对应的数据集大小，也可写作$|D_i|$，显然$D_1 + D_2 + .. + D_n$</p>
<p>$$<br>Gain_{split} = H(t) - \sum_{i = 1}^{k}{\frac{n_i}{n} H(i)}<br>$$</p>
<p>不过信息增益容易将结果分为很多个小类，因此提出了信息增益比的概念</p>
<p>$$<br>GainRatio_{split} = \frac{Gain_{split}}{ H_{A}(D) }<br>$$</p>
<p>基尼指数是用来度量impurity的，例如在节点$t$上。基尼系数熵之半的函数图像差别不大，但是计算要简单很多，所以很常用</p>
<p>$$<br>Gini = 1 - \sum_{j = 1}^k{p(j|t)^2}<br>$$</p>
<p>使用基尼系数分类具有相似的规则，同样是为了使得“增益最大”，所以可以求下面式子的argmax</p>
<p>$$<br>Gini_{split} = Gini(t) - \sum_{i = 1}^{k}{\frac{n_i}{n} Gini(i)}<br>$$<br>但通常而言是直接求argmin</p>
<p>$$<br>Gini_{split} = \sum_{i = 1}^{k}{\frac{n_i}{n} Gini(i)}<br>$$</p>
<p>当数据是均匀分布在所有类上时，基尼系数取最大值$1-1/n$，当数据值集中在一个类上时（更为有趣的信息），基尼系数取最小值0。</p>
<h3 id="决策树训练"><a href="#决策树训练" class="headerlink" title="决策树训练"></a>决策树训练</h3><p>由周志华教授的《机器学习》P77，当信息增益出现平票时可以任意选择。此外还有一种情况，当最后没有其他属性，只能进行多数表决时，出现平票也是随便选。</p>
<h3 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h3><p>欠拟合表现为模型太简单，训练集误差(Re-substitution errors)和泛化误差(Generalization errors)都很大。<br>过拟合表现为模型对新数据的泛化能力较弱，模型过于复杂。<br>为了估算泛化误差，可以使用Reduced error pruning方法，即使用验证集来估算。<br>奥卡姆剃刀法则，复杂的模型有更大的几率是拟合了数据里面的误差，因此在选择模型时应当考虑模型复杂度。<br>常用的解决过拟合的办法有早停策略，对于决策树来说可以在决策树彻底生成前停止算法，即预剪枝；还可以在决策树生成后进行处理，例如后剪枝。</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><h2 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h2><p>朴素贝叶斯算法相对决策树和部分神经网络算法要快，对于大数据更准确，基于贝叶斯定理<br>$$<br>P(H|X) = \frac{P(X, H)}{P(X)} = \frac{P(X|H) P(H)}{P(X)}<br>$$</p>
<p>其中$X = (x_1, …, x_n)$是数据集中的一个向量，$H$是有关分类的假设，例如$P(C_i|X), i \in [1, m]$表示$X$为类$C_i$的概率。$P(H|X)$是后验概率，$P(H)$是先验概率。<br>对于未知向量$X$，朴素贝叶斯算法要求找到最大化$P(C_i|X)$的类$C_i$<br>$$<br>P(C_i|X) = \frac{P(X|C_i) P(C_i)}{P(X)} \quad for \, i \in [1, m]<br>$$<br>其中$P(C_i)$来自先验假设或根据数量统计得到，$P(X)$也是先验的，是个常数，所以有的时候并不考虑这个分母。<br>在Class conditional independence假设下，$P(X|C_i)$计算变得简单很多，只需要对$X$中的每个属性$x_k$分别计算即可<br>$$<br>P(X|C_i) = \prod_{k = 1}^{n}{P(x_k | C_i)}<br>$$<br>例如计算$P([rain, hot, high] | Yes)$，就可以计算<br>$$<br>P(rain | Yes) \, P(hot | Yes) \, P(high | Yes)<br>$$<br>然后可以得到<br>$$<br>P(Yes | [rain, hot, high]) \propto (P(rain | Yes) \, P(hot | Yes) \, P(high | Yes)) P(Yes)<br>$$</p>
<h2 id="分类评估"><a href="#分类评估" class="headerlink" title="分类评估"></a>分类评估</h2><p>我们定义$TP$、$TN$、$FP$、$FN$分别为真正例、真反例、假正例、假反例。这里$T$、$F$表示预测和真实值是否相同，$P$、$N$表示我们的预测结果是正例还是反例。令$U = TP + TN + FP + FN$为样例总数<br>由此可以派生出一系列评价指标：<br>accuracy/recognition rate是所有的$T$比上总数$U$；error rate是所有的$F$比上总数$U$。衡量了整个分类器在正反例上的准确度。<br>查准率precision是$TP$比上预测结果中所有的$P$($ = TP + FP$)，也就是所有预测的正例中正确的比率。<br>查全率、敏感度recall为$TP$比上真实情况下所有的$P$($ = TP + FN$)，也就是所有正例中被预测出的比率<br>specificity是$TN$比上真实情况下所有的$N$，也就是所有反例中被预测出的比率<br>$F_1$值为precision和recall的调和平均</p>
<h2 id="Holdout方法"><a href="#Holdout方法" class="headerlink" title="Holdout方法"></a>Holdout方法</h2><p>包括交叉验证和留一法</p>
<h1 id="Alternative-Classification"><a href="#Alternative-Classification" class="headerlink" title="Alternative Classification"></a>Alternative Classification</h1><h2 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h2><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h2 id="Lazy-Learning"><a href="#Lazy-Learning" class="headerlink" title="Lazy Learning"></a>Lazy Learning</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h1 id="Basic-Clustering"><a href="#Basic-Clustering" class="headerlink" title="Basic Clustering"></a>Basic Clustering</h1><p>聚类分析包含Partitioning、Hierarchical、Density-based和Grid-based方法。</p>
<h2 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h2><p>k-means的目的是最小化簇内方差<br>$$<br>E = \sum_{i = 1}^{k}{ \sum_{p \in C_i}{EuclideanDistance(p, c_i)^2} }<br>$$<br>这个问题是NP难的，k-means使用的贪心的方法不保证最优解，其步骤是：</p>
<ol>
<li>选择$k$个簇的中心</li>
<li>将数据点分配到距离最近的中心对应的簇</li>
<li>使用每个簇中点的均值更新中心</li>
<li>重复2-3直到收敛</li>
</ol>
<h3 id="初始值确定"><a href="#初始值确定" class="headerlink" title="初始值确定"></a>初始值确定</h3><p>一般可以取$k = \sqrt{n/2}$，或者使用Elbow method<br>$k$值确定后可以使用Sampling或Pick “dispersed” set of points（选择到已选点最小距离最大的点）方法来取出$k$个中心点</p>
<p>由此可以看出k-means方法对初始化很敏感。并且有的数据是没有定义均值的，这时候可以选择使用k-modes。此外k-means对<strong>离群点</strong>和<strong>噪音</strong>和敏感。</p>
<p>对于大数据集，可以采用采样、micro-clusters、additional data structure来实现scalability</p>
<h2 id="k-medoids和PAM方法"><a href="#k-medoids和PAM方法" class="headerlink" title="k-medoids和PAM方法"></a>k-medoids和PAM方法</h2><p>由于k-means对噪声很敏感，所以引入了k-medoids，它并不是用均值，而是用数据集中的一个代表点来表示集群的中心</p>
<p>$$<br>E = \sum_{i = 1}^{k}{ \sum_{p \in C_i}{EuclideanDistance(p, o_i)^2} }<br>$$<br>如上式，$o_i$是簇$C_i$的代表点。该算法流程如下：</p>
<ol>
<li>选取$k$个代表点</li>
<li>尝试使用非代表对象$o_{random}$替换代表点$o_1 .. o_k$，假设正在替换某代表点$o_j$，则更新其代价函数</li>
<li>对于所有的对象重新进行分配，并计算交换总代价。如果总代价小于0，则接受这次替换，否则维持$o_j$不变</li>
</ol>
<p>对于大数据可使用CLARA、CLARANS等方法</p>
<h2 id="agglomerative-clustering"><a href="#agglomerative-clustering" class="headerlink" title="agglomerative clustering"></a>agglomerative clustering</h2><p>dendrogram图，类似一个从底部构建的二叉树。<br>Hierarchical Clustering的优点是不需要预测簇的数量，并且往往能和一些分类学的知识建立联系。<br>agglomerative方法是自底而上地不断merge，divisive方法是自定而上的不断split<br>基础的算法应用一个proximity matrix描述两个簇之间的相似度</p>
<p>agglomerative的方法也有缺陷，例如簇之间merge的结果不能取消，没有像kmeans一样针对目标函数优化，三种簇间距离的度量方法各有缺陷</p>
<h3 id="度量两个簇之间的距离"><a href="#度量两个簇之间的距离" class="headerlink" title="度量两个簇之间的距离"></a>度量两个簇之间的距离</h3><ol>
<li>两簇间最相近的两个元素的距离：对噪声和离群点敏感</li>
<li>两簇间最相异的两个元素的距离：减少了集群半径的增加，容易打破大的集群</li>
<li>每个点对间距离的平均：对噪声和离群点不那么敏感，Biased towards globular clusters </li>
</ol>
<h2 id="divisive-clustering和最小生成树"><a href="#divisive-clustering和最小生成树" class="headerlink" title="divisive clustering和最小生成树"></a>divisive clustering和最小生成树</h2><h2 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h2><p>DBSCAN是基于密度的方法，将所有的点分为三类</p>
<ol>
<li>Core<br> 在集群中，且neighborhood是dense的（Eps-MinPts条件）</li>
<li>Border<br> 在集群中，但neighborhood不dense</li>
<li>Outlier<br> 不在集群中</li>
</ol>
<p>可以做出下面的讨论</p>
<ol>
<li>从$q$直接密度可达$p$<br> 当$p$在$q$的Eps-neighborhood内，并且$q$是一个Core</li>
<li>从$q$密度可达$p$<br> 存在对象链$p_1, .. , p_{n-1}, p$，其中$p_{i+1}$直接密度可达$p_i$</li>
<li>$p$和$q$密度连接<br> 存在一点$o$从$p$和$q$都密度可达</li>
</ol>
<p>因此算法流程如下：</p>
<ol>
<li>标记所有节点为未读</li>
<li>选取随机未读节点$p$并标记<ol>
<li>如果$p$是一个Core，则包含所有密度可达$p$点的点$p’$建立一个新的簇<br> 如果$p’$未访问，把$p’$加入簇，并递归。<br> 如果$p’$已访问，但不属于任何簇（之前被标记为噪音了），把$p’$加入簇。<br> 通过以上的两点可以保证点$p’$如果在Eps-neighborhood内有一个Core点$q$，那么即使$p’$本身不是Core，在一开始被划为Noise，最后也能正确地被分到对应的簇中。</li>
<li><strong>否则标记$p$为噪声</strong></li>
</ol>
</li>
</ol>
<p>DBSCAN对参数取值敏感</p>
<h2 id="OPTICS"><a href="#OPTICS" class="headerlink" title="OPTICS"></a>OPTICS</h2><h2 id="聚类评估"><a href="#聚类评估" class="headerlink" title="聚类评估"></a>聚类评估</h2><h3 id="Clustering-Tendency"><a href="#Clustering-Tendency" class="headerlink" title="Clustering Tendency"></a>Clustering Tendency</h3><p>数据集是否根据一个均匀分布产生的<br>Hopkin Statistics</p>
<h3 id="Cluster-Quality"><a href="#Cluster-Quality" class="headerlink" title="Cluster Quality"></a>Cluster Quality</h3><h4 id="外在方法"><a href="#外在方法" class="headerlink" title="外在方法"></a>外在方法</h4><p>外在方法通过把聚类(cluster, $C$)和基本事实(catagory, $L$)比较</p>
<ol>
<li>Cluster Homogeneity：簇的纯度</li>
<li>Cluster Completeness：如果在基于基本事实，两个对象属于同一catagory，那么他们应当属于同一个簇</li>
<li>Rag bag：翻译叫碎布袋，即一个异类的对象最好放入碎布袋中，而不是放入纯的簇中</li>
<li>Small cluster preservation：将小catagory再分成碎片是非常有害的，因为它使得这些小簇可能成为噪声</li>
<li>BCubed precision：<br> Correctness：等于1如果$L(o_i) = L(O_j) \Leftrightarrow C(o_i) = C(O_j)$<br> BCubed recall<br> BCubed precision</li>
</ol>
<h4 id="内在方法"><a href="#内在方法" class="headerlink" title="内在方法"></a>内在方法</h4><p>内在方法通过比较簇之间分离的优劣<br>轮廓系数Silhouette coefficient，定义$a(o)$为对象$o$到所属簇中其他对象之间的平均距离，定义$b(o)$为$o$到$o$不属于的所有簇的最小平均距离。则<br>$$<br>s(o) = \frac{b(o) - a(o)}{max(a(o), b(o))}<br>$$</p>
<h1 id="Alternative-Clustering"><a href="#Alternative-Clustering" class="headerlink" title="Alternative Clustering"></a>Alternative Clustering</h1><h2 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h2><p>在Fuzzy clusters和Probabilitic-model based clustering中，一个对象可以属于多个簇，按照对应的概率。<br>高斯混合分布属于生成模型，它可以描述数据是如何从模型中产生的。</p>
<h3 id="EM算法：E步骤"><a href="#EM算法：E步骤" class="headerlink" title="EM算法：E步骤"></a>EM算法：E步骤</h3><h3 id="EM算法：M步骤"><a href="#EM算法：M步骤" class="headerlink" title="EM算法：M步骤"></a>EM算法：M步骤</h3><h2 id="Biclustering"><a href="#Biclustering" class="headerlink" title="Biclustering"></a>Biclustering</h2><p>对于高维数据，传统的基于距离的方法，例如基于欧几里得距离的方法是不可信的，容易被多个维度的噪音所掩盖。因此高维数据的聚类常常是用一小组属性来定义的。<br>常见有两种高维聚类的方式，第一类是Subspace clustering methods，在高维数据的一个子空间里面搜索聚类，常见的有Biclustering；第二类是Dimensionality reduction approaches，构造一个更低维数的空间，并且在那个空间里面搜索聚类，例如Spectral Clustering</p>
<h2 id="Clustering-with-constraints"><a href="#Clustering-with-constraints" class="headerlink" title="Clustering with constraints"></a>Clustering with constraints</h2><h1 id="Outlier-Analysis"><a href="#Outlier-Analysis" class="headerlink" title="Outlier Analysis"></a>Outlier Analysis</h1><p>离群点，相对于normal/expected data，是指距离其他对象显著远的对象。离群点不是噪音，噪音是没有研究价值的，类似于<strong>random</strong> error或者variance<br>离群点分为全局离群点（相对于其余数据）、情景离群点（相对于某些特定context的数据）和集体离群点。对集体离群点来说，里面的点单个考虑可能就不是离群点了。</p>
<h2 id="统计学方法"><a href="#统计学方法" class="headerlink" title="统计学方法"></a>统计学方法</h2><h3 id="参数法之Univariate-Outlier-Detection"><a href="#参数法之Univariate-Outlier-Detection" class="headerlink" title="参数法之Univariate Outlier Detection"></a>参数法之Univariate Outlier Detection</h3><p>Univariate Outlier Detection假设数据仅对一个指标服从正态分布。然后就可以使用3$\sigma$原则来检测一维离群点了（不过为啥要用极大似然估计呢）。<br>还可以使用之前学过的box plot来进行可视化估计，认为在$Q1 - 1.5 \, IQR$以下和$Q3 + 1.5 \, IQR$要上的点都是离群点<br>还可以使用Grubb’s检验（最大标准残差检验），与z-score有关</p>
<h3 id="参数法之Multivariate-Outlier-Detection"><a href="#参数法之Multivariate-Outlier-Detection" class="headerlink" title="参数法之Multivariate Outlier Detection"></a>参数法之Multivariate Outlier Detection</h3><p>Mahalanobis距离方法<br>$\chi^2$统计量</p>
<h3 id="参数法之混合参数分布"><a href="#参数法之混合参数分布" class="headerlink" title="参数法之混合参数分布"></a>参数法之混合参数分布</h3><h3 id="非参数法之直方图"><a href="#非参数法之直方图" class="headerlink" title="非参数法之直方图"></a>非参数法之直方图</h3><h3 id="非参数法之Kernel-Density-Estimation-KDE"><a href="#非参数法之Kernel-Density-Estimation-KDE" class="headerlink" title="非参数法之Kernel Density Estimation(KDE)"></a>非参数法之Kernel Density Estimation(KDE)</h3><h2 id="基于邻近性的方法"><a href="#基于邻近性的方法" class="headerlink" title="基于邻近性的方法"></a>基于邻近性的方法</h2><h3 id="距离方法"><a href="#距离方法" class="headerlink" title="距离方法"></a>距离方法</h3><h3 id="网格方法"><a href="#网格方法" class="headerlink" title="网格方法"></a>网格方法</h3><h3 id="密度方法"><a href="#密度方法" class="headerlink" title="密度方法"></a>密度方法</h3><h1 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h1><p>Cluster Computing架构：backbone between racks和rack between nodes<br>MapReduce是一种批处理算法，核心思想是Bring computation to data和Store files multiple times for reliability。</p>
<h1 id="Mining-Streaming-data"><a href="#Mining-Streaming-data" class="headerlink" title="Mining Streaming data"></a>Mining Streaming data</h1><p>流处理查询主要有两种类型：Ad-hoc查询和Stading查询</p>
<h2 id="流数据取样"><a href="#流数据取样" class="headerlink" title="流数据取样"></a>流数据取样</h2><h2 id="Bloom-Filter"><a href="#Bloom-Filter" class="headerlink" title="Bloom Filter"></a>Bloom Filter</h2><p>我记得之前柳晗就和我讨论过这个算法。<br>布隆过滤器用来检测一个数是否在集合中，对于确定性的方法而言，这通常意味着对$log(n)$的时间复杂度进行常数优化，或者对哈希函数和哈希方法进行优化。但无论如何，空间开销是免不了的。<br>布隆过滤器牺牲了准确性，他可能造成FP假阳性，即可能认为没出现过的数出现过，但换来了空间性能的提高。<br>算法思想很简单，将$n$个输入送给$k$个哈希函数$h_1, .., h_k$，哈希函数的计算结果将依次<strong>按位或</strong>到一个长度为$n$的bitset中，因此这个bitset中的某一位只能从0变成1。<br>例如，假设此时bitset为$b_i$，现对于新输入$x$，判断是否出现过。然后把$x$依次送入$k$个哈希函数，如果$h_j(x) = a$，则将第$a$位设为1。注意到如果此时第$a$位为0，则说明$x$肯定没出现过，但反之不一定成立。</p>
<div style="display:none"><br>例如，假设此时bitset为$b_i$，现对于新输入$x$，判断是否出现过。创建一个新的bitset为$b_{i+1}$，并memset为0，然后把$x$依次送入$k$个哈希函数，将哈希的结果按位或到$b_{i+1}$上。接着比较$b_i \&amp; b_{i+1}$。如果不等于$b_{i+1}$，那么肯定没有出现过。但是如果相等，并不一定就真的出现，可能两个数对$k$个哈希函数的输出都一样。<br></div>

<h3 id="Bloom-Filter算法分析"><a href="#Bloom-Filter算法分析" class="headerlink" title="Bloom Filter算法分析"></a>Bloom Filter算法分析</h3><p>Bloom Filter时间空间复杂度相对于数据规模是常数，因此主要分析出现FP的概率。<br>FP的概率与1的密度有关，显然1越多，越容易出现碰撞，因此可以表示为$a^k$，其中$a$表示此时1占的比例，不过碰撞概率是要略低于这个值的。我们还可以这样理解，把FP的概率近似看做对元素$x_i$哈希后输出的$k$个位置上都是1的概率$p_1$（当然有可能是重复了）。为了方便计算，我们实际考虑截至$x_i$，某一位仍然是0的概率$p_0 = 1 - p_1$。<br>将任意一位从0变为1的概率相当于将$d$和飞镖随机扔向$t$个目标，这里的$d$相当于所有的$n$个输入通过$k$个哈希函数得到的$nk$个结果，$t$相当于bitset。目标$T_i$被指定飞镖集中的概率是$1/t$，因此所有的$d$个飞镖都没有击中目标$T_i$的概率就是$(1 - 1/t)^d$，由于$t$通常很大，可以将其改写为$e^{-d/t}$（重要极限）。</p>
<h2 id="Bit-Counting和DGIM算法"><a href="#Bit-Counting和DGIM算法" class="headerlink" title="Bit Counting和DGIM算法"></a>Bit Counting和DGIM算法</h2><p>对一个01串，在线回答问题最近的$k &lt;= N$位中有多少个1。朴素的方法需要$O(N)$的空间，每次查询需要花费$O(k)$的时间。<br>DGIM算法能够只储存$O(log^2N)$位，在$O(log N)$的时间复杂度内给出一个大概的值，误差在50%以内，并可以进一步缩小到任意eps。<br>算法思路是维护一个容量为$N$的线性队列，从队尾到队头按照1的个数将其划分为$m \approx O(log_2N)$个桶，每个桶中分别有$2^0, 2^1, .., 2^m$个1（0的数量不考虑）。为了方便讨论，定义桶的大小是桶里面1的数目。<br>因此我们容易看出这个线性队列具有下面的性质：</p>
<ol>
<li>每个桶的最右（靠近队尾）端总是1</li>
<li>所有的1都在某个桶中；但是0不一定，可能在桶间</li>
<li>每种大小的桶最多容忍有两个</li>
</ol>
<p>下面使用该线性队列回答开始的问题：<br>通过比较每个桶两端的位置与$k$的大小，找到$k$值所在的桶$b$，累加$b$右边所有桶的大小及桶$b$的一半大小，即为估计值。<br>下面考虑如何维护该线性队列，考虑接受一个新比特时</p>
<ol>
<li>首先弹出队头，并更新队头所属的桶（如果属于某个桶的话），如果此时桶里已经没有1了，就删除这个桶</li>
<li>如果新比特是0，则不做任何处理</li>
<li>如果新比特是1，则从右至左检查是否破坏性质<strong>每种大小的桶最多容忍有两个</strong>，并进行合并处理</li>
</ol>
<h3 id="DGIM算法分析"><a href="#DGIM算法分析" class="headerlink" title="DGIM算法分析"></a>DGIM算法分析</h3><p>首先可以看到我们实际上不要将整个线性表存下来，我们只需要记录每个桶两端的坐标即可，这样的坐标有$O(log_2N)$个。对于每个坐标，他的值域是$[0, n)$，因此我们需要$O(log_2N)$比特来表示它。注意由于$N$很大，所以不能理解成一个坐标用一个$O(1)$的常数空间（例如<code>int</code>）就好。<br>下面分析DGIM误差的上下界</p>
<h2 id="流数据聚类和BDMO算法"><a href="#流数据聚类和BDMO算法" class="headerlink" title="流数据聚类和BDMO算法"></a>流数据聚类和BDMO算法</h2><h1 id="Recommendation-System"><a href="#Recommendation-System" class="headerlink" title="Recommendation System"></a>Recommendation System</h1><p>Content Based方法注意物品的属性，协同过滤方法注意物品与用户的关系</p>
<h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF.IDF"></a>TF.IDF</h2><p>考虑从文档中选择关键词来最好地概括文档，一个简单的想法是希望这个词出现的频率越来越高，这也就是TF。但其实这是不够的，因为很多助词，例如“也”、“是”这些词出现的频率很高，但却不能用来概括文档。这时候我们需要IDF来描述，也就是说一个词如果出现的文档数越多，它就越common，权重就越底。<br>$f_{ij}$为term$t_i$在文档$d_j$中出现的频率<br>定义$TF_{ij} = f_{ij} / max_k(f_{kj})$<br>$n_i$为出现term$t_i$的文档数<br>定义$IDF_i = log \frac{N}{n_i}$<br>由此可以计算得到<br>$TF.IDF = TF_{if} * IDF_i$</p>
<h2 id="Content-Filtering"><a href="#Content-Filtering" class="headerlink" title="Content Filtering"></a>Content Filtering</h2><p>Content Filtering首先对每一个item建立profile，这里的profile指的是一组属性。使用余弦相似度衡量user profile$c$和item profile$s$。<br>$$<br>u(c, s) = cos(c, s) = \frac{c.s}{|c||s|}<br>$$<br>这种方法的缺点是过于专门化，它从来不推荐在user’s content profile外面的item，并且人们可能具有多个兴趣爱好。<br>此外需要有效的办法去查找high utility的items，也就是相似度高的profile，这时候可以借助于Locality Sensitive Hashing。</p>
<h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><p>协同过滤算法的基本思路是对于用户$c$，找到其他用户组$D$和$c$的对所有item的交集给出的评分相近，然后对于新的item，基于$D$的评分估计$c$的评分。<br>设$x$的评分向量为$r_x$，那么$x$，$y$的相似度可以用余弦相似度衡量$cos(r_x, r_y)$，也可以计算$sim$的的皮尔逊相关系数（仅对$x$、$y$都评分项计算）<br>下面使用协同过滤算法进行预测$c$给item$s$的打分。在给item$s$打分的用户里面找出$k$个最接近用户$c$的用户，令为$D$。则<br>$$<br>r_{cs} = \frac{1}{k \sum_{d \, in \, D}{r_{ds}}} \\<br>r_{cs} =  \frac{ \sum_{d \, in \, D}{sim(c, d) \, r_{ds}} }{ \sum_{d \, in \, D}{sim(c, d)} }<br>$$<br>协同过滤算法的计算$D$是比较昂贵的，对于每个用户，需要线性的时间。此时我们可以同样借助Locality Sensitive Hashing。<br>此外可以使用MapReduce来计算协方差矩阵，我记得当时参加第一节云计算大赛的时候的技能题就有一条是这个。<br>刚才的算法是基于用户的，user-user协同过滤，也有item-item协同过滤，对于item$s$，找到相似的items，当然这里还是使用对相似item的rating而不是对用户的rating，因此可以使用相同的矩阵和预测函数。在实践上item-item的常优于user-user的。</p>
<h2 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a>Locality Sensitive Hashing</h2><p>在之前提到的两种算法中提到了Locality Sensitive Hashing这个算法可以用来快速的找到高相似度的向量对（例如各种profile）<br>Locality Sensitive Hashing属于一种Approximate Nearest Neighbor Search方法<br>Locality-sensitive family是一组可以组合起来将向量按相似度区分开的函数。这些函数在统计上是彼此独立的。他们的速度要比遍历所有向量对来得快，并且能够被组合起来解决FP和FN问题。</p>
<p>$(d_1, d_2, p_1, p_2)$敏感表示：</p>
<ol>
<li>如果$d(x, y) \le d1$，则$h(x) = h(y)$的概率至少为$p1$</li>
<li>如果$d(x, y) \ge d2$，则$h(x) = h(y)$的概率至多为$p2$</li>
</ol>
<h3 id="AND、OR-of-Hash-functions"><a href="#AND、OR-of-Hash-functions" class="headerlink" title="AND、OR of Hash functions"></a>AND、OR of Hash functions</h3><p>给定family$H$，从$H$中选择$r$个函数构造family$H’$。对于$H’$中的$h = [h_1, .., h_r]$：</p>
<ol>
<li>$h(x) = h(y)$当且仅当对于任意的$i$都存在$h_i(x) = h_i(y)$<br> 这是AND构造，指的是这$k$个哈希值里面要全部相同，才会被投影到相同的桶内<br> AND操作能够使得保持$p1$较大时$p2$更小，即降低FN<br> 相应的定理是如果$(d_1, d_2, p_1, p_2)$敏感的，那么$H’$是$(d_1, d_2, p_1^r, p_2^r)$敏感的</li>
<li>$h(x) = h(y)$当且仅当对于存在一个以上$i$使得$h_i(x) = h_i(y)$<br> 这是OR构造，指的是这$k$个哈希值里面有一对以上相同，就会被投影到相同的桶内<br> OR操作能够使得$p1$较小时$p2$更大，即降低FP<br> 相应的定理是如果$(d_1, d_2, p_1, p_2)$敏感的，那么$H’$是$(d_1, d_2, 1-(1-p_1)^r, 1-(1-p_2)^r)$敏感的</li>
</ol>
<h3 id="Amplify-LS-family"><a href="#Amplify-LS-family" class="headerlink" title="Amplify LS family"></a>Amplify LS family</h3><h1 id="考试内容2017"><a href="#考试内容2017" class="headerlink" title="考试内容2017"></a>考试内容2017</h1><ol>
<li>列出数据的种类（Nominal等）。解释Mean、Medoid等。</li>
<li>简述缺失值处理方法。简介PCA。</li>
<li>简述支持度等概念。证明Apriori性质。</li>
<li>比较Apriori和FP-Growth的性能。使用Apriori计算频繁项集。</li>
<li>为什么Naive Bayes是Naive的。如何基于Gini系数构建决策树。</li>
<li>使用BP计算神经网络</li>
<li>k-means有哪些缺点。DBScan。什么是dendrogram，如何度量两个簇的距离</li>
<li>协同过滤和内容过滤的区别是什么。什么是$(d_1, d_2, p_1, p_2)$敏感。证明（类似Excecise 2）。</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div></div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/img/fkm/wxfk.jpg" alt="Calvin Neo WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/img/fkm/zfbfk.jpg" alt="Calvin Neo Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/数据挖掘/" rel="tag"># 数据挖掘</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/17/C++-functor/" rel="next" title="C++仿函数的作用实现">
                <i class="fa fa-chevron-left"></i> C++仿函数的作用实现
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/02/UDP-Socket-Programming/" rel="prev" title="UDP套接字编程">
                UDP套接字编程 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/favicon.jpg"
               alt="Calvin Neo" />
          <p class="site-author-name" itemprop="name">Calvin Neo</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">109</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">122</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/CalvinNeo" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/CalvinNeo0" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1568200035" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://xqq.im/" title="xqq" target="_blank">xqq</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.lovelywen.com/" title="wenwen" target="_blank">wenwen</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://smlight.github.io/blog/" title="zyyyyy" target="_blank">zyyyyy</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Intro"><span class="nav-number">1.</span> <span class="nav-text">Intro</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Exploration"><span class="nav-number">2.</span> <span class="nav-text">Data Exploration</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#度量数据"><span class="nav-number">2.1.</span> <span class="nav-text">度量数据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#度量中心"><span class="nav-number">2.1.1.</span> <span class="nav-text">度量中心</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#度量离散程度"><span class="nav-number">2.1.2.</span> <span class="nav-text">度量离散程度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据可视化"><span class="nav-number">2.2.</span> <span class="nav-text">数据可视化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Box-Plots"><span class="nav-number">2.2.1.</span> <span class="nav-text">Box Plots</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Parallel-Coordinates"><span class="nav-number">2.2.2.</span> <span class="nav-text">Parallel Coordinates</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dissimilarity-matrix"><span class="nav-number">2.3.</span> <span class="nav-text">Dissimilarity matrix</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对Nominal而言"><span class="nav-number">2.3.1.</span> <span class="nav-text">对Nominal而言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对Binary而言"><span class="nav-number">2.3.2.</span> <span class="nav-text">对Binary而言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对Numeric而言"><span class="nav-number">2.3.3.</span> <span class="nav-text">对Numeric而言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对Ordinal而言"><span class="nav-number">2.3.4.</span> <span class="nav-text">对Ordinal而言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#混合属性"><span class="nav-number">2.3.5.</span> <span class="nav-text">混合属性</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Data-Preprocessing"><span class="nav-number">3.</span> <span class="nav-text">Data Preprocessing</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据清洗"><span class="nav-number">3.1.</span> <span class="nav-text">数据清洗</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#处理缺失值"><span class="nav-number">3.1.1.</span> <span class="nav-text">处理缺失值</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理噪声"><span class="nav-number">3.1.2.</span> <span class="nav-text">处理噪声</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集成"><span class="nav-number">3.2.</span> <span class="nav-text">数据集成</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#chi-2-检验"><span class="nav-number">3.2.1.</span> <span class="nav-text">$\chi^2$检验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#皮尔逊相关系数"><span class="nav-number">3.2.2.</span> <span class="nav-text">皮尔逊相关系数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据归约"><span class="nav-number">3.3.</span> <span class="nav-text">数据归约</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#小波分析"><span class="nav-number">3.3.1.</span> <span class="nav-text">小波分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#主成分分析"><span class="nav-number">3.3.2.</span> <span class="nav-text">主成分分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#标准化方法"><span class="nav-number">3.3.3.</span> <span class="nav-text">标准化方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Frequent-Itemsets"><span class="nav-number">4.</span> <span class="nav-text">Frequent Itemsets</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Apriori算法"><span class="nav-number">4.1.</span> <span class="nav-text">Apriori算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用Apriori生成频繁项集"><span class="nav-number">4.1.1.</span> <span class="nav-text">使用Apriori生成频繁项集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用频繁项集生成关联项集"><span class="nav-number">4.1.2.</span> <span class="nav-text">使用频繁项集生成关联项集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Apriori优化方法"><span class="nav-number">4.1.3.</span> <span class="nav-text">Apriori优化方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FP-Growth"><span class="nav-number">4.2.</span> <span class="nav-text">FP-Growth</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#利用FP-Tree生成频繁项集"><span class="nav-number">4.2.1.</span> <span class="nav-text">利用FP-Tree生成频繁项集</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-Classification"><span class="nav-number">5.</span> <span class="nav-text">Basic Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本概念"><span class="nav-number">5.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#决策树"><span class="nav-number">5.2.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#节点的信息熵"><span class="nav-number">5.2.1.</span> <span class="nav-text">节点的信息熵</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#信息增益、信息增益比和基尼指数"><span class="nav-number">5.2.2.</span> <span class="nav-text">信息增益、信息增益比和基尼指数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树训练"><span class="nav-number">5.2.3.</span> <span class="nav-text">决策树训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#欠拟合与过拟合"><span class="nav-number">5.2.4.</span> <span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缺失值处理"><span class="nav-number">5.2.5.</span> <span class="nav-text">缺失值处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#贝叶斯分类器"><span class="nav-number">5.3.</span> <span class="nav-text">贝叶斯分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#分类评估"><span class="nav-number">5.4.</span> <span class="nav-text">分类评估</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Holdout方法"><span class="nav-number">5.5.</span> <span class="nav-text">Holdout方法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Alternative-Classification"><span class="nav-number">6.</span> <span class="nav-text">Alternative Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机"><span class="nav-number">6.1.</span> <span class="nav-text">支持向量机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络"><span class="nav-number">6.2.</span> <span class="nav-text">神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lazy-Learning"><span class="nav-number">6.3.</span> <span class="nav-text">Lazy Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#集成学习"><span class="nav-number">6.4.</span> <span class="nav-text">集成学习</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-Clustering"><span class="nav-number">7.</span> <span class="nav-text">Basic Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#k-means"><span class="nav-number">7.1.</span> <span class="nav-text">k-means</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始值确定"><span class="nav-number">7.1.1.</span> <span class="nav-text">初始值确定</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#k-medoids和PAM方法"><span class="nav-number">7.2.</span> <span class="nav-text">k-medoids和PAM方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#agglomerative-clustering"><span class="nav-number">7.3.</span> <span class="nav-text">agglomerative clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#度量两个簇之间的距离"><span class="nav-number">7.3.1.</span> <span class="nav-text">度量两个簇之间的距离</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#divisive-clustering和最小生成树"><span class="nav-number">7.4.</span> <span class="nav-text">divisive clustering和最小生成树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DBSCAN"><span class="nav-number">7.5.</span> <span class="nav-text">DBSCAN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OPTICS"><span class="nav-number">7.6.</span> <span class="nav-text">OPTICS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#聚类评估"><span class="nav-number">7.7.</span> <span class="nav-text">聚类评估</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Clustering-Tendency"><span class="nav-number">7.7.1.</span> <span class="nav-text">Clustering Tendency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cluster-Quality"><span class="nav-number">7.7.2.</span> <span class="nav-text">Cluster Quality</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#外在方法"><span class="nav-number">7.7.2.1.</span> <span class="nav-text">外在方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内在方法"><span class="nav-number">7.7.2.2.</span> <span class="nav-text">内在方法</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Alternative-Clustering"><span class="nav-number">8.</span> <span class="nav-text">Alternative Clustering</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#高斯混合模型"><span class="nav-number">8.1.</span> <span class="nav-text">高斯混合模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法：E步骤"><span class="nav-number">8.1.1.</span> <span class="nav-text">EM算法：E步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法：M步骤"><span class="nav-number">8.1.2.</span> <span class="nav-text">EM算法：M步骤</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Biclustering"><span class="nav-number">8.2.</span> <span class="nav-text">Biclustering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Clustering-with-constraints"><span class="nav-number">8.3.</span> <span class="nav-text">Clustering with constraints</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Outlier-Analysis"><span class="nav-number">9.</span> <span class="nav-text">Outlier Analysis</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#统计学方法"><span class="nav-number">9.1.</span> <span class="nav-text">统计学方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#参数法之Univariate-Outlier-Detection"><span class="nav-number">9.1.1.</span> <span class="nav-text">参数法之Univariate Outlier Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数法之Multivariate-Outlier-Detection"><span class="nav-number">9.1.2.</span> <span class="nav-text">参数法之Multivariate Outlier Detection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数法之混合参数分布"><span class="nav-number">9.1.3.</span> <span class="nav-text">参数法之混合参数分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非参数法之直方图"><span class="nav-number">9.1.4.</span> <span class="nav-text">非参数法之直方图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#非参数法之Kernel-Density-Estimation-KDE"><span class="nav-number">9.1.5.</span> <span class="nav-text">非参数法之Kernel Density Estimation(KDE)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于邻近性的方法"><span class="nav-number">9.2.</span> <span class="nav-text">基于邻近性的方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#距离方法"><span class="nav-number">9.2.1.</span> <span class="nav-text">距离方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网格方法"><span class="nav-number">9.2.2.</span> <span class="nav-text">网格方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#密度方法"><span class="nav-number">9.2.3.</span> <span class="nav-text">密度方法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MapReduce"><span class="nav-number">10.</span> <span class="nav-text">MapReduce</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Mining-Streaming-data"><span class="nav-number">11.</span> <span class="nav-text">Mining Streaming data</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#流数据取样"><span class="nav-number">11.1.</span> <span class="nav-text">流数据取样</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bloom-Filter"><span class="nav-number">11.2.</span> <span class="nav-text">Bloom Filter</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bloom-Filter算法分析"><span class="nav-number">11.2.1.</span> <span class="nav-text">Bloom Filter算法分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bit-Counting和DGIM算法"><span class="nav-number">11.3.</span> <span class="nav-text">Bit Counting和DGIM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#DGIM算法分析"><span class="nav-number">11.3.1.</span> <span class="nav-text">DGIM算法分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#流数据聚类和BDMO算法"><span class="nav-number">11.4.</span> <span class="nav-text">流数据聚类和BDMO算法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Recommendation-System"><span class="nav-number">12.</span> <span class="nav-text">Recommendation System</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TF-IDF"><span class="nav-number">12.1.</span> <span class="nav-text">TF.IDF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Content-Filtering"><span class="nav-number">12.2.</span> <span class="nav-text">Content Filtering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Collaborative-Filtering"><span class="nav-number">12.3.</span> <span class="nav-text">Collaborative Filtering</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Locality-Sensitive-Hashing"><span class="nav-number">12.4.</span> <span class="nav-text">Locality Sensitive Hashing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#AND、OR-of-Hash-functions"><span class="nav-number">12.4.1.</span> <span class="nav-text">AND、OR of Hash functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Amplify-LS-family"><span class="nav-number">12.4.2.</span> <span class="nav-text">Amplify LS family</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#考试内容2017"><span class="nav-number">13.</span> <span class="nav-text">考试内容2017</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Calvin Neo</span>
  <span> &nbsp; Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
</div>
<div>
  <span><a href="/about/yytl/">版权声明</a></span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse 
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://calvinneo.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://www.calvinneo.com/2017/11/21/数据挖掘简易复习/';
          this.page.identifier = '2017/11/21/数据挖掘简易复习/';
          this.page.title = '数据挖掘简易复习';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://calvinneo.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      $('#local-search-input').focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
