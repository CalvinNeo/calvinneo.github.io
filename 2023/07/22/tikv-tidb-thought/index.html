<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>





<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="分布式,事务,数据库," />





  <link rel="alternate" href="/atom.xml" title="Calvin's Marbles" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="一些常见问题的思考，只代表个人见解。对于一些已经沉淀下来的知识，会被挪到专门的文章中讨论。">
<meta name="keywords" content="分布式,事务,数据库">
<meta property="og:type" content="article">
<meta property="og:title" content="关于 TiKV、TiDB、TiFlash 的一些思考">
<meta property="og:url" content="http://www.calvinneo.com/2023/07/22/tikv-tidb-thought/index.html">
<meta property="og:site_name" content="Calvin&#39;s Marbles">
<meta property="og:description" content="一些常见问题的思考，只代表个人见解。对于一些已经沉淀下来的知识，会被挪到专门的文章中讨论。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2025-03-14T05:47:30.102Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="关于 TiKV、TiDB、TiFlash 的一些思考">
<meta name="twitter:description" content="一些常见问题的思考，只代表个人见解。对于一些已经沉淀下来的知识，会被挪到专门的文章中讨论。">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.calvinneo.com/2023/07/22/tikv-tidb-thought/"/>





  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-5487541356791902"
     crossorigin="anonymous"></script>
  <title>关于 TiKV、TiDB、TiFlash 的一些思考 | Calvin's Marbles</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Calvin's Marbles</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://www.calvinneo.com/2023/07/22/tikv-tidb-thought/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Calvin Neo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Calvin's Marbles">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                关于 TiKV、TiDB、TiFlash 的一些思考
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-07-22T23:20:37+08:00">
                2023-07-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>一些常见问题的思考，只代表个人见解。对于一些已经沉淀下来的知识，会被挪到专门的文章中讨论。</p>
<a id="more"></a>

<h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><h2 id="关于-SQL-on-KVStore"><a href="#关于-SQL-on-KVStore" class="headerlink" title="关于 SQL on KVStore"></a>关于 SQL on KVStore</h2><p>TiDB 的架构是在一个分布式的 KVStore 上构建无状态的 SQL 层。</p>
<h1 id="TiKV-相关"><a href="#TiKV-相关" class="headerlink" title="TiKV 相关"></a>TiKV 相关</h1><h2 id="高可用和持久性"><a href="#高可用和持久性" class="headerlink" title="高可用和持久性"></a>高可用和持久性</h2><ul>
<li>Raft 系统高可用 HA 的主要关注点是<strong>选举耗时</strong></li>
<li>Raft Learner 系统高可用 HA 的主要关注点是<strong>重放耗时</strong></li>
<li>Raft 系统的持久性的主要关注点是<strong>迁移速度</strong><br>  三副本挂掉一个之后，如果再挂掉一个，就不能保证持久性了。所以将数据迁移到新扩容出来的节点的速度直接决定了容错能力。</li>
<li>Raft Learner 系统持久性的要求较低，因为始终可以从 Leader 恢复</li>
</ul>
<h2 id="TiKV-写入"><a href="#TiKV-写入" class="headerlink" title="TiKV 写入"></a>TiKV 写入</h2><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a>。</p>
<h3 id="关于-Region-大小的讨论"><a href="#关于-Region-大小的讨论" class="headerlink" title="关于 Region 大小的讨论"></a>关于 Region 大小的讨论</h3><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a>。</p>
<h3 id="日志和数据分离存储"><a href="#日志和数据分离存储" class="headerlink" title="日志和数据分离存储"></a>日志和数据分离存储</h3><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a>。</p>
<h3 id="Raft-存储-–-RaftEngine"><a href="#Raft-存储-–-RaftEngine" class="headerlink" title="Raft 存储 – RaftEngine"></a>Raft 存储 – RaftEngine</h3><p>原来 TiKV 使用 RocksDB 存储 Raft Log 和相关 Meta，存在几个问题：</p>
<ol>
<li>WAL + 实际数据，需要写两次盘，产生写放大。</li>
<li>数据变多，Compaction 负担变大，写放大更大。层数更多，写放大更大。</li>
</ol>
<p>因此引入了类似 bitcask 架构的 RaftEngine 来解决这个问题。RaftEngine 中每个 Region 对应一个 Memtable，数据先通过 Group Write 写入到文件中，然后再注册到 Memtable 中。在读取时从 Memtable 获取位置，再从文件中读取。因此随着 Region 日志 Apply 进度的不同，RaftEngine 在文件中会存在空洞，因此需要 rewrite。这使得存在一部分 CPU 和 IO 花费在 rewrite 逻辑上，而不能像 PolarDB 一样按照水位线直接删除。RaftEngine 这么做可以减少 fsync 的调用频率，并且充分利用文件系统 buffer 来做聚合。</p>
<p>此外，Raftstore 还使用 async_io 来异步落盘 Raft 日志和 Raft 状态。这样，Raftstore 线程不被 io 阻塞，能够处理更多的 Raft 相关请求和日志。需要注意，这反过来可能会加重 PeerFsm、ApplyFsm 和网络的负担，对 CPU 的要求更高。</p>
<h3 id="Why-RockDB？"><a href="#Why-RockDB？" class="headerlink" title="Why RockDB？"></a>Why RockDB？</h3><p><a href="https://cn.pingcap.com/article/post/3946.html" target="_blank" rel="noopener">https://cn.pingcap.com/article/post/3946.html</a> 中介绍得很好。</p>
<h3 id="KVEngine-上的-WAL"><a href="#KVEngine-上的-WAL" class="headerlink" title="KVEngine 上的 WAL"></a>KVEngine 上的 WAL</h3><p>目前，TiKV 上还是需要开启 KVEngine 对应的 Rocksdb 的 WAL。</p>
<h3 id="Titan"><a href="#Titan" class="headerlink" title="Titan"></a>Titan</h3><p>Titan 是类似 WiscKey 的一个实现。</p>
<p>见 <a href="/2024/11/01/database-paper-2/">database paper 2</a></p>
<h3 id="SST-的格式"><a href="#SST-的格式" class="headerlink" title="SST 的格式"></a>SST 的格式</h3><p>key 一般是按照 <code>(user_key, version)</code> 的顺序来排布的，如果按照 <code>(version, user_key)</code> 的顺序来排布，则可能能快速读到最新版本。比如如下排布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a5 a4 a3 a1 b2 b1 c3 c1</span><br><span class="line">=&gt;</span><br><span class="line">a5 b2 c3 a4 a3 c3 b2 a1 b1 c1</span><br></pre></td></tr></table></figure>

<p>则可以从开头就读到最新的数据。或者，可以在比较新的 SST 中按照这种形式进行存储，这样当 read tso 满足的时候，则就可以快速读。特别地，甚至可以将最新版本的数据作为 SST 的一个 extras block 多存一份。<br>当然，对于 TiFlash 来说，因为有 delta-stable merge 的过程，也许这种方案性能不是很好，除非也改 delta。</p>
<h2 id="TiKV-内存管理"><a href="#TiKV-内存管理" class="headerlink" title="TiKV 内存管理"></a>TiKV 内存管理</h2><p>见 <a href="/2025/01/12/tikv-resource-management/">TiKV 的资源管理模型</a>。</p>
<h2 id="TiKV-读取"><a href="#TiKV-读取" class="headerlink" title="TiKV 读取"></a>TiKV 读取</h2><h3 id="Cache"><a href="#Cache" class="headerlink" title="Cache"></a>Cache</h3><p>TiKV 处理读请求对 Block Cache 要求较高，较低的 Block Cache Hit 会导致读性能倍数下滑。Block Cache 需要占用接近一半的内存，但也需要保留一部分给系统作为 Page Cache，以及处理查询时的内存。</p>
<p>TiKV 没有默认开启 RocksDB 的 direct io，所以理论上 block cache 和 page cache 中的内容是可能有重复的。这里使用 block cache 可能是为了避免重复解压的开销。</p>
<p>在 raftstore-v2 中，多个 Rocksdb 实例共享一个 cache。</p>
<h3 id="Lease"><a href="#Lease" class="headerlink" title="Lease"></a>Lease</h3><p>见<a href="/2025/01/19/raft-learner/">Raft learner</a></p>
<h2 id="Coprocessor"><a href="#Coprocessor" class="headerlink" title="Coprocessor"></a>Coprocessor</h2><h3 id="Cop-可以支持写入么？"><a href="#Cop-可以支持写入么？" class="headerlink" title="Cop 可以支持写入么？"></a>Cop 可以支持写入么？</h3><p>一个合理的优化是让 Cop 能支持 update where 类型的下推。这样就能免去从 TiKV 到 TiDB 的额外一次处理的开销。当然，对于 TiKV 本身来说还是需要将数据从 Rocksdb 读出来，在写回去，从而导致缓存被刷新的问题的。</p>
<h1 id="Multi-Raft-相关"><a href="#Multi-Raft-相关" class="headerlink" title="Multi Raft 相关"></a>Multi Raft 相关</h1><h2 id="关于-Raft-协议本身"><a href="#关于-Raft-协议本身" class="headerlink" title="关于 Raft 协议本身"></a>关于 Raft 协议本身</h2><h3 id="线性一致读"><a href="#线性一致读" class="headerlink" title="线性一致读"></a>线性一致读</h3><p>见<a href="/2025/01/19/raft-learner/">Raft learner</a></p>
<h3 id="乱序-apply-和乱序-commit"><a href="#乱序-apply-和乱序-commit" class="headerlink" title="乱序 apply 和乱序 commit"></a>乱序 apply 和乱序 commit</h3><p>乱序 apply 实际上破坏了 FSM 的特点，基于乱序 apply 的 Learner Read 需要重新设计。</p>
<p>乱序 apply 本身相比乱序 commit 能更多的提高性能。因为 apply 是一个更重的写入。</p>
<h2 id="Raft-状态的思考"><a href="#Raft-状态的思考" class="headerlink" title="Raft 状态的思考"></a>Raft 状态的思考</h2><p>RaftLocalState 中相比 Raft 协议多包含了 last_index 和 commit。其中 commit 可以避免重启后不能立即 apply 的情况。</p>
<h2 id="存储-Raft-状态和-Region-状态"><a href="#存储-Raft-状态和-Region-状态" class="headerlink" title="存储 Raft 状态和 Region 状态"></a>存储 Raft 状态和 Region 状态</h2><p>TiKV 使用 Raft Engine 存储 Raft 元信息和 Raft 日志。使用 KV Engine 存 Region 信息、Region Apply 信息和具体的 KV数据。</p>
<h3 id="一个-Eager-落盘导致的问题"><a href="#一个-Eager-落盘导致的问题" class="headerlink" title="一个 Eager 落盘导致的问题"></a>一个 Eager 落盘导致的问题</h3><p>并不是所有时候，eager 落盘都能保证正确性问题。下面就是一个例子。<br>前面说过，在 TiKV 的实现中有两个 engine，KVEngine 存储 KV Meta 和 KV Data，RaftEngine 存储 Raft Meta 和 Raft Data。其中有一个 Apply Snapshot 的场景会同时原子地修改这两个 Engine，但显然这两个 Engine 无法做到原子地落盘。并且因为两个 Engine 中都存有 Meta 和 Data，所以任意的先后顺序，都会导致数据不一致。这里的解决方式是将 RaftEngine 中的的 Raft Meta 写到 KVEngine 中，称为 Snapshot Meta。写入的时候，会先写 KVEngine，再写 RaftEngine。当在两个非原子写入中间出现宕机，从而不一致的时候，会使用 KVEngine 中的 Raft Meta 替换 RaftEngine 中的 Raft Meta。</p>
<p>在 <a href="https://github.com/tikv/tikv/blob/1ce8ee7df84503e889ff8bc6834a57128a858a16/components/raftstore/src/store/peer_storage.rs#L645" target="_blank" rel="noopener">Apply Snapshot</a> 阶段开始时，它会调用 <code>clear_meta</code> 删除掉 KV Meta、Raft Meta 和 Raft Data，但这个删除是不应该立即落盘的，而是在 WriteBatch 里面。在这之后，还会再往 WriteBatch 中写入 Snapshot Meta 等。这些写入会被一起发送给一个 Async Write 写入。我们的错误是，在实现删除 Raft Engine 数据时，并不是写 Write Batch，而是直接写盘。在 <code>clear_meta</code> 之后系统又立即宕机了。这样重启恢复后，就会看到空的 Raft Meta 和 Raft Data，但 KV Meta 却还存在。这是一个 Panic 错误，因为两个 Meta 不一致了。</p>
<h4 id="Region-Meta-和-Raft-Meta"><a href="#Region-Meta-和-Raft-Meta" class="headerlink" title="Region Meta 和 Raft Meta"></a>Region Meta 和 Raft Meta</h4><p>其实，TiKV raftstore 中还有个 Region Meta，主要是记录这个 Region 的相关信息，例如 Range 等。显然 Region Meta 肯定是要和对应的数据一致的，因此它们要被放到 KVEngine 里面。</p>
<h4 id="经验总结"><a href="#经验总结" class="headerlink" title="经验总结"></a>经验总结</h4><p>这样的错误是难以调查的，我们可以加日志获得重启后从磁盘中读到的结果，但仍然不知道这个结果是如何被写入的。查的方式是脑补，也就是针对这样的场景，假设在不同时刻宕机，考虑会出现什么样的持久化状态。<br>这里，KV Meta 的落盘信息是有的，它可能是没清就宕机了，也可能是写完新的数据之后宕机的。考量这个可以看一些 Meta 信息有没有写入，比如我们发现 Snapshot Meta 并不存在，因此说明是前一种情况。既然如此，为什么 Raft Meta 和 Data 都没了呢？只能说明是 Raft 的清早了。</p>
<p>当然，这里有个迷惑点，就是 KV Meta 提示当前是在 Applying Snapshot 状态，而如果我们是第一种情况的话，这个 Applying 状态应该还没有被写入。这个原因是这个实例发生了多次重启，在 T-2 次启动后 Apply Snapshot 时，KVEngine 和 RaftEngine 都落盘成功了，但是后续的流程没进行下去就重启了。所以在 T-1 次启动会重新 Apply Snapshot，但这一次甚至没到落盘就重启了，而 Snapshot Meta 是金标准。然后就是我们见到的 T 次启动的错误。这启示我们不能只通过一个元数据来判断当前集群的状态，而是要检查所有的元数据，来石锤当前状态是如何得到的。</p>
<h3 id="持久化元信息"><a href="#持久化元信息" class="headerlink" title="持久化元信息"></a>持久化元信息</h3><p>只要在 store 上存在过的 Region，哪怕后续被移动走，都会留下印记。它可能是一个 tombstone 标记，或者一个 merge target 标记（表示自己被 merge 给谁了）。</p>
<h2 id="基于共识层之上的事务"><a href="#基于共识层之上的事务" class="headerlink" title="基于共识层之上的事务"></a>基于共识层之上的事务</h2><p>见<a href="/2025/01/18/percolator-2/">关于 Percolator 的进一步论述</a></p>
<h2 id="Multi-Raft-的思考"><a href="#Multi-Raft-的思考" class="headerlink" title="Multi Raft 的思考"></a>Multi Raft 的思考</h2><p>在一个集群中，维护多个 Raft Group，相对于 Raft 本身来说，是一个全新的挑战。</p>
<h3 id="Split-Merge-和事务"><a href="#Split-Merge-和事务" class="headerlink" title="Split/Merge 和事务"></a>Split/Merge 和事务</h3><h3 id="Split-Merge-和-Read"><a href="#Split-Merge-和-Read" class="headerlink" title="Split/Merge 和 Read"></a>Split/Merge 和 Read</h3><p>Split 和 Merge 会导致 Region 发生变化，自然也可能会影响读取。主要体现在下面几个方面：</p>
<ol>
<li>影响 Lease 本身或者 Lease 续约</li>
<li>推高 RegionEpoch 从而导致 ReadIndex 失败</li>
</ol>
<h3 id="Split-Merge-和-Apply-Snapshot"><a href="#Split-Merge-和-Apply-Snapshot" class="headerlink" title="Split/Merge 和 Apply Snapshot"></a>Split/Merge 和 Apply Snapshot</h3><p>Multi Raft 实现的复杂度，很大程度在处理 Split/Merge 和 Apply Snapshot 的冲突上。</p>
<h4 id="下层存储使用-Mono-LSM-还是-Multi-LSMs"><a href="#下层存储使用-Mono-LSM-还是-Multi-LSMs" class="headerlink" title="下层存储使用 Mono LSM 还是 Multi LSMs"></a>下层存储使用 Mono LSM 还是 Multi LSMs</h4><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a>。</p>
<h4 id="Split-和-Apply-Snapshot-的冲突"><a href="#Split-和-Apply-Snapshot-的冲突" class="headerlink" title="Split 和 Apply Snapshot 的冲突"></a>Split 和 Apply Snapshot 的冲突</h4><p>我们需要处理一个 Region 上的 Follower 还没有执行到分裂为 Base 和 Derived 前，一份来自 Derived 的 Snapshot 已经被发过来的情况。这会产生 Region Overlap 的问题，在一些下层存储中会导致数据损坏。一种方案是在 Base 完成分裂前根据 Epoch 拒绝掉这些 Snapshot。</p>
<h4 id="Merge-和-Apply-Snapshot-的冲突"><a href="#Merge-和-Apply-Snapshot-的冲突" class="headerlink" title="Merge 和 Apply Snapshot 的冲突"></a>Merge 和 Apply Snapshot 的冲突</h4><p>Merge 过程可以简单理解为下面几步：</p>
<ol>
<li>调度 Source 和 Target Region 的各个 Peer，让它们对齐到同一个 Store 上。</li>
<li>Source Peer 执行 Prepare Merge。</li>
<li>Source Peer 等待 Target Peer 追完 Source Peer 的日志。</li>
<li>Source Peer 对 Target Peer 去 Propose Commit Merge。</li>
<li>Target Peer 执行 Commit Merge。</li>
</ol>
<p>可能在下面一些阶段收到 Snapshot：</p>
<ol>
<li>Prepare Merge 结束</li>
<li>Leader 上的 Commit Merge 结束，但 Follower 上的 Commit Merge 还没有开始</li>
</ol>
<h4 id="Split-和-Generate-Snapshot-的冲突"><a href="#Split-和-Generate-Snapshot-的冲突" class="headerlink" title="Split 和 Generate Snapshot 的冲突"></a>Split 和 Generate Snapshot 的冲突</h4><p>主要指 Split 等会改变 RegionEpoch 从而导致 Snapshot 失效。</p>
<h3 id="Raft-Group-和-Data-Range-的对应关系"><a href="#Raft-Group-和-Data-Range-的对应关系" class="headerlink" title="Raft Group 和 Data Range 的对应关系"></a>Raft Group 和 Data Range 的对应关系</h3><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a></p>
<h3 id="Raft-到底复制什么？"><a href="#Raft-到底复制什么？" class="headerlink" title="Raft 到底复制什么？"></a>Raft 到底复制什么？</h3><p>Raft 日志中到底记录什么呢？可以看下面的总结：</p>
<ol>
<li>TiKV<br> TiKV 中 Raft 日志分为 Admin 和 Write。Admin 基本只和 Raft 和 Region 管理有关。Raft 指的是 Raft 的成员变更，比如 Add/Remove Voter/Learner，TransferLeader 等。Region 指的是管理的 key range 的元数据变更，比如 Split、Merge、数据校验等。<br> Admin 和 Write 在一起构成全序关系，这个话题之前已经展开讨论过了。<br> Write 包含 Put、Delete、DeleteRange 和 IngestSST，这些都是逻辑日志，或者说是不 aware 下层 rocksdb 的。</li>
<li>OceanBase<br> OceanBase 中复制的是 clog。从<a href="https://en.oceanbase.com/docs/common-oceanbase-database-10000000001029737" target="_blank" rel="noopener">文档</a>来看，它们复制的是物理日志。通过 replay clog，能够得到同样的 log 文件，其中记录的是 redo log。<br> 下面来自<a href="https://www.oceanbase.com/docs/community-observer-cn-10000000000016344" target="_blank" rel="noopener">Oceanbase 文档</a><blockquote>
<p>OceanBase 数据库单台物理机上启动一个 observer 进程，有几万到十万分区，所有分区同时共用一个 Clog 文件，当写入的 Clog 文件超过配置的阈值（默认为 64 MB）时，会打开新的 Clog 文件进行写入。<br>OBServer 收到的某个分区 Leader 的写请求产生的 Clog、其他节点 OBServer 同步过来的 Clog（存在分区同在一个 Paxos Group)，都写入 Log Buffer 中，由单个 IO 线程批量刷入 Clog 文件。</p>
</blockquote>
</li>
<li>PolarDB<br> 在《PolarFS: An Ultra-low Latency and Failure Resilient Distributed File System for Shared Storage Cloud Database》中讲得比较清楚。<br> PolarDB 的存储层基于 PolarFS，计算节点共享地访问这个存储层。PolarDB 中每个数据库对应 PolarFS 中的一个卷，每个卷由若干 Chunk 组成。不同于 TiKV 的 Region，这里 Chunk 大小为 10GB，而卷的大小在 10GB 到 100TB 之间，所以它们元数据节点的调度压力会小很多，并且所有节点的元数据都可以缓存在内存中。一个 Chunck Server 管理多个 Chunk，PolarDB 通过增加 ChunkServer 的数量来平衡热点。这里我觉得 TiKV 的 multi rocks 方案可能更好，因为它允许一个 hot region 被分裂。在 PolarDB 中，一个服务器上运行多个 ChunkServer，但每个 ChunkServer 对应一个专用的 SSD，并且绑定一个专用的 CPU 核心。<br> 一个 Chunk 由 64KB 大小的 block 组成。PolarFS 的 Raft 日志实际复制的是这些 block 的 WAL。</li>
<li>Kudu<br> Kudu 中复制的是逻辑日志。他们的观点是这样可以实现各个 Replica 在存储格式上是解耦的。</li>
</ol>
<h3 id="进一步讨论：日志和选举的关系"><a href="#进一步讨论：日志和选举的关系" class="headerlink" title="进一步讨论：日志和选举的关系"></a>进一步讨论：日志和选举的关系</h3><p>Raft 中的领导人完全性原则要求 Leader 必须拥有所有已提交的日志，这实际上是一个比较强的约束。在 Ongaro 等人对于 MultiPaxos 的描述中，可以发现该约束是可以被消减掉的，从而选举过程可以不关注日志的完备性。<br>在此基础上，可以让选举体现出其他的优先级。以 Ob 的 Palf 为例，它的“一呼百应”的方案，可以始终给距离自己最“近”的节点投票。而 Raft 选举的实质是谁状态更新，谁就更容易当选。这个方案目前来看，无论是否效果最优，但确实代价比较大。</p>
<p>有关 Raft 的日志和选举关系的讨论，可以见 <a href="/2019/03/12/raft-algorithm/">Raft 算法介绍</a> 中的“日志和选举”章节详细讨论。</p>
<h3 id="进一步讨论：日志和事务的关系"><a href="#进一步讨论：日志和事务的关系" class="headerlink" title="进一步讨论：日志和事务的关系"></a>进一步讨论：日志和事务的关系</h3><p>将多个分区的写入统一到一个 Raft Group 中进行复制，应该是有利于事务的。因为如果一个事务跨 Region，就会是一个分布式事务，而如果只有一个 Raft Group，那么就不会涉及到跨 Region 的问题。</p>
<h3 id="Mono-LSM-和-Multi-LSM-的考量"><a href="#Mono-LSM-和-Multi-LSM-的考量" class="headerlink" title="Mono LSM 和 Multi LSM 的考量"></a>Mono LSM 和 Multi LSM 的考量</h3><p>这里指的是不同的 Region 的数据是否 share 一个 LSM 树。我认为如果使用 range partition，那么 multi lsm 的策略是一个非常重要的优化。</p>
<h2 id="多-Region-的调度"><a href="#多-Region-的调度" class="headerlink" title="多 Region 的调度"></a>多 Region 的调度</h2><h3 id="TiKV-的做法"><a href="#TiKV-的做法" class="headerlink" title="TiKV 的做法"></a>TiKV 的做法</h3><p><a href="https://docs.pingcap.com/zh/tidb/stable/pd-scheduling-best-practices" target="_blank" rel="noopener">PD</a> 中有一些策略：</p>
<ol>
<li>balance-leader<br> 目的是均衡 client 请求服务的压力</li>
<li>balance-region<br> 目的是分散存储压力，防止爆盘。因此会在磁盘剩余空间充足的时候使得使用量均衡，在不充足的时候使得剩余量均衡。</li>
<li>hot-region-scheduler<br> 目的是分散热点 Region</li>
<li>location-labels</li>
</ol>
<p>实际上这些策略不太够，还需要：</p>
<ol>
<li>balance-leader-within-table</li>
<li>balance-region-within-table</li>
</ol>
<p>这些 balancer 可以基于 count，也可以基于 size。</p>
<h1 id="事务相关"><a href="#事务相关" class="headerlink" title="事务相关"></a>事务相关</h1><p>见<a href="/2025/01/18/percolator-2/">关于 Percolator 的进一步论述</a></p>
<h1 id="Partitioned-RaftKV-相关"><a href="#Partitioned-RaftKV-相关" class="headerlink" title="Partitioned RaftKV 相关"></a>Partitioned RaftKV 相关</h1><p>见 <a href="/2023/07/23/on-partition-raft-kv/">TiKV 的 partitioned raft kv 特性</a></p>
<h1 id="TiFlash-相关"><a href="#TiFlash-相关" class="headerlink" title="TiFlash 相关"></a>TiFlash 相关</h1><h2 id="Raft-和-CDC"><a href="#Raft-和-CDC" class="headerlink" title="Raft 和 CDC"></a>Raft 和 CDC</h2><h3 id="CDC-的相关背景"><a href="#CDC-的相关背景" class="headerlink" title="CDC 的相关背景"></a>CDC 的相关背景</h3><p>TiCDC 提供的是 at-least-once 语义。具体来说，当 TiKV 或者 TiCDC 集群发生故障，则可能会发送相同的 DDL 或者 DML。</p>
<p>从实现上，TiCDC 中一个 changefeed 表示一个同步任务，它会被拆解为若干个 Task，分配给 TiCDC 集群的各个节点上的 Capture 进程进行处理。其中，每个节点上有一个 Capture 进程负责管理集群内部调度，称为 Owner Capture。Changefeed 的相关信息会被持久化到 pd 上。</p>
<p>在 TiFlash 建项，以及后续一段较长的时间中，CDC 存在一些问题：</p>
<ul>
<li>并发较低，changefeed 数量受限<br>  导致对多表的支持比较差</li>
<li>吞吐量受限</li>
<li>对 DDL 的支持较差</li>
<li>集群的 scalability 受限</li>
</ul>
<p>CDC 的同步遵照几个时间戳：</p>
<ul>
<li><p>Resolve TS<br>  所有 TiKV 节点上的 Region leader 的 ResolvedTS 的最小值，被称为 Global ResolvedTS。TiDB 集群确保 Global ResolvedTS 之前的事务都被提交了。<br>  容易想到，通过维护所有 Region 上的所有事务的 start_ts 的最小值就可以达到这个目的。这里从 Leader 节点获取 start_ts。</p>
<ul>
<li>TiCDC 上维护的 Table ResolvedTS<br>  和 TiKV 节点上这张表的各个 Region 的 ResolvedTS 的最小值是相同的。</li>
<li>TiCDC 上维护的 Global ResolvedTS<br>  各个 TiCDC 节点上的 Processor ResolvedTS 的最小值。<br>  由于 TiCDC 每个节点上都会存在一个或多个 Processor，每个 Processor 又对应多个 table pipeline。<br>  容易看出  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table ResolvedTS &gt;= Global ResolvedTS</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>CheckpointTS<br>  TiCDC 认为在这个时间戳之前的数据已经被同步到下游系统了。<br>  容易看出</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Table ResolvedTS &gt;= Global ResolvedTS &gt;= Table CheckpointTS &gt;= Global CheckpointTS</span><br></pre></td></tr></table></figure></li>
<li><p>Barrier TS<br>  一个 Barrier TS 被生成后, TiCDC 会保证只有小于 Barrier TS 的数据会被复制到下游，并且保证小于 Barrier TS 的数据全部被复制到下游之前，同步任务不会再推进。</p>
</li>
</ul>
<p>TiCDC 中的 Sorter 负责排序。它使用 PebbleDB 来暂存 KV 数据。</p>
<h3 id="为什么-TiFlash-实现-HTAP-基于-Raft？"><a href="#为什么-TiFlash-实现-HTAP-基于-Raft？" class="headerlink" title="为什么 TiFlash 实现 HTAP 基于 Raft？"></a>为什么 TiFlash 实现 HTAP 基于 Raft？</h3><p>Raft 帮助我们实现：</p>
<ol>
<li>LB</li>
<li>HA</li>
<li>Sharding</li>
</ol>
<p>但是 TiFlash 只通过 Raft 同步各个表的 record 部分的数据。我们不同步索引，因为不需要。我们不同步 DDL 相关结构，因为并不是所有表都存在 TiFlash 副本。取而代之的是在解析失败，或者后台任务中，定期取请求 TiKV 的 Schema。</p>
<p>另一种强一致的方案是基于 CDC 和 safe TS，这样的方案理论上达不到和 Raft 一样的性能。这是因为类似 CDC 的方案的 safe TS 是基于表的，而 Raft 的 applied_index 是基于 Region 的。在一些场景下，如果一个 write 涉及到多个 Region，那么为了保证原子性，需要这些 Region 上的数据全部被写完，才能前进 ts，这会影响大事务的同步效率。另外，在读取时，也需要等待 safe TS 前进之后，才能读取。而基于 Raft 的方案只需要相关的 Region 的 applied_index 前进到 ReadIndex 就可以了。另外，CDC 也只保证单表事务。</p>
<h2 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h2><h3 id="为什么在-TiSpark-之外还开发-TiFlash"><a href="#为什么在-TiSpark-之外还开发-TiFlash" class="headerlink" title="为什么在 TiSpark 之外还开发 TiFlash"></a>为什么在 TiSpark 之外还开发 TiFlash</h3><p>TiSpark 直接操作 TiKV，绕过了事务层，可能产生一致性问题。<br>TiSpark 没有自己的列式存储，处理分析性查询并不占优势。</p>
<h3 id="副本-节点数量和延迟的关系"><a href="#副本-节点数量和延迟的关系" class="headerlink" title="副本/节点数量和延迟的关系"></a>副本/节点数量和延迟的关系</h3><p>不是。理论上是 1 副本的性能最好，但是考虑到高可用，通常建议 2 副本。</p>
<p>1 副本性能最好的原因是，DeltaTree 的 Segment 的粒度要显著比 TiKV 的 region 大，因此同一个 Segment 上会存在多个 Region。</p>
<p>考虑存在 4 个 Region，从 A 到 D，后面的数字表示 replica id。如果只设置一个副本，其分布类似</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Store1: [A0, B0]</span><br><span class="line">Store2: [C0, D0]</span><br></pre></td></tr></table></figure>

<p>而如果设置两个副本，其分布类似</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Store1: [A0, B0, C0, D0]</span><br><span class="line">Store2: [A1, B1, C1, D1]</span><br></pre></td></tr></table></figure>

<p>假如一个查询同时覆盖这 4 个 region，那么一副本的情况下，Store1 和 Store2 分别扫描自己的一部分数据就行了。而两副本的情况下，则可能扫描到多余的 Region 的数据。</p>
<p>这里可能会有 argument，因为 TiKV 的数据格式是有序的，同一个 table 一定是有空间局部性的。假如表 t1 对应了 A 和 B 两个 Region，那么就不可能扫到 C 和 D。但需要注意几个问题：</p>
<ol>
<li>在 Filter 的情况下，我们并不是 full table scan，而是一段区间一段区间去读。</li>
<li>TiFlash 的 IO 单位是 Pack，一个 Pack 对应大约 8192 行数据（跟随 ClickHouse 这么设置的）。这里 Pack 是不一定和 Region 边界对齐的。</li>
</ol>
<p>因此，如果让副本更为分散，则可能导致读出更多的 Pack。</p>
<h3 id="副本-节点数量和并发的关系"><a href="#副本-节点数量和并发的关系" class="headerlink" title="副本/节点数量和并发的关系"></a>副本/节点数量和并发的关系</h3><p>副本数越多，并发能力越强？但在基于 Raft 的分区策略下，并发能力是通过合理的 Sharding 来提升的。而具体到一个副本上是可以支持大量的并发查询的，并且我们也更容易对这些查询做 Cache，当然在 AP 场景下可能有限。</p>
<p>当然，需要考虑小表的情况，因为小表只有一个 Region。</p>
<h3 id="DDL-如何同步？"><a href="#DDL-如何同步？" class="headerlink" title="DDL 如何同步？"></a>DDL 如何同步？</h3><p>TiDB 的 DDL 的优化点：</p>
<ol>
<li>延迟 reorg 到读<br> 例如 add column 的 reorg 阶段实际上不会写入默认值，而是在读的时候才返回默认值。</li>
<li>以新增代替变更<br> 例如 alter column 只会扩大列的值域，比如 int8 扩大为 int64。如果涉及缩小至于或者改变类型，则会体现为新增一个 column，然后把老的 detach 掉。<br> 因此新的 Schema 能够解析老的 Schema。</li>
</ol>
<p>TiFlash 上 DDL 的特点：</p>
<ol>
<li>TiFlash 只需要同步需要表的 DDL。</li>
<li>TiFlash 只需要同步部分 DDL 类型，诸如 add index 等 DDL 并不需要处理，更没有 reorg 过程。</li>
<li>尽管 TiDB 将 schema 存在 TiKV 上，但 TiKV 是 schemaless 的。所以如果 TiFlash 只从 TiKV 同步数据，就会涉及解码等工作。</li>
</ol>
<p>因此，TiFlash 有两种 DDL 同步方式：</p>
<ol>
<li>定期拉取（一般是 10s）并更新<br> 根据 TiFlash 和 TiDB 上 version 落后的情况，可以分为拉 diff 和拉全量。<br> 该方式已经能解决大部分 drop table 的问题了。但通过该方式无法保证当前任意时间点上的 schema 一定和 TiDB 是一致的，所以一定存在解析失败的情况。</li>
<li>当解析 row 失败的时候更新 schema，称为 lazy sync</li>
</ol>
<p>在更新之后，TiFlash 会自己维护一份 schema。</p>
<p>这里面存在的问题主要是两种 DDL 同步方式和实际 raft log 是异步的。因为 TiDB 和 TiFlash 的特点，这个异步是可以被处理的，并且尽可能去掉全序的依赖是很多系统的设计理念，所以这种做法本身也是挺好的，但其中 corner case 很多。例如：</p>
<ol>
<li>Schema 和 row data 中的列数对不上。这种情况无论是谁缺，至少可以通过拉一次 Schema 来解决。有些场景甚至可以不拉 schema。</li>
<li>某个列的类型变了</li>
<li>一张表 drop 后，TiKV 中就无法读取该表的 schema 了。如果在 drop 前有一条 add column，但 lazy sync 又没有读到，那么 TiFlash 就看不到。所以如果后续有一条 row 写入过来，TiFlash 就会丢弃这个 column。假如这个 table 被 recover 了，那么 TiFlash 就会读不到这个 column 的数据。</li>
<li><a href="https://github.com/pingcap/tiflash/pull/8422" target="_blank" rel="noopener">在一张表</a>对应的 DeltaMerge 实例创建前，这张表就被 drop 掉了。在此之后，row 数据到来，并导致 DeltaMerge 实例被创建。</li>
</ol>
<h3 id="TiFlash-的高可用"><a href="#TiFlash-的高可用" class="headerlink" title="TiFlash 的高可用"></a>TiFlash 的高可用</h3><p>对于复制自动机的系统，高可用主要取决于选举的速度。<br>对于 TiFlash 来说，它不参与选举，但选举本身同样会有影响，一方面是 ReadIndex，另一方面是无主的时候无法复制日志。但除此之外，TiFlash 自身的宕机和重启也影响高可用。因为一个批量查询会被下推给 tiflash，以避免影响 TP，如果此时 TiFlash 没追上，则查询会 hang 住。所以 TiFlash 的高可用还和追日志的规模有关。</p>
<h2 id="Raft-共识层"><a href="#Raft-共识层" class="headerlink" title="Raft 共识层"></a>Raft 共识层</h2><h3 id="有关-Learner-Peer"><a href="#有关-Learner-Peer" class="headerlink" title="有关 Learner Peer"></a>有关 Learner Peer</h3><p>见<a href="/2025/01/19/raft-learner/">Raft learner</a></p>
<h3 id="有关-Learner-Read"><a href="#有关-Learner-Read" class="headerlink" title="有关 Learner Read"></a>有关 Learner Read</h3><p>由 Follower Read 派生出来的 Learner Read 也让 TiFlash 成为一个强一致的 HTAP。<br>见<a href="/2025/01/19/raft-learner/">Raft learner</a></p>
<h3 id="有关-lock"><a href="#有关-lock" class="headerlink" title="有关 lock"></a>有关 lock</h3><h4 id="Bypass-lock-机制"><a href="#Bypass-lock-机制" class="headerlink" title="Bypass lock 机制"></a>Bypass lock 机制</h4><p>见<a href="/2025/01/18/percolator-2/">关于 Percolator 的进一步论述</a></p>
<h4 id="Read-through-lock-机制"><a href="#Read-through-lock-机制" class="headerlink" title="Read through lock 机制"></a>Read through lock 机制</h4><p>见<a href="/2025/01/18/percolator-2/">关于 Percolator 的进一步论述</a></p>
<h3 id="Raft-Log-的存储"><a href="#Raft-Log-的存储" class="headerlink" title="Raft Log 的存储"></a>Raft Log 的存储</h3><h2 id="存储-–-KVStore"><a href="#存储-–-KVStore" class="headerlink" title="存储 – KVStore"></a>存储 – KVStore</h2><h3 id="为什么在列存前还有一个-KVStore？"><a href="#为什么在列存前还有一个-KVStore？" class="headerlink" title="为什么在列存前还有一个 KVStore？"></a>为什么在列存前还有一个 KVStore？</h3><p>在 CStore 模型中，WS 和 RS 都是列存，WS 的数据通过 Tuple Mover 被批量合并到 RS 中。体现在 TiFlash 中，WS 是 DM 中基于 PS 的 Delta 层，而 RS 是 Stable 层。</p>
<p>除此之外，TiFlash 还有一个 KVStore，目的是：</p>
<ol>
<li>保存未提交的数据，并实现 Percolator 事务的部分功能<br> 因为只有已提交的数据才会写入行存，为了和 Apply 状态机一致，所以未提交的数据同样需要持久化，因此引入 KVStore。</li>
<li>KVStore 管控 Apply 进度，对 DM 屏蔽了上游。DM 可以异步落盘。日志复制的架构下，上游的落盘进度不能比下游更新，因为下游更新，重放是幂等的；而上游更新，会丢数据。</li>
</ol>
<h3 id="为什么不将未提交的数据直接写在列存中呢？"><a href="#为什么不将未提交的数据直接写在列存中呢？" class="headerlink" title="为什么不将未提交的数据直接写在列存中呢？"></a>为什么不将未提交的数据直接写在列存中呢？</h3><ol>
<li>KVStore 需要负责维护 apply 状态机<br> 当然我们可以将这一部分作为单独的 Raft 模块，所以这不是很 solid 的理由。</li>
<li>KVStore 不仅是一个容器，还是 Percolator 事务的执行器<br> 例如，它需要维护当前 Region 上的所有 Lock。在一个查询过来时，需要检查该查询是否和 Lock 冲突，并尝试 resolve lock。而在列存中维护 lock cf 会很奇怪。</li>
<li>这意味着要执行近乎实时的行转列<br> 首先，如果存一些未提交数据在 KVStore 中，然后在提交时 batch 执行行转列，有可能可以只读取一次 schema 结构，减少开销。<br> 其次，TiDB 中存在乐观事务和悲观事务。如果使用乐观事务，并且冲突比较大，那么很可能 TiFlash 要花费大量时间在多余的行转列上。</li>
</ol>
<p>实际上，在后续支持大事务的实践中，我们确实会进行一部分提前的行转列。但这是处于内存的优化，并且也存在很大的局限，例如暂时无法做到跨 Region Spill。</p>
<h3 id="KVStore-的落盘模式相关问题"><a href="#KVStore-的落盘模式相关问题" class="headerlink" title="KVStore 的落盘模式相关问题"></a>KVStore 的落盘模式相关问题</h3><p>理论上 KVStore 也可以做到独立写盘，从而使得 DM 的落盘进度不会阻塞 Raft Log 的回收。缺点是会使 KVStore 完全变成上游，写链路更长。虽然我们底层用的 PS，Compaction 相对较少，但同样有写放大。但这目前也无法实现，因为：</p>
<ol>
<li>KVStore 落盘是全量的，KVStore 和 DM 的内存操作又绑在一块。<br> 这导致在落盘 KVStore 前必须先落盘 DM。并且整个过程还需要加自己的锁，否则会导致数据丢失，而加锁导致阻塞 Apply。特别在一些场景下，少量的 Raft Log 就会导致 KVStore 和 DM 的落盘，严重影响读取延迟。</li>
<li>Raftstore V1 的 Apply 落盘又是同步的。<br> 在 Raftstore V1 中，写入的数据可能在操作系统的 Page Cache 中，也有可能被刷入了磁盘。如果是前者，那么会在 raftlog_gc 等地方被显式地 sync。但困难在于，V1 中无法精确获得这些时刻，从而进行通知。又因为 TiFlash 的状态不能落后于 Proxy，否则 Proxy 的 applied_index 可能比 KVStore 新从而丢数据。所以这里索性当做同步落盘处理，让 TiFlash 先落盘。即使 TiKV 重放，也是幂等的。代价是我们要劫持 TiKV 所有可能写 apply state 的行为，哪怕这个写不是 sync 写。后面会介绍我的一些异步落盘的想法。</li>
</ol>
<p>一个优化方案是解耦 KVStore 和 DM 的落盘。也就是在 DM 落盘后，再清理掉 KVStore 中的数据。这需要将 Region 中的数据拆分成 KV 对落盘，但这会失去对 KV 对做聚合的能力，从而将顺序写转换为随机写，如果写入很密集，性能也许会比较差，所以这个在功能和性能上都依赖 UniPS。</p>
<p>另一种方案比较简单，也就是限制由 KVStore，实际上就是 Raftstore 发起的落盘，改为由 DM 发起。但这个方案并不感知 Raft Log 的占用，可能导致它膨胀。</p>
<p>前面提到异步落盘 KVStore 的问题，一个思路是落盘时使用过去的状态+当前的数据。但存在一些问题：</p>
<ol>
<li>这个“过去的状态”也需要比 DM 的落盘状态要新，所以还是要先加锁获取 KVStore 状态，再无锁落盘 DM，再用旧状态落盘 KVStore。这样不能解耦和 DM 的落盘，但能够在落盘 DM 的时候无锁已经很好了。</li>
<li>Split/Merge 或者可能 Apply Snapshot 改变了全局状态。这样的指令在 V1 中是不能被重放的，不然新 Split 出来的 Region 可能和重启前已经被 Split 和 Persist 出来的 Region 冲突。这样就需要在处理这些 Admin 指令的时候同步等待异步的 Persist 完成。其实更简单的方式是根据之前加锁获取的状态来推断有没有执行这些 Admin。</li>
<li>需要让 KVStore 支持其他命令的重放。目前来看，应该存在一些 corner case。</li>
<li>需要让 KVStore 通知 Proxy，当前落盘的 applied_index 并不是期望的 applied_index。这实际上破坏了 TiKV 的 MultiRaft 约束，更好的方式是拒接来自 Proxy 的落盘请求，然后从 KVStore 重新主动发起一个。</li>
<li>落盘 KVStore 同样需要加锁，从而阻塞 Raft 层的写入。</li>
</ol>
<p>另一种方案是过去的状态和过去的数据。比如可以在 KVStore 在落盘时，新开一个 Memtable 处理新写入。此时需要处理新 Memtable 上的 Write 可能依赖老 Memtable 上的 Default 之类的问题。这样的好处是在落盘 KVStore 的时候都不需要加锁了。但是还存在两个问题：</p>
<ol>
<li>在这前面需要落盘 DM，当然这个锁先前说了可以去掉。</li>
<li>如果写入很大，那么可能在旧的 Memtable 还没写完之前，新的 Memtable 就满了。这样还是 Write Stall。</li>
</ol>
<p>如果希望彻底和 DM 解耦，就需要想办法保存上次 DM 落盘到现在落盘 KVStore 期间被写到 DMCache 上的数据。这是困难的。</p>
<h3 id="KVStore-如何处理事务"><a href="#KVStore-如何处理事务" class="headerlink" title="KVStore 如何处理事务"></a>KVStore 如何处理事务</h3><p>在每一次 Raftstore 的 apply 写入时，会遍历所有 write 写入，并进行事务提交，也就是将数据从 KVStore 移动到 DeltaMerge。事务提交并不一定落盘，大部分情况是写在 DeltaMerge 的 DeltaCache 中的。<br>如果出现事务 rollback 回滚，则 TiKV 不仅会删除掉之前写的 default 和 lock，还会写一条 Rollback 记录，它也会被写到 Write CF 中，其用途是避免同 start_ts 事务再次被发起，client 需要用新请求的 start_ts。<br>可以看到，因为共识层的存在，TiFlash 无需处理事务 rollback 的问题。这也是 KVStore 存在的意义之一。</p>
<h3 id="KVStore-的存储格式"><a href="#KVStore-的存储格式" class="headerlink" title="KVStore 的存储格式"></a>KVStore 的存储格式</h3><h4 id="是否直接用-protobuf-存储-Region？"><a href="#是否直接用-protobuf-存储-Region？" class="headerlink" title="是否直接用 protobuf 存储 Region？"></a>是否直接用 protobuf 存储 Region？</h4><p>protobuf 具有的几个特性让它不适合存储 Region：</p>
<ol>
<li>较大的 size 下性能较差</li>
<li>不能只读取部分数据</li>
</ol>
<h4 id="是否使用-flag-存储-Region-Extension？"><a href="#是否使用-flag-存储-Region-Extension？" class="headerlink" title="是否使用 flag 存储 Region Extension？"></a>是否使用 flag 存储 Region Extension？</h4><p><a href="https://github.com/pingcap/tiflash/issues/8590" target="_blank" rel="noopener">https://github.com/pingcap/tiflash/issues/8590</a> 不建议这样做。</p>
<h3 id="Raft-机制带来的内存和存储开销"><a href="#Raft-机制带来的内存和存储开销" class="headerlink" title="Raft 机制带来的内存和存储开销"></a>Raft 机制带来的内存和存储开销</h3><p>有没有可能 TiFlash 自己 truncate 日志呢？理论上 Learner 不会成为 Leader 从而发送日志，也不会处理 Follower Snapshot 请求。而 Raft 协议本身就是让每个节点自己做 Snapshot 然后 truncate 日志的。</p>
<p>我们在云上 TiFlash 做这样的称为 Eager GC 的优化，因为云上使用的 UniPS 对内存更敏感。PageDirectory 为每个 Page 占用大约 0.5KB 的内存。另一方面，UniPS 全部受我们控制，所以相比 Raft Engine 也更好做透明的回收。透明回收小于 persisted applied_index 的所有 Entry，如果 Raftstore 会访问已经被回收的 Entry，会给一个 Panic。</p>
<h3 id="TiFlash-如何处理-Raft-Snapshot？"><a href="#TiFlash-如何处理-Raft-Snapshot？" class="headerlink" title="TiFlash 如何处理 Raft Snapshot？"></a>TiFlash 如何处理 Raft Snapshot？</h3><ol>
<li>raftstore 执行 apply snapshot</li>
<li>raftstore 将 snapshot 入队 region worker</li>
<li>TiFlash 进行 Prehandle</li>
<li>TiFlash 执行 apply snapshot data</li>
</ol>
<h3 id="为什么-TiFlash-不处理-DeleteRange？"><a href="#为什么-TiFlash-不处理-DeleteRange？" class="headerlink" title="为什么 TiFlash 不处理 DeleteRange？"></a>为什么 TiFlash 不处理 DeleteRange？</h3><p>TiKV 通过 DeleteRange 来删表。TiFlash 则是通过拉取 DDL，并确保已经过了 gc safepoint 后，才会物理删除表。</p>
<p>需要注意的是，除了删表之外，pd 可能从 TiFlash 调度走某个 Region，这也涉及删除操作。对于这样的操作，TiFlash 就得立即响应。</p>
<p>在 gc 时，在 write cf 上写一个 DEL 记录，也就是所谓的 tombstone key 是比较少见的。现在的做法是在 Compaction 的时候将这些 key filter 掉。<br>当然 DEL lock cf 是很常见的，这通常发生在：</p>
<ul>
<li>提交事务的时候，会将乐观锁替换为 write 记录。</li>
<li>提交悲观事务的时候，会将悲观锁覆写为乐观锁。</li>
</ul>
<h3 id="重放"><a href="#重放" class="headerlink" title="重放"></a>重放</h3><p>TiFlash 在启动时，会有个 WaitRegionReady 的过程，它会尽可能等所有的 Region 重放日志，从而让它们追上进度。这增加了 TiFlash 的启动时间，但也避免后续查询频繁报错。</p>
<p>在<a href="http://mysql.taobao.org/monthly/2023/11/01/" target="_blank" rel="noopener">文章</a>中提到了一个写省略的方案，也就是对于一个 Page，重放的时候，并不直接 Apply，而是在真的要读的时候，“通过 Log 的按 Page 索引找到需要的 Log Record”去 Apply。目的是节省刷盘的开销，以及提升踢动的速度。</p>
<h2 id="存储-–-Proxy"><a href="#存储-–-Proxy" class="headerlink" title="存储 – Proxy"></a>存储 – Proxy</h2><h3 id="为什么-Proxy-不能静态链接"><a href="#为什么-Proxy-不能静态链接" class="headerlink" title="为什么 Proxy 不能静态链接"></a>为什么 Proxy 不能静态链接</h3><p>之前有一些<a href="https://github.com/pingcap/tiflash/pull/4842/files" target="_blank" rel="noopener">尝试</a>，结论是需要通过一些比较 hack 的方式进行改名。</p>
<p>具体来说，就是：</p>
<ul>
<li>通过 <code>nm -D --extern-only --defined-only</code> 去获得 proxy 所有引用的外部符号，包括 C 库啥的。</li>
<li>然后使用 <code>objcopy --prefix-symbols=prefix_ proxy.so new_proxy.so</code> 给 proxy 中所有的符号都加一个前缀。</li>
<li>然后使用 <code>objcopy --redefine-sym &quot;{prefix}{i}={i}&quot;</code> 重命名第一步中的每个符号 <code>i</code>。 </li>
</ul>
<h3 id="关于-Proxy-的重构"><a href="#关于-Proxy-的重构" class="headerlink" title="关于 Proxy 的重构"></a>关于 Proxy 的重构</h3><p>我觉得其中一个非常重要的点是测试方案的改造。测试分为几个层面：</p>
<ol>
<li>Proxy 的 ut<br> 因为 Proxy 的逻辑是在 TiKV 逻辑之下的，所以这个 ut 也会包含 TiKV 的一部分代码，但具体执行的代码是受到控制的。我们通过 new-mock-engine-store 中的 mock cluster 模块去做这一点。</li>
<li>Proxy 和 TiKV 的集成测试<br> 这个是主要的功能性测试。因为更上层的测试引入了 TiFlash，链路更长，更难诊断问题。<br> 因此提出了两个方案：<ul>
<li>第一个是 mixed mode 测试的方案</li>
<li>第二个是引入了 new-mock-engine-store 去 mock TiFlash 部分的逻辑。</li>
</ul>
</li>
<li>Proxy + TiFlash 的 CI 测试</li>
<li>Proxy + TiFlash 的 daily run 测试</li>
</ol>
<p>Proxy + TiKV 的 mixed mode 测试的方案：</p>
<ol>
<li>初始状态：code base 中全部为旧 Proxy 的代码逻辑，测试只包含旧 Proxy 从 TiKV migrate 来的测试 tests/tikv。</li>
<li>引入 new-mock-engine-store 和 tests/proxy 作为新 Proxy 的测试框架和内容，给它们打上 compat_new_proxy 这个 feature。</li>
<li>在迁移的初期，我们对 code base 中的修改打上 compat_new_proxy。在测试环境中，这些逻辑只对 tests/proxy 生效，因而可以验证迁移后的逻辑，并不影响 tests/tikv。</li>
<li>随着迁移的进行，Proxy 特有逻辑部分的测试被慢慢转移到 tests/proxy 中；而由于我们从 TiKV 中解耦了 Proxy 的特有逻辑，也可以逐步去除 tests/tikv 中手动适配的代码，使得它们恢复在 TiKV 中原来的样子。</li>
<li>在迁移的后期，我们已经可以将不少模块 checkout 回 TiKV 的代码了，在这些代码中保留 compat_new_proxy 就没有必要。因此我们删除 compat_new_proxy，对于残余模块中需要保持旧 Proxy 代码逻辑的部分，打上 compat_old_proxy 这个 feature。这样带上 compat_old_proxy，我们依然可以运行 tests/tikv 测试。</li>
<li>迁移结束意味着 Proxy 特有逻辑被彻底被移出 TiKV。最终我们得到了 tests/proxy 用于测试 Proxy 特有逻辑，而 tests/tikv 用于测试 TiKV 原始逻辑。</li>
</ol>
<p>Proxy + TiFlash 的 mixed mode 测试方案：这个比较简单，主要就是选择一个 TiFlash 为旧版本，另一个为新版本，测试两个 TiFlash 是否行为一致。特别地，因为 TiFlash 的查询会轮询可用的存储，所以新老节点只要有不一致，就会被发现。</p>
<h2 id="存储-–-列存"><a href="#存储-–-列存" class="headerlink" title="存储 – 列存"></a>存储 – 列存</h2><h3 id="为什么-TiFlash-使用-DeltaTree-作为存储"><a href="#为什么-TiFlash-使用-DeltaTree-作为存储" class="headerlink" title="为什么 TiFlash 使用 DeltaTree 作为存储"></a>为什么 TiFlash 使用 DeltaTree 作为存储</h3><p>目的是为了适应频繁的更新。TiFlash 采用类似 <a href="https://gogim1.github.io/posts/c_store" target="_blank" rel="noopener">CStore</a>的思路，引入了 PageStorage 这个对象存储。其中针对写优化的部分称为 Delta 层，类似于 RocksDB 的 L0，存储在 PageStorage 中。针对读优化的部分称为 Stable 层，以 DTFile 文件的形式存储，但文件路径在 PageStorage 作为 External Page 的形式维护。</p>
<h3 id="存储模型的进一步讨论"><a href="#存储模型的进一步讨论" class="headerlink" title="存储模型的进一步讨论"></a>存储模型的进一步讨论</h3><h4 id="和-StarRocks-的比较"><a href="#和-StarRocks-的比较" class="headerlink" title="和 StarRocks 的比较"></a>和 StarRocks 的比较</h4><p>例如可以将 update 操作分为 delete 和 insert 操作。查询时，同时查询 delete 和 insert，并决定最终的输出。StarRocks 使用这样的方式，他们指出 Delete+Insert 这样的模式<a href="https://zhuanlan.zhihu.com/p/566219916" target="_blank" rel="noopener">有利于下推 Filter</a>。StarRocks 据此实现了<a href="https://docs.starrocks.io/zh/docs/table_design/table_types/primary_key_table/" target="_blank" rel="noopener">主键模型</a>。<br>这里需要区分他们的<a href="https://docs.starrocks.io/zh/docs/table_design/table_types/unique_key_table/" target="_blank" rel="noopener">更新模型</a>，也就是一种不支持 MVCC，始终返回最新数据的模型。这种模型应该就是一种类似 LSM 的方案，在 Compaction 的时候只保留一个版本。但是在查询的时候仍然需要 merge 多个版本，并且不支持下推 filter。<br>主键模型的优势就是查询时不需要 merge，并且支持下推 filter 和索引。这种方式主要是将主键索引加载到内存中，对于 Update 操作，通过主键索引找到记录的位置，写一个 Delete，然后再写一个 Insert。可以发现这种方案仍然是不支持 MVCC 的，我理解如果要支持 MVCC 那么 merge 可能是必然的。<br>此外，主键模型对内存是有开销的，我理解这个应该不是关键问题。首先，如果数据有冷热之分，可以持久化一部分主键索引到磁盘上。其次，这个场景在大宽表有优势。</p>
<h4 id="来自-TiKV-的约束"><a href="#来自-TiKV-的约束" class="headerlink" title="来自 TiKV 的约束"></a>来自 TiKV 的约束</h4><p>从 Raft 层接入数据导致 TiFlash 的存储层的分区会收到 TiKV Key Format 的影响。例如尽管 TiFlash 的 Segment 和 TiKV 的 Region 并不对应，Segment 远大于 Region。但它们都被映射到同一个 Key Range 上。</p>
<p>这就导致 TiFlash 数据的物理排列一定是根据 TiKV 的主键有序的，TiFlash 无法自行指定主键。另外 TiFlash 本身也没有二级索引。</p>
<p>目前来自 TiKV 的约束有：</p>
<ol>
<li>MVCC 字段<br> 如果要和 TiDB 一起玩，就必须要支持 MVCC，不能只保存最新的版本。</li>
<li>Unique 的主键</li>
</ol>
<h3 id="DM-的-Delta-层是如何实现的？"><a href="#DM-的-Delta-层是如何实现的？" class="headerlink" title="DM 的 Delta 层是如何实现的？"></a>DM 的 Delta 层是如何实现的？</h3><p>PageStorage 先前使用 Append 写加上 GC 的方案，但带来写放大、读放大和空间放大。因为这里 GC 采用的 Copy Out 的方式，所以理论上写放大和空间放大构成一个 trade off：</p>
<ol>
<li>如果允许更少的有效数据和更多的碎片，那么空间放大更严重</li>
<li>否则，写放大更严重</li>
</ol>
<p>旧的 PageStorage 主要存在下面的问题：</p>
<ol>
<li>GC 开销很大，因为需要遍历所有的 Version 或者说 Snapshot 才能得到可以被安全删除的数据。这样会产生很多额外的遍历。</li>
<li>每张表一个实例，如果存在很多小表，则会产生非常多的文件，甚至会用光 fd。</li>
<li>冷热数据分离。因为 WAL 一般会被频繁更新，而它和真正的数据被一起放到 PageFile 中。这会导致每次 compact WAL 会重写大量 PageFile，而这一部分基本都是不需要被 gc 的冷数据。等到一定程度之后，又会触发 gc，进一步加剧问题。</li>
</ol>
<p>在 SSD 盘上，随机写和顺序写的差距不大，原因是 FTL 会将随机写转换为顺序写，所以寻址相关的开销并不是很大。尽管如此，顺序写依然存在优势，首先顺序写可以做聚合，同样的 IOPS 写入带宽是会比随机写要大很多，然后是顺序写的 gc 会更容易。此外，因为变成随机读，性能会变差。特别是对类似 Raft Log 这样的 scan 场景。</p>
<p>新一版本的设计中：</p>
<ul>
<li>TiFlash 会通过 SpaceMap 尽量选择从已有的文件中分配一块合适的空间用来写入 blob。<br>  当 blob 被分配完毕后，多个 writer 可以并发地写自己的部分。在写入 blob 完成后，会写 WAL 记录相关元信息。在这之后就可以更新内存中的数据。<br>  这主要是缓解了空间放大，通过复用之前的 blob file，从而减少了 gc 的数量。</li>
<li>分离冷热数据<br>  引入专门的 WALStore 管理 WAL。</li>
</ul>
<p>PageStorage 在内存中维护一个 PageDirectory 去根据 page_id 找到对应 Page 的位置。</p>
<p>PageStorage 中使用 FullGC 去移除掉 valid rate 不高的 blob file，剩余的数据会被拷贝出来组成一个新 blob file。一次 GC 之后，epoch 自增。</p>
<h3 id="DM-的-Stable-层是如何实现的？"><a href="#DM-的-Stable-层是如何实现的？" class="headerlink" title="DM 的 Stable 层是如何实现的？"></a>DM 的 Stable 层是如何实现的？</h3><p>Stable 层数据是按照 DTFile 的形式存储的，且每个 DTFile 中包含多个 Pack，一个 Pack 默认是包含 8192 行数据。但是相同主键不同版本的行都会写在一个 Pack 里面，目的之一是方便构造 Min-Max 索引。</p>
<p>我们为每个 Pack 维护一个 Min-Max 索引，这样可以在扫描的时候比较方便地跳过某些 Pack。理论上 Pack 越小，MinMax 索引的效率越好。因为更容易有 Pack 被整个选中，或者整个拒绝。</p>
<h3 id="为什么只有-Min-Max-索引？"><a href="#为什么只有-Min-Max-索引？" class="headerlink" title="为什么只有 Min-Max 索引？"></a>为什么只有 Min-Max 索引？</h3><p>还有其他索引没有来得及实现。</p>
<p>但对于 BloomFilter 这是一个例外，因为按照目前 TiFlash 的查询 Pattern，布隆过滤器在大部分场景下优化不是很大，这样的查询其实是可以下推给 TiKV 来做的。只有在一些子查询里面如果出现点查，则可能会有作用。</p>
<h3 id="为什么-DM-的-Stable-只有一层？"><a href="#为什么-DM-的-Stable-只有一层？" class="headerlink" title="为什么 DM 的 Stable 只有一层？"></a>为什么 DM 的 Stable 只有一层？</h3><p>DM 的设计目标包含优化读性能和支持 MVCC 过滤。这就导致要<a href="https://tidb.net/book/tidb-monthly/2022/2022-04/update/tiflash-storage" target="_blank" rel="noopener">解决下面的场景</a>：</p>
<blockquote>
<p>TiFlash 有比较多的数据更新操作，与此同时承载的读请求，都会需要通过 MVCC 版本过滤出需要读的数据。而以 LSM Tree 形式组织数据的话，在处理 Scan 操作的时候，会需要从 L0 的所有文件，以及其他层中与查询的 key-range 有 overlap 的所有文件，以堆排序的形式合并、过滤数据。在合并数据的这个入堆、出堆的过程中 CPU 的分支经常会 miss，cache 命中也会很低。测试结果表明，在处理 Scan 请求的时候，大量的 CPU 都消耗在这个堆排序的过程中。</p>
</blockquote>
<blockquote>
<p>另外，采用 LSM Tree 结构，对于过期数据的清理，通常在 level compaction 的过程中，才能被清理掉（即 Lk-1 层与 Lk 层 overlap 的文件进行 compaction）。而 level compaction 的过程造成的写放大会比较严重。当后台 compaction 流量比较大的时候，会影响到前台的写入和数据读取的性能，造成性能不稳定。</p>
</blockquote>
<p>为了缓解单层带来的写放大，DM 按照 key range 分成了多个 Segment。每个 Segment 中包含自己的 Stable 和 Delta。其中 Delta 合并 Stable 会产生一个新的 Stable。</p>
<h3 id="为什么-TiFlash-按-TSO-升序存储？"><a href="#为什么-TiFlash-按-TSO-升序存储？" class="headerlink" title="为什么 TiFlash 按 TSO 升序存储？"></a>为什么 TiFlash 按 TSO 升序存储？</h3><p>TiKV 的 TSO 按照逆序存，有利于找新版本。<br>TiFlash 因为都是处理扫表，所以逆序的收益不是很大。ClickHouse 使用升序存储，所以 TiFlash 也沿用了升序。<br>但这里就导致在处理 Snapshot 写入的时候，需要读完每个 row key 的所有版本，并在一个 read 调用中返回给下游的 stream。</p>
<h3 id="Segment-和-Region-的对应关系"><a href="#Segment-和-Region-的对应关系" class="headerlink" title="Segment 和 Region 的对应关系"></a>Segment 和 Region 的对应关系</h3><p>通过将一张表分为多个 Segment，可以减少 delta merge 的负担。但同时产生了额外 split 和 merge 这些 segment 的代价。</p>
<p>并不是 Segment 越多，性能越好。例如对于 Vector Index 来说，Segment 数量增大会导致一个大 HSNW 图被分为若干个小 HSNW 图。因为在每个图中查找的复杂度是 <code>O(log(n))</code>，拆分为 k 个图实际上会导致复杂度变为 <code>k * O(log(n / k))</code>。因此，性能会受损。</p>
<h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><h3 id="为什么-TiFlash-没有-buffer-pool"><a href="#为什么-TiFlash-没有-buffer-pool" class="headerlink" title="为什么 TiFlash 没有 buffer pool"></a>为什么 TiFlash 没有 buffer pool</h3><p>对于 AP 负载，扫表的数据规模很大，Cache 起不到太大作用。</p>
<p>但是，如果能 cache 住一些解压后的 page 数据，对于某些返回集合比较小的查询，性能提升也还是非常明显的。</p>
<h2 id="Delta-Index"><a href="#Delta-Index" class="headerlink" title="Delta Index"></a>Delta Index</h2><p>Delta Index，有个简单的介绍 <a href="https://tidb.net/blog/7926aa79%E3%80%82" target="_blank" rel="noopener">https://tidb.net/blog/7926aa79。</a></p>
<p>总的来说，Delta Index 主要是为了减少归并多个 CF Tiny 以及 Memtable 和 DMFile 所使用的内存和 CPU（例如维护二叉堆结构所需要的比较和移动操作），从而引入的一个结构。这个结构中记录了 Delta 层中读取的顺序，以及两次 Delta 层读取操作之间，需要读取的 Stable 数据的行数。所以，它有点像是一个 DP 或者记搜，记忆化的是归并排序的结果。</p>
<p>容易发现，如果 Segment 发生了 range 变化，或者发生了 delta-merge，则 Delta Index 就会失效，从而要被重新构建。</p>
<p>需要注意的是，Delta Index 本身并不包含 MVCC 语义，因此还是需要通过它把 Delta 数据逐个读出之后，才能应用 MVCC Filter。</p>
<p>DeltaIndex 的两个场景：</p>
<ul>
<li>第一个场景是 delta-merge，也就是所谓的 Major Compaction。这个操作需要将 tiered 的 delta 层和 stable 层加在一起重新生成一个 stable 层。所以在这个操作之后，delta 层清空，而 Delta Index 也随之清空。注意到这个操作是对整个表的一次全量遍历，并最终要生成一个有序的流。</li>
<li>第二个是普通的读取，此时，数据并不需要按照 PK 被返回，顺序的要求只是为了便于 MVCC 处理。实际上我们需要的是 PK 成组，以及组内（关于时间戳）有序。</li>
</ul>
<h2 id="资源管理"><a href="#资源管理" class="headerlink" title="资源管理"></a>资源管理</h2><h3 id="弹性的资源管理和存算分离"><a href="#弹性的资源管理和存算分离" class="headerlink" title="弹性的资源管理和存算分离"></a>弹性的资源管理和存算分离</h3><p>在目前的计算机架构下，进程是资源的分配单位。这就意味着如果程序对除了 CPU 之外的某个资源的需求存在很大的弹性，那么就需要将这一部分单独剥离出来。<br>TiFlash Cloud 中就使用了存算分离，当然还使用了 OSS 等方案，我认为是正交的设计。</p>
<h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><p>历史上计算层出现过不少因为查询过大导致的 OOM，计算层通过 kill query 或者 spill 的方式进行解决。但存储层目前还缺少这块。理论上存储层的开销主要分为几类：</p>
<ol>
<li>Memtable<br> 包含 KVStore 的 RegionData 和 DeltaTree 的 DeltaCache。<br> 这类场景下，OOM 主要发生在大事务场景。</li>
<li>Cache<br> 主要用来服务计算节点，列存主要是扫表，所以没有做 Block cache 或者 row cache。</li>
<li>索引<br> 包含 DeltaTree 的 DeltaIndex，PageStorage 的 PageDirectory 等。</li>
<li>Compaction 相关，比如 delte merge 等</li>
<li>行转列相关</li>
<li>系统的 Page Cache</li>
</ol>
<p>在一些场景下，因为存储层和计算层并不互相感知，会导致存储层会被计算层的大任务干到 OOM 或者报异常。而实际上这些任务可以被 kill，stall 或者通过 kill query 抢占计算层的内存。</p>
<p>因此，在 TiFlash 侧实现一个<a href="/2024/08/23/monitor-alloc-in-C++/">统一的内存管理</a>还是有必要的。</p>
<h4 id="空指针"><a href="#空指针" class="headerlink" title="空指针"></a>空指针</h4><p>严格来讲避免空指针也不完全算是内存管理。但确实是工作中遇到的一个比较关键的问题。在 <a href="/2023/01/12/high-concurrency/">分布式架构和高并发相关场景</a> 这篇文章里面讨论。</p>
<h3 id="线程"><a href="#线程" class="headerlink" title="线程"></a>线程</h3><h3 id="IO"><a href="#IO" class="headerlink" title="IO"></a>IO</h3><h3 id="CPU-和-off-CPU"><a href="#CPU-和-off-CPU" class="headerlink" title="CPU 和 off-CPU"></a>CPU 和 off-CPU</h3><p><a href="/2024/11/21/cpu-profiling/">CPU Profiling 经验之谈</a>。</p>
<h1 id="副本管理"><a href="#副本管理" class="headerlink" title="副本管理"></a>副本管理</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol>
<li><a href="https://docs.pingcap.com/zh/tidb/stable/troubleshoot-hot-spot-issues" target="_blank" rel="noopener">https://docs.pingcap.com/zh/tidb/stable/troubleshoot-hot-spot-issues</a></li>
<li><a href="https://www.infoq.com/articles/raft-engine-tikv-database/" target="_blank" rel="noopener">https://www.infoq.com/articles/raft-engine-tikv-database/</a> RaftEngine</li>
<li><a href="https://www.zhihu.com/question/47544675" target="_blank" rel="noopener">https://www.zhihu.com/question/47544675</a> 固态硬盘性能</li>
<li><a href="https://docs.pingcap.com/zh/tidb/stable/titan-overview" target="_blank" rel="noopener">https://docs.pingcap.com/zh/tidb/stable/titan-overview</a> Titan 设计</li>
<li>Fast scans on key-value stores</li>
<li><a href="https://cn.pingcap.com/article/post/3946.html" target="_blank" rel="noopener">https://cn.pingcap.com/article/post/3946.html</a> PingCAP 的早期技术分享</li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
    <div></div>
    <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
      <span>赏</span>
    </button>
    <div id="QR" style="display: none;">
      
        <div id="wechat" style="display: inline-block">
          <img id="wechat_qr" src="/img/fkm/wxfk.jpg" alt="Calvin Neo WeChat Pay"/>
          <p>微信打赏</p>
        </div>
      
      
        <div id="alipay" style="display: inline-block">
          <img id="alipay_qr" src="/img/fkm/zfbfk.jpg" alt="Calvin Neo Alipay"/>
          <p>支付宝打赏</p>
        </div>
      
    </div>
  </div>


      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/分布式/" rel="tag"># 分布式</a>
          
            <a href="/tags/事务/" rel="tag"># 事务</a>
          
            <a href="/tags/数据库/" rel="tag"># 数据库</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2023/07/14/tiflash-pt-1/" rel="next" title="TiFlash 性能测试的一个场景">
                <i class="fa fa-chevron-left"></i> TiFlash 性能测试的一个场景
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2023/07/22/on-cloud-tidb-tiflash/" rel="prev" title="云上的 TiDB 和 TiFlash">
                云上的 TiDB 和 TiFlash <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/favicon.jpg"
               alt="Calvin Neo" />
          <p class="site-author-name" itemprop="name">Calvin Neo</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">242</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">152</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/CalvinNeo" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://twitter.com/CalvinNeo0" target="_blank" title="Twitter">
                  
                    <i class="fa fa-fw fa-twitter"></i>
                  
                  Twitter
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/1568200035" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://xqq.im/" title="xqq" target="_blank">xqq</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://www.lovelywen.com/" title="wenwen" target="_blank">wenwen</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="https://smlight.github.io/blog/" title="zyyyyy" target="_blank">zyyyyy</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#架构"><span class="nav-number">1.</span> <span class="nav-text">架构</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#关于-SQL-on-KVStore"><span class="nav-number">1.1.</span> <span class="nav-text">关于 SQL on KVStore</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TiKV-相关"><span class="nav-number">2.</span> <span class="nav-text">TiKV 相关</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#高可用和持久性"><span class="nav-number">2.1.</span> <span class="nav-text">高可用和持久性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TiKV-写入"><span class="nav-number">2.2.</span> <span class="nav-text">TiKV 写入</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#关于-Region-大小的讨论"><span class="nav-number">2.2.1.</span> <span class="nav-text">关于 Region 大小的讨论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#日志和数据分离存储"><span class="nav-number">2.2.2.</span> <span class="nav-text">日志和数据分离存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-存储-–-RaftEngine"><span class="nav-number">2.2.3.</span> <span class="nav-text">Raft 存储 – RaftEngine</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-RockDB？"><span class="nav-number">2.2.4.</span> <span class="nav-text">Why RockDB？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KVEngine-上的-WAL"><span class="nav-number">2.2.5.</span> <span class="nav-text">KVEngine 上的 WAL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Titan"><span class="nav-number">2.2.6.</span> <span class="nav-text">Titan</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SST-的格式"><span class="nav-number">2.2.7.</span> <span class="nav-text">SST 的格式</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TiKV-内存管理"><span class="nav-number">2.3.</span> <span class="nav-text">TiKV 内存管理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TiKV-读取"><span class="nav-number">2.4.</span> <span class="nav-text">TiKV 读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cache"><span class="nav-number">2.4.1.</span> <span class="nav-text">Cache</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lease"><span class="nav-number">2.4.2.</span> <span class="nav-text">Lease</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Coprocessor"><span class="nav-number">2.5.</span> <span class="nav-text">Coprocessor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cop-可以支持写入么？"><span class="nav-number">2.5.1.</span> <span class="nav-text">Cop 可以支持写入么？</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-Raft-相关"><span class="nav-number">3.</span> <span class="nav-text">Multi Raft 相关</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#关于-Raft-协议本身"><span class="nav-number">3.1.</span> <span class="nav-text">关于 Raft 协议本身</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#线性一致读"><span class="nav-number">3.1.1.</span> <span class="nav-text">线性一致读</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#乱序-apply-和乱序-commit"><span class="nav-number">3.1.2.</span> <span class="nav-text">乱序 apply 和乱序 commit</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Raft-状态的思考"><span class="nav-number">3.2.</span> <span class="nav-text">Raft 状态的思考</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储-Raft-状态和-Region-状态"><span class="nav-number">3.3.</span> <span class="nav-text">存储 Raft 状态和 Region 状态</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#一个-Eager-落盘导致的问题"><span class="nav-number">3.3.1.</span> <span class="nav-text">一个 Eager 落盘导致的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Region-Meta-和-Raft-Meta"><span class="nav-number">3.3.1.1.</span> <span class="nav-text">Region Meta 和 Raft Meta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#经验总结"><span class="nav-number">3.3.1.2.</span> <span class="nav-text">经验总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#持久化元信息"><span class="nav-number">3.3.2.</span> <span class="nav-text">持久化元信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基于共识层之上的事务"><span class="nav-number">3.4.</span> <span class="nav-text">基于共识层之上的事务</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Multi-Raft-的思考"><span class="nav-number">3.5.</span> <span class="nav-text">Multi Raft 的思考</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Split-Merge-和事务"><span class="nav-number">3.5.1.</span> <span class="nav-text">Split/Merge 和事务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Split-Merge-和-Read"><span class="nav-number">3.5.2.</span> <span class="nav-text">Split/Merge 和 Read</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Split-Merge-和-Apply-Snapshot"><span class="nav-number">3.5.3.</span> <span class="nav-text">Split/Merge 和 Apply Snapshot</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#下层存储使用-Mono-LSM-还是-Multi-LSMs"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">下层存储使用 Mono LSM 还是 Multi LSMs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Split-和-Apply-Snapshot-的冲突"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">Split 和 Apply Snapshot 的冲突</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Merge-和-Apply-Snapshot-的冲突"><span class="nav-number">3.5.3.3.</span> <span class="nav-text">Merge 和 Apply Snapshot 的冲突</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Split-和-Generate-Snapshot-的冲突"><span class="nav-number">3.5.3.4.</span> <span class="nav-text">Split 和 Generate Snapshot 的冲突</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-Group-和-Data-Range-的对应关系"><span class="nav-number">3.5.4.</span> <span class="nav-text">Raft Group 和 Data Range 的对应关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-到底复制什么？"><span class="nav-number">3.5.5.</span> <span class="nav-text">Raft 到底复制什么？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#进一步讨论：日志和选举的关系"><span class="nav-number">3.5.6.</span> <span class="nav-text">进一步讨论：日志和选举的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#进一步讨论：日志和事务的关系"><span class="nav-number">3.5.7.</span> <span class="nav-text">进一步讨论：日志和事务的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mono-LSM-和-Multi-LSM-的考量"><span class="nav-number">3.5.8.</span> <span class="nav-text">Mono LSM 和 Multi LSM 的考量</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多-Region-的调度"><span class="nav-number">3.6.</span> <span class="nav-text">多 Region 的调度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TiKV-的做法"><span class="nav-number">3.6.1.</span> <span class="nav-text">TiKV 的做法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#事务相关"><span class="nav-number">4.</span> <span class="nav-text">事务相关</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Partitioned-RaftKV-相关"><span class="nav-number">5.</span> <span class="nav-text">Partitioned RaftKV 相关</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#TiFlash-相关"><span class="nav-number">6.</span> <span class="nav-text">TiFlash 相关</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Raft-和-CDC"><span class="nav-number">6.1.</span> <span class="nav-text">Raft 和 CDC</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CDC-的相关背景"><span class="nav-number">6.1.1.</span> <span class="nav-text">CDC 的相关背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-TiFlash-实现-HTAP-基于-Raft？"><span class="nav-number">6.1.2.</span> <span class="nav-text">为什么 TiFlash 实现 HTAP 基于 Raft？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#架构-1"><span class="nav-number">6.2.</span> <span class="nav-text">架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么在-TiSpark-之外还开发-TiFlash"><span class="nav-number">6.2.1.</span> <span class="nav-text">为什么在 TiSpark 之外还开发 TiFlash</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#副本-节点数量和延迟的关系"><span class="nav-number">6.2.2.</span> <span class="nav-text">副本/节点数量和延迟的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#副本-节点数量和并发的关系"><span class="nav-number">6.2.3.</span> <span class="nav-text">副本/节点数量和并发的关系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DDL-如何同步？"><span class="nav-number">6.2.4.</span> <span class="nav-text">DDL 如何同步？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TiFlash-的高可用"><span class="nav-number">6.2.5.</span> <span class="nav-text">TiFlash 的高可用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Raft-共识层"><span class="nav-number">6.3.</span> <span class="nav-text">Raft 共识层</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#有关-Learner-Peer"><span class="nav-number">6.3.1.</span> <span class="nav-text">有关 Learner Peer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关-Learner-Read"><span class="nav-number">6.3.2.</span> <span class="nav-text">有关 Learner Read</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关-lock"><span class="nav-number">6.3.3.</span> <span class="nav-text">有关 lock</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Bypass-lock-机制"><span class="nav-number">6.3.3.1.</span> <span class="nav-text">Bypass lock 机制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Read-through-lock-机制"><span class="nav-number">6.3.3.2.</span> <span class="nav-text">Read through lock 机制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-Log-的存储"><span class="nav-number">6.3.4.</span> <span class="nav-text">Raft Log 的存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储-–-KVStore"><span class="nav-number">6.4.</span> <span class="nav-text">存储 – KVStore</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么在列存前还有一个-KVStore？"><span class="nav-number">6.4.1.</span> <span class="nav-text">为什么在列存前还有一个 KVStore？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么不将未提交的数据直接写在列存中呢？"><span class="nav-number">6.4.2.</span> <span class="nav-text">为什么不将未提交的数据直接写在列存中呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KVStore-的落盘模式相关问题"><span class="nav-number">6.4.3.</span> <span class="nav-text">KVStore 的落盘模式相关问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KVStore-如何处理事务"><span class="nav-number">6.4.4.</span> <span class="nav-text">KVStore 如何处理事务</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#KVStore-的存储格式"><span class="nav-number">6.4.5.</span> <span class="nav-text">KVStore 的存储格式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#是否直接用-protobuf-存储-Region？"><span class="nav-number">6.4.5.1.</span> <span class="nav-text">是否直接用 protobuf 存储 Region？</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#是否使用-flag-存储-Region-Extension？"><span class="nav-number">6.4.5.2.</span> <span class="nav-text">是否使用 flag 存储 Region Extension？</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Raft-机制带来的内存和存储开销"><span class="nav-number">6.4.6.</span> <span class="nav-text">Raft 机制带来的内存和存储开销</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TiFlash-如何处理-Raft-Snapshot？"><span class="nav-number">6.4.7.</span> <span class="nav-text">TiFlash 如何处理 Raft Snapshot？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-TiFlash-不处理-DeleteRange？"><span class="nav-number">6.4.8.</span> <span class="nav-text">为什么 TiFlash 不处理 DeleteRange？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重放"><span class="nav-number">6.4.9.</span> <span class="nav-text">重放</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储-–-Proxy"><span class="nav-number">6.5.</span> <span class="nav-text">存储 – Proxy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-Proxy-不能静态链接"><span class="nav-number">6.5.1.</span> <span class="nav-text">为什么 Proxy 不能静态链接</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关于-Proxy-的重构"><span class="nav-number">6.5.2.</span> <span class="nav-text">关于 Proxy 的重构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储-–-列存"><span class="nav-number">6.6.</span> <span class="nav-text">存储 – 列存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-TiFlash-使用-DeltaTree-作为存储"><span class="nav-number">6.6.1.</span> <span class="nav-text">为什么 TiFlash 使用 DeltaTree 作为存储</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#存储模型的进一步讨论"><span class="nav-number">6.6.2.</span> <span class="nav-text">存储模型的进一步讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#和-StarRocks-的比较"><span class="nav-number">6.6.2.1.</span> <span class="nav-text">和 StarRocks 的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#来自-TiKV-的约束"><span class="nav-number">6.6.2.2.</span> <span class="nav-text">来自 TiKV 的约束</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DM-的-Delta-层是如何实现的？"><span class="nav-number">6.6.3.</span> <span class="nav-text">DM 的 Delta 层是如何实现的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DM-的-Stable-层是如何实现的？"><span class="nav-number">6.6.4.</span> <span class="nav-text">DM 的 Stable 层是如何实现的？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么只有-Min-Max-索引？"><span class="nav-number">6.6.5.</span> <span class="nav-text">为什么只有 Min-Max 索引？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-DM-的-Stable-只有一层？"><span class="nav-number">6.6.6.</span> <span class="nav-text">为什么 DM 的 Stable 只有一层？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-TiFlash-按-TSO-升序存储？"><span class="nav-number">6.6.7.</span> <span class="nav-text">为什么 TiFlash 按 TSO 升序存储？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Segment-和-Region-的对应关系"><span class="nav-number">6.6.8.</span> <span class="nav-text">Segment 和 Region 的对应关系</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#读取"><span class="nav-number">6.7.</span> <span class="nav-text">读取</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么-TiFlash-没有-buffer-pool"><span class="nav-number">6.7.1.</span> <span class="nav-text">为什么 TiFlash 没有 buffer pool</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Delta-Index"><span class="nav-number">6.8.</span> <span class="nav-text">Delta Index</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#资源管理"><span class="nav-number">6.9.</span> <span class="nav-text">资源管理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#弹性的资源管理和存算分离"><span class="nav-number">6.9.1.</span> <span class="nav-text">弹性的资源管理和存算分离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#内存"><span class="nav-number">6.9.2.</span> <span class="nav-text">内存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#空指针"><span class="nav-number">6.9.2.1.</span> <span class="nav-text">空指针</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#线程"><span class="nav-number">6.9.3.</span> <span class="nav-text">线程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IO"><span class="nav-number">6.9.4.</span> <span class="nav-text">IO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-和-off-CPU"><span class="nav-number">6.9.5.</span> <span class="nav-text">CPU 和 off-CPU</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#副本管理"><span class="nav-number">7.</span> <span class="nav-text">副本管理</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Reference"><span class="nav-number">8.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy;  2015 - 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Calvin Neo</span>
  <span> &nbsp; Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a></span>
</div>
<div>
  <span><a href="/about/yytl/">版权声明</a></span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse 
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://www.calvinneo.com/2023/07/22/tikv-tidb-thought/';
          this.page.identifier = '2023/07/22/tikv-tidb-thought/';
          this.page.title = '关于 TiKV、TiDB、TiFlash 的一些思考';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://calvinneo.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      $('#local-search-input').focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
